@online{adlakhaEvaluatingCorrectnessFaithfulness2023,
  title = {Evaluating {{Correctness}} and {{Faithfulness}} of {{Instruction-Following Models}} for {{Question Answering}}},
  author = {Adlakha, Vaibhav and BehnamGhader, Parishad and Lu, Xing Han and Meade, Nicholas and Reddy, Siva},
  date = {2023-07-31},
  eprint = {2307.16877},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.16877},
  url = {http://arxiv.org/abs/2307.16877},
  urldate = {2023-10-07},
  abstract = {Retriever-augmented instruction-following models are attractive alternatives to fine-tuned approaches for information-seeking tasks such as question answering (QA). By simply prepending retrieved documents in its input along with an instruction, these models can be adapted to various information domains and tasks without additional fine-tuning. While the model responses tend to be natural and fluent, the additional verbosity makes traditional QA evaluation metrics such as exact match (EM) and F1 unreliable for accurately quantifying model performance. In this work, we investigate the performance of instruction-following models across three information-seeking QA tasks. We use both automatic and human evaluation to evaluate these models along two dimensions: 1) how well they satisfy the user's information need (correctness), and 2) whether they produce a response based on the provided knowledge (faithfulness). Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness. We then propose simple token-overlap based and model-based metrics that reflect the true performance of these models. Our analysis reveals that instruction-following models are competitive, and sometimes even outperform fine-tuned models for correctness. However, these models struggle to stick to the provided knowledge and often hallucinate in their responses. We hope our work encourages a more holistic evaluation of instruction-following models for QA. Our code and data is available at https://github.com/McGill-NLP/instruct-qa},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CGVYPKG5\\Adlakha et al. - 2023 - Evaluating Correctness and Faithfulness of Instruc.pdf;C\:\\Users\\mospr\\Zotero\\storage\\5PVCSLT5\\2307.html}
}

@online{albertiSyntheticQACorpora2019,
  title = {Synthetic {{QA Corpora Generation}} with {{Roundtrip Consistency}}},
  author = {Alberti, Chris and Andor, Daniel and Pitler, Emily and Devlin, Jacob and Collins, Michael},
  date = {2019-06-12},
  eprint = {1906.05416},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.05416},
  url = {http://arxiv.org/abs/1906.05416},
  urldate = {2023-10-07},
  abstract = {We introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction, and by filtering the results to ensure roundtrip consistency. By pretraining on the resulting corpora we obtain significant improvements on SQuAD2 and NQ, establishing a new state-of-the-art on the latter. Our synthetic data generation models, for both question generation and answer extraction, can be fully reproduced by finetuning a publicly available BERT model on the extractive subsets of SQuAD2 and NQ. We also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation, obtaining exact match and F1 at less than 0.1\% and 0.4\% from human performance on SQuAD2.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\G8ZLPTTD\\Alberti et al. - 2019 - Synthetic QA Corpora Generation with Roundtrip Con.pdf;C\:\\Users\\mospr\\Zotero\\storage\\PCBNFZ9G\\1906.html}
}

@online{anilLargeScaleDistributed2018,
  title = {Large Scale Distributed Neural Network Training through Online Distillation},
  author = {Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre and Ormandi, Robert and Dahl, George E. and Hinton, Geoffrey E.},
  date = {2018-04-01},
  doi = {10.48550/arXiv.1804.03235},
  url = {https://ui.adsabs.harvard.edu/abs/2018arXiv180403235A},
  urldate = {2023-10-05},
  abstract = {Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing \$6\textbackslash times 10\^\{11\}\$ tokens and based on the Common Crawl repository of web data.},
  organization = {{arXiv e-prints}},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ADS Bibcode: 2018arXiv180403235A},
  file = {C:\Users\mospr\Zotero\storage\BV42K38E\Anil et al. - 2018 - Large scale distributed neural network training th.pdf}
}

@inproceedings{anilLargeScaleDistributed2018a,
  title = {Large Scale Distributed Neural Network Training through Online Distillation},
  author = {Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre Tachard and Ormandi, Robert and Dahl, George and Hinton, Geoffrey},
  date = {2018},
  url = {https://openreview.net/pdf?id=rkr1UDeC-},
  urldate = {2023-10-05}
}

@online{anilLargeScaleDistributed2020,
  title = {Large Scale Distributed Neural Network Training through Online Distillation},
  author = {Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre and Ormandi, Robert and Dahl, George E. and Hinton, Geoffrey E.},
  date = {2020-08-20},
  eprint = {1804.03235},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1804.03235},
  url = {http://arxiv.org/abs/1804.03235},
  urldate = {2023-09-08},
  abstract = {Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing \$6\textbackslash times 10\^\{11\}\$ tokens and based on the Common Crawl repository of web data.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\BEMU4QIZ\\Anil et al. - 2020 - Large scale distributed neural network training th.pdf;C\:\\Users\\mospr\\Zotero\\storage\\W2BBPW2K\\1804.html}
}

@online{baDeepNetsReally2014,
  title = {Do {{Deep Nets Really Need}} to Be {{Deep}}?},
  author = {Ba, Lei Jimmy and Caruana, Rich},
  date = {2014-10-10},
  eprint = {1312.6184},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1312.6184},
  url = {http://arxiv.org/abs/1312.6184},
  urldate = {2023-09-08},
  abstract = {Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model. We evaluate our method on the TIMIT phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\M9QBMABV\\Ba and Caruana - 2014 - Do Deep Nets Really Need to be Deep.pdf;C\:\\Users\\mospr\\Zotero\\storage\\8229XIBK\\1312.html}
}

@inproceedings{baDeepNetsReally2014a,
  title = {Do {{Deep Nets Really Need}} to Be {{Deep}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ba, Jimmy and Caruana, Rich},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.},
  file = {C:\Users\mospr\Zotero\storage\YTT25NH2\Ba and Caruana - 2014 - Do Deep Nets Really Need to be Deep.pdf}
}

@inproceedings{bannerPostTraining4bit2019,
  title = {Post Training 4-Bit Quantization of Convolutional Networks for Rapid-Deployment},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Banner, Ron and Nahshan, Yury and Soudry, Daniel},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/c0a62e133894cdce435bcb4a5df1db2d-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Convolutional neural networks require significant memory bandwidth and storage for intermediate computations, apart from substantial computing resources. Neural network quantization has significant benefits in reducing the amount of intermediate results, but it often requires the full datasets and time-consuming fine tuning to recover the accuracy lost after quantization. This paper introduces the first practical 4-bit post training quantization approach: it does not involve training the quantized model (fine-tuning), nor it requires the availability of the full dataset. We target the quantization of both activations and weights and suggest three complementary methods for minimizing quantization error at the tensor level, two of whom obtain a closed-form analytical solution. Combining these methods, our approach achieves accuracy that is just a few percents less the state-of-the-art baseline across a wide range of convolutional models. The source code to replicate all experiments is available on GitHub: \textbackslash url\{https://github.com/submission2019/cnn-quantization\}.},
  file = {C:\Users\mospr\Zotero\storage\865M3I9M\Banner et al. - 2019 - Post training 4-bit quantization of convolutional .pdf}
}

@online{bannerPosttraining4bitQuantization2019,
  title = {Post-Training 4-Bit Quantization of Convolution Networks for Rapid-Deployment},
  author = {Banner, Ron and Nahshan, Yury and Hoffer, Elad and Soudry, Daniel},
  date = {2019-05-29},
  eprint = {1810.05723},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.05723},
  url = {http://arxiv.org/abs/1810.05723},
  urldate = {2023-09-06},
  abstract = {Convolutional neural networks require significant memory bandwidth and storage for intermediate computations, apart from substantial computing resources. Neural network quantization has significant benefits in reducing the amount of intermediate results, but it often requires the full datasets and time-consuming fine tuning to recover the accuracy lost after quantization. This paper introduces the first practical 4-bit post training quantization approach: it does not involve training the quantized model (fine-tuning), nor it requires the availability of the full dataset. We target the quantization of both activations and weights and suggest three complementary methods for minimizing quantization error at the tensor level, two of whom obtain a closed-form analytical solution. Combining these methods, our approach achieves accuracy that is just a few percents less the state-of-the-art baseline across a wide range of convolutional models. The source code to replicate all experiments is available on GitHub: \textbackslash url\{https://github.com/submission2019/cnn-quantization\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\M5JLJ2J5\\Banner et al. - 2019 - Post-training 4-bit quantization of convolution ne.pdf;C\:\\Users\\mospr\\Zotero\\storage\\HKAYCG6M\\1810.html}
}

@online{bengioEstimatingPropagatingGradients2013,
  title = {Estimating or {{Propagating Gradients Through Stochastic Neurons}} for {{Conditional Computation}}},
  author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
  date = {2013-08-15},
  eprint = {1308.3432},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1308.3432},
  url = {http://arxiv.org/abs/1308.3432},
  urldate = {2023-09-06},
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{\textbackslash em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\5UYYRCX3\\Bengio et al. - 2013 - Estimating or Propagating Gradients Through Stocha.pdf;C\:\\Users\\mospr\\Zotero\\storage\\9VV6HHHD\\1308.html}
}

@online{beyerKnowledgeDistillationGood2022,
  title = {Knowledge Distillation: {{A}} Good Teacher Is Patient and Consistent},
  shorttitle = {Knowledge Distillation},
  author = {Beyer, Lucas and Zhai, Xiaohua and Royer, Amélie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander},
  date = {2022-06-21},
  eprint = {2106.05237},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.05237},
  url = {http://arxiv.org/abs/2106.05237},
  urldate = {2023-09-11},
  abstract = {There is a growing discrepancy in computer vision between large-scale models that achieve state-of-the-art performance and models that are affordable in practical applications. In this paper we address this issue and significantly bridge the gap between these two types of models. Throughout our empirical investigation we do not aim to necessarily propose a new method, but strive to identify a robust and effective recipe for making state-of-the-art large scale models affordable in practice. We demonstrate that, when performed correctly, knowledge distillation can be a powerful tool for reducing the size of large models without compromising their performance. In particular, we uncover that there are certain implicit design choices, which may drastically affect the effectiveness of distillation. Our key contribution is the explicit identification of these design choices, which were not previously articulated in the literature. We back up our findings by a comprehensive empirical study, demonstrate compelling results on a wide range of vision datasets and, in particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which achieves 82.8\% top-1 accuracy.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{beyerKnowledgeDistillationGood2022a,
  title = {Knowledge {{Distillation}}: {{A Good Teacher Is Patient}} and {{Consistent}}},
  shorttitle = {Knowledge {{Distillation}}},
  author = {Beyer, Lucas and Zhai, Xiaohua and Royer, Amélie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander},
  date = {2022},
  pages = {10925--10934},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Beyer_Knowledge_Distillation_A_Good_Teacher_Is_Patient_and_Consistent_CVPR_2022_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\YD9V4I5E\Beyer et al. - 2022 - Knowledge Distillation A Good Teacher Is Patient .pdf}
}

@online{bhalgatLSQImprovingLowbit2020,
  title = {{{LSQ}}+: {{Improving}} Low-Bit Quantization through Learnable Offsets and Better Initialization},
  shorttitle = {{{LSQ}}+},
  author = {Bhalgat, Yash and Lee, Jinwon and Nagel, Markus and Blankevoort, Tijmen and Kwak, Nojun},
  date = {2020-04-20},
  eprint = {2004.09576},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2004.09576},
  url = {http://arxiv.org/abs/2004.09576},
  urldate = {2023-09-06},
  abstract = {Unlike ReLU, newer activation functions (like Swish, H-swish, Mish) that are frequently employed in popular efficient architectures can also result in negative activation values, with skewed positive and negative ranges. Typical learnable quantization schemes [PACT, LSQ] assume unsigned quantization for activations and quantize all negative activations to zero which leads to significant loss in performance. Naively using signed quantization to accommodate these negative values requires an extra sign bit which is expensive for low-bit (2-, 3-, 4-bit) quantization. To solve this problem, we propose LSQ+, a natural extension of LSQ, wherein we introduce a general asymmetric quantization scheme with trainable scale and offset parameters that can learn to accommodate the negative activations. Gradient-based learnable quantization schemes also commonly suffer from high instability or variance in the final training performance, hence requiring a great deal of hyper-parameter tuning to reach a satisfactory performance. LSQ+ alleviates this problem by using an MSE-based initialization scheme for the quantization parameters. We show that this initialization leads to significantly lower variance in final performance across multiple training runs. Overall, LSQ+ shows state-of-the-art results for EfficientNet and MixNet and also significantly outperforms LSQ for low-bit quantization of neural nets with Swish activations (e.g.: 1.8\% gain with W4A4 quantization and upto 5.6\% gain with W2A2 quantization of EfficientNet-B0 on ImageNet dataset). To the best of our knowledge, ours is the first work to quantize such architectures to extremely low bit-widths.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CUZNDJM8\\Bhalgat et al. - 2020 - LSQ+ Improving low-bit quantization through learn.pdf;C\:\\Users\\mospr\\Zotero\\storage\\NCKGYVS7\\2004.html}
}

@inproceedings{bhalgatLSQImprovingLowBit2020,
  title = {{{LSQ}}+: {{Improving Low-Bit Quantization Through Learnable Offsets}} and {{Better Initialization}}},
  shorttitle = {{{LSQ}}+},
  author = {Bhalgat, Yash and Lee, Jinwon and Nagel, Markus and Blankevoort, Tijmen and Kwak, Nojun},
  date = {2020},
  pages = {696--697},
  url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w40/Bhalgat_LSQ_Improving_Low-Bit_Quantization_Through_Learnable_Offsets_and_Better_Initialization_CVPRW_2020_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  file = {C:\Users\mospr\Zotero\storage\6MZL8NKW\Bhalgat et al. - 2020 - LSQ+ Improving Low-Bit Quantization Through Learn.pdf}
}

@online{blalockWhatStateNeural2020,
  title = {What Is the {{State}} of {{Neural Network Pruning}}?},
  author = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
  date = {2020-03-06},
  eprint = {2003.03033},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2003.03033},
  url = {http://arxiv.org/abs/2003.03033},
  urldate = {2023-09-07},
  abstract = {Neural network pruning---the task of reducing the size of a network by removing parameters---has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\NYCFKXQ6\\Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf;C\:\\Users\\mospr\\Zotero\\storage\\VM5MNYN3\\2003.html}
}

@article{blalockWhatStateNeural2020a,
  title = {What Is the {{State}} of {{Neural Network Pruning}}?},
  author = {Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
  date = {2020-03-15},
  journaltitle = {Proceedings of Machine Learning and Systems},
  volume = {2},
  pages = {129--146},
  url = {https://proceedings.mlsys.org/paper_files/paper/2020/hash/6c44dc73014d66ba49b28d483a8f8b0d-Abstract.html},
  urldate = {2023-10-05},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\FH2BMUNH\Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf}
}

@online{caoBTRBinaryToken2023,
  title = {{{BTR}}: {{Binary Token Representations}} for {{Efficient Retrieval Augmented Language Models}}},
  shorttitle = {{{BTR}}},
  author = {Cao, Qingqing and Min, Sewon and Wang, Yizhong and Hajishirzi, Hannaneh},
  date = {2023-10-02},
  url = {https://arxiv.org/abs/2310.01329v1},
  urldate = {2023-10-08},
  abstract = {Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95\% task performance.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\mospr\Zotero\storage\FDI2EECF\Cao et al. - 2023 - BTR Binary Token Representations for Efficient Re.pdf}
}

@online{chenCrossLayerDistillationSemantic2021,
  title = {Cross-{{Layer Distillation}} with {{Semantic Calibration}}},
  author = {Chen, Defang and Mei, Jian-Ping and Zhang, Yuan and Wang, Can and Feng, Yan and Chen, Chun},
  date = {2021-08-29},
  eprint = {2012.03236},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.03236},
  url = {http://arxiv.org/abs/2012.03236},
  urldate = {2023-09-08},
  abstract = {Knowledge distillation is a technique to enhance the generalization ability of a student model by exploiting outputs from a teacher model. Recently, feature-map based variants explore knowledge transfer between manually assigned teacher-student pairs in intermediate layers for further improvement. However, layer semantics may vary in different neural networks and semantic mismatch in manual layer associations will lead to performance degeneration due to negative regularization. To address this issue, we propose Semantic Calibration for cross-layer Knowledge Distillation (SemCKD), which automatically assigns proper target layers of the teacher model for each student layer with an attention mechanism. With a learned attention distribution, each student layer distills knowledge contained in multiple teacher layers rather than a specific intermediate layer for appropriate cross-layer supervision. We further provide theoretical analysis of the association weights and conduct extensive experiments to demonstrate the effectiveness of our approach. Code is avaliable at \textbackslash url\{https://github.com/DefangChen/SemCKD\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\2CW585CU\\Chen et al. - 2021 - Cross-Layer Distillation with Semantic Calibration.pdf;C\:\\Users\\mospr\\Zotero\\storage\\RYMJ47D2\\2012.html}
}

@article{chenCrossLayerDistillationSemantic2021a,
  title = {Cross-{{Layer Distillation}} with {{Semantic Calibration}}},
  author = {Chen, Defang and Mei, Jian-Ping and Zhang, Yuan and Wang, Can and Wang, Zhe and Feng, Yan and Chen, Chun},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {8},
  pages = {7028--7036},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i8.16865},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/16865},
  urldate = {2023-10-05},
  abstract = {Recently proposed knowledge distillation approaches based on feature-map transfer validate that intermediate layers of a teacher model can serve as effective targets for training a student model to obtain better generalization ability. Existing studies mainly focus on particular representation forms for knowledge transfer between manually specified pairs of teacher-student intermediate layers. However, semantics of intermediate layers may vary in different networks and manual association of layers might lead to negative regularization caused by semantic mismatch between certain teacher-student layer pairs. To address this problem, we propose Semantic Calibration for Cross-layer Knowledge Distillation (SemCKD), which automatically assigns proper target layers of the teacher model for each student layer with an attention mechanism. With a learned attention distribution, each student layer distills knowledge contained in multiple layers rather than a single fixed intermediate layer from the teacher model for appropriate cross-layer supervision in training. Consistent improvements over state-of-the-art approaches are observed in extensive experiments with various network architectures for teacher and student models, demonstrating the effectiveness and flexibility of the proposed attention based soft layer association mechanism for cross-layer distillation.},
  issue = {8},
  langid = {english},
  keywords = {Learning \& Optimization for CV},
  file = {C:\Users\mospr\Zotero\storage\APCPUBSU\Chen et al. - 2021 - Cross-Layer Distillation with Semantic Calibration.pdf}
}

@online{chenFELMBenchmarkingFactuality2023,
  title = {{{FELM}}: {{Benchmarking Factuality Evaluation}} of {{Large Language Models}}},
  shorttitle = {{{FELM}}},
  author = {Chen, Shiqi and Zhao, Yiran and Zhang, Jinghan and Chern, I.-Chun and Gao, Siyang and Liu, Pengfei and He, Junxian},
  date = {2023-10-01},
  url = {https://arxiv.org/abs/2310.00741v1},
  urldate = {2023-10-08},
  abstract = {Assessing factuality of text generated by large language models (LLMs) is an emerging yet crucial research area, aimed at alerting users to potential errors and guiding the development of more reliable LLMs. Nonetheless, the evaluators assessing factuality necessitate suitable evaluation themselves to gauge progress and foster advancements. This direction remains under-explored, resulting in substantial impediments to the progress of factuality evaluators. To mitigate this issue, we introduce a benchmark for Factuality Evaluation of large Language Models, referred to as felm. In this benchmark, we collect responses generated from LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous studies that primarily concentrate on the factuality of world knowledge (e.g.\textasciitilde information from Wikipedia), felm focuses on factuality across diverse domains, spanning from world knowledge to math and reasoning. Our annotation is based on text segments, which can help pinpoint specific factual errors. The factuality annotations are further supplemented by predefined error types and reference links that either support or contradict the statement. In our experiments, we investigate the performance of several LLM-based factuality evaluators on felm, including both vanilla LLMs and those augmented with retrieval mechanisms and chain-of-thought processes. Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\mospr\Zotero\storage\JZUWUVD9\Chen et al. - 2023 - FELM Benchmarking Factuality Evaluation of Large .pdf}
}

@article{chengModelCompressionAcceleration2018,
  title = {Model {{Compression}} and {{Acceleration}} for {{Deep Neural Networks}}: {{The Principles}}, {{Progress}}, and {{Challenges}}},
  shorttitle = {Model {{Compression}} and {{Acceleration}} for {{Deep Neural Networks}}},
  author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  date = {2018-01},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {126--136},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2765695},
  abstract = {In recent years, deep neural networks (DNNs) have received increased attention, have been applied to different applications, and achieved dramatic accuracy improvements in many tasks. These works rely on deep networks with millions or even billions of parameters, and the availability of graphics processing units (GPUs) with very high computation capability plays a key role in their success. For example, Krizhevsky et al. achieved breakthrough results in the 2012 ImageNet Challenge using a network containing 60 million parameters with five convolutional layers and three fully connected layers. Usually, it takes two to three days to train the whole model on the ImagetNet data set with an NVIDIA K40 machine. In another example, the top face-verification results from the Labeled Faces in the Wild (LFW) data set were obtained with networks containing hundreds of millions of parameters, using a mix of convolutional, locally connected, and fully connected layers. It is also very time-consuming to train such a model to obtain a reasonable performance. In architectures that only rely on fully connected layers, the number of parameters can grow to billions.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {Computational modeling,Convolution,Convolutional codes,Machine learning,Neural networks,Quantization (signal),Training data},
  file = {C:\Users\mospr\Downloads\Model_Compression_and_Acceleration_for_Deep_Neural_Networks_The_Principles_Progress_and_Challenges.pdf}
}

@online{chengSurveyModelCompression2020,
  title = {A {{Survey}} of {{Model Compression}} and {{Acceleration}} for {{Deep Neural Networks}}},
  author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  date = {2020-06-14},
  eprint = {1710.09282},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1710.09282},
  url = {http://arxiv.org/abs/1710.09282},
  urldate = {2023-09-05},
  abstract = {Deep neural networks (DNNs) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. During the past five years, tremendous progress has been made in this area. In this paper, we review the recent techniques for compacting and accelerating DNN models. In general, these techniques are divided into four categories: parameter pruning and quantization, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and quantization are described first, after that the other techniques are introduced. For each category, we also provide insightful analysis about the performance, related applications, advantages, and drawbacks. Then we go through some very recent successful methods, for example, dynamic capacity networks and stochastic depths networks. After that, we survey the evaluation matrices, the main datasets used for evaluating the model performance, and recent benchmark efforts. Finally, we conclude this paper, discuss remaining the challenges and possible directions for future work.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\RDN44QRU\\Cheng et al. - 2020 - A Survey of Model Compression and Acceleration for.pdf;C\:\\Users\\mospr\\Zotero\\storage\\WJVG3SRV\\1710.html}
}

@inproceedings{chenLearningEfficientObject2017,
  title = {Learning Efficient Object Detection Models with Knowledge Distillation},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
  date = {2017-12-04},
  series = {{{NIPS}}'17},
  pages = {742--751},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {Despite significant accuracy improvement in convolutional neural networks (CNN) based object detectors, they often require prohibitive runtimes to process an image for real-time applications. State-of-the-art models often use very deep networks with a large number of floating point operations. Efforts such as model compression learn compact models with fewer number of parameters, but with much reduced accuracy. In this work, we propose a new framework to learn compact and fast object detection networks with improved accuracy using knowledge distillation [20] and hint learning [34]. Although knowledge distillation has demonstrated excellent improvements for simpler classification setups, the complexity of detection poses new challenges in the form of regression, region proposals and less voluminous labels. We address this through several innovations such as a weighted cross-entropy loss to address class imbalance, a teacher bounded loss to handle the regression component and adaptation layers to better learn from intermediate teacher distributions. We conduct comprehensive empirical evaluation with different distillation configurations over multiple datasets including PASCAL, KITTI, ILSVRC and MS-COCO. Our results show consistent improvement in accuracy-speed trade-offs for modern multi-class detection models.},
  isbn = {978-1-5108-6096-4},
  file = {C:\Users\mospr\Zotero\storage\BW7388AA\Chen et al. - 2017 - Learning efficient object detection models with kn.pdf}
}

@online{chenLearningStudentNetworks2018,
  title = {Learning {{Student Networks}} via {{Feature Embedding}}},
  author = {Chen, Hanting and Wang, Yunhe and Xu, Chang and Xu, Chao and Tao, Dacheng},
  date = {2018-12-16},
  eprint = {1812.06597},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1812.06597},
  url = {http://arxiv.org/abs/1812.06597},
  urldate = {2023-09-08},
  abstract = {Deep convolutional neural networks have been widely used in numerous applications, but their demanding storage and computational resource requirements prevent their applications on mobile devices. Knowledge distillation aims to optimize a portable student network by taking the knowledge from a well-trained heavy teacher network. Traditional teacher-student based methods used to rely on additional fully-connected layers to bridge intermediate layers of teacher and student networks, which brings in a large number of auxiliary parameters. In contrast, this paper aims to propagate information from teacher to student without introducing new variables which need to be optimized. We regard the teacher-student paradigm from a new perspective of feature embedding. By introducing the locality preserving loss, the student network is encouraged to generate the low-dimensional features which could inherit intrinsic properties of their corresponding high-dimensional features from teacher network. The resulting portable network thus can naturally maintain the performance as that of the teacher network. Theoretical analysis is provided to justify the lower computation complexity of the proposed method. Experiments on benchmark datasets and well-trained networks suggest that the proposed algorithm is superior to state-of-the-art teacher-student learning methods in terms of computational and storage complexity.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\9BU8U63R\\Chen et al. - 2018 - Learning Student Networks via Feature Embedding.pdf;C\:\\Users\\mospr\\Zotero\\storage\\3EB4QGGA\\1812.html}
}

@article{chenLearningStudentNetworks2021,
  title = {Learning {{Student Networks}} via {{Feature Embedding}}},
  author = {Chen, Hanting and Wang, Yunhe and Xu, Chang and Xu, Chao and Tao, Dacheng},
  date = {2021-01},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {1},
  pages = {25--35},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.2970494},
  url = {https://ieeexplore.ieee.org/abstract/document/9007474},
  urldate = {2023-10-05},
  abstract = {Deep convolutional neural networks have been widely used in numerous applications, but their demanding storage and computational resource requirements prevent their applications on mobile devices. Knowledge distillation aims to optimize a portable student network by taking the knowledge from a well-trained heavy teacher network. Traditional teacher-student-based methods used to rely on additional fully connected layers to bridge intermediate layers of teacher and student networks, which brings in a large number of auxiliary parameters. In contrast, this article aims to propagate information from teacher to student without introducing new variables that need to be optimized. We regard the teacher-student paradigm from a new perspective of feature embedding. By introducing the locality preserving loss, the student network is encouraged to generate the low-dimensional features that could inherit intrinsic properties of their corresponding high-dimensional features from the teacher network. The resulting portable network, thus, can naturally maintain the performance as that of the teacher network. Theoretical analysis is provided to justify the lower computation complexity of the proposed method. Experiments on benchmark data sets and well-trained networks suggest that the proposed algorithm is superior to state-of-the-art teacher-student learning methods in terms of computational and storage complexity.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  file = {C:\Users\mospr\Zotero\storage\LTXMCJYG\Chen et al. - 2021 - Learning Student Networks via Feature Embedding.pdf}
}

@online{chernFacToolFactualityDetection2023,
  title = {{{FacTool}}: {{Factuality Detection}} in {{Generative AI}} -- {{A Tool Augmented Framework}} for {{Multi-Task}} and {{Multi-Domain Scenarios}}},
  shorttitle = {{{FacTool}}},
  author = {Chern, I.-Chun and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Kehua and Zhou, Chunting and He, Junxian and Neubig, Graham and Liu, Pengfei},
  date = {2023-07-25},
  url = {https://arxiv.org/abs/2307.13528v2},
  urldate = {2023-10-08},
  abstract = {The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\mospr\Zotero\storage\83994MCE\Chern et al. - 2023 - FacTool Factuality Detection in Generative AI -- .pdf}
}

@online{chiesurinDangersTrustingStochastic2023,
  title = {The {{Dangers}} of Trusting {{Stochastic Parrots}}: {{Faithfulness}} and {{Trust}} in {{Open-domain Conversational Question Answering}}},
  shorttitle = {The {{Dangers}} of Trusting {{Stochastic Parrots}}},
  author = {Chiesurin, Sabrina and Dimakopoulos, Dimitris and Cabezudo, Marco Antonio Sobrevilla and Eshghi, Arash and Papaioannou, Ioannis and Rieser, Verena and Konstas, Ioannis},
  date = {2023-05-25},
  eprint = {2305.16519},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.16519},
  url = {http://arxiv.org/abs/2305.16519},
  urldate = {2023-10-08},
  abstract = {Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. "unfaithful" with respect to a rationale as retrieved from a knowledge base. In this paper, we show that task-based systems which exhibit certain advanced linguistic dialog behaviors, such as lexical alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred. We use open-domain question answering systems as our test-bed for task based dialog generation and compare several open- and closed-book models. Our results highlight the danger of systems that appear to be trustworthy by parroting user input while providing an unfaithful response.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\ZW4UFTQ9\\Chiesurin et al. - 2023 - The Dangers of trusting Stochastic Parrots Faithf.pdf;C\:\\Users\\mospr\\Zotero\\storage\\HYAYG6BD\\2305.html}
}

@inproceedings{choiLimitNetworkQuantization2016,
  title = {Towards the {{Limit}} of {{Network Quantization}}},
  author = {Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
  date = {2016-11-04},
  url = {https://openreview.net/forum?id=rJ8uNptgl},
  urldate = {2023-10-05},
  abstract = {Network quantization is one of network compression techniques to reduce the redundancy of deep neural networks. It reduces the number of distinct network parameter values by quantization in order to save the storage for them. In this paper, we design network quantization schemes that minimize the performance loss due to quantization given a compression ratio constraint. We analyze the quantitative relation of quantization errors to the neural network loss function and identify that the Hessian-weighted distortion measure is locally the right objective function for the optimization of network quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize. When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm. Finally, using the simple uniform quantization followed by Huffman coding, we show from our experiments that the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet, 32-layer ResNet and AlexNet, respectively.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\4VMGNB9P\Choi et al. - 2016 - Towards the Limit of Network Quantization.pdf}
}

@online{choiLimitNetworkQuantization2017,
  title = {Towards the {{Limit}} of {{Network Quantization}}},
  author = {Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
  date = {2017-11-13},
  eprint = {1612.01543},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1612.01543},
  url = {http://arxiv.org/abs/1612.01543},
  urldate = {2023-09-06},
  abstract = {Network quantization is one of network compression techniques to reduce the redundancy of deep neural networks. It reduces the number of distinct network parameter values by quantization in order to save the storage for them. In this paper, we design network quantization schemes that minimize the performance loss due to quantization given a compression ratio constraint. We analyze the quantitative relation of quantization errors to the neural network loss function and identify that the Hessian-weighted distortion measure is locally the right objective function for the optimization of network quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize. When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm. Finally, using the simple uniform quantization followed by Huffman coding, we show from our experiments that the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet, 32-layer ResNet and AlexNet, respectively.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\XKD84S88\\Choi et al. - 2017 - Towards the Limit of Network Quantization.pdf;C\:\\Users\\mospr\\Zotero\\storage\\LUNY3T3T\\1612.html}
}

@article{choudharyComprehensiveSurveyModel2020,
  title = {A Comprehensive Survey on Model Compression and Acceleration},
  author = {Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan},
  date = {2020-10-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {53},
  number = {7},
  pages = {5113--5155},
  issn = {1573-7462},
  doi = {10.1007/s10462-020-09816-7},
  url = {https://doi.org/10.1007/s10462-020-09816-7},
  urldate = {2023-09-05},
  abstract = {In recent years, machine learning (ML) and deep learning (DL) have shown remarkable improvement in computer vision, natural language processing, stock prediction, forecasting, and audio processing to name a few. The size of the trained DL model is large for these complex tasks, which makes it difficult to deploy on resource-constrained devices. For instance, size of the pre-trained VGG16 model trained on the ImageNet dataset is more than 500~MB. Resource-constrained devices such as mobile phones and internet of things devices have limited memory and less computation power. For real-time applications, the trained models should be deployed on resource-constrained devices. Popular convolutional neural network models have millions of parameters that leads to increase in the size of the trained model. Hence, it becomes essential to compress and accelerate these models before deploying on resource-constrained devices while making the least compromise with the model accuracy. It is a challenging task to retain the same accuracy after compressing the model. To address this challenge, in the last couple of years many researchers have suggested different techniques for model compression and acceleration. In this paper, we have presented a survey of various techniques suggested for compressing and accelerating the ML and DL models. We have also discussed the challenges of the existing techniques and have provided future research directions in the field.},
  langid = {english},
  keywords = {CNN,Deep learning,Efficient neural networks,Machine learning,Model compression and acceleration,Resource-constrained devices,RNN},
  file = {C:\Users\mospr\Downloads\s10462-020-09816-7.pdf}
}

@online{cordtsCityscapesDatasetSemantic2016,
  title = {The {{Cityscapes Dataset}} for {{Semantic Urban Scene Understanding}}},
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  date = {2016-04-07},
  eprint = {1604.01685},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1604.01685},
  url = {http://arxiv.org/abs/1604.01685},
  urldate = {2023-09-09},
  abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\UN78XVU5\\Cordts et al. - 2016 - The Cityscapes Dataset for Semantic Urban Scene Un.pdf;C\:\\Users\\mospr\\Zotero\\storage\\PXHKSYJU\\1604.html}
}

@inproceedings{cordtsCityscapesDatasetSemantic2016a,
  title = {The {{Cityscapes Dataset}} for {{Semantic Urban Scene Understanding}}},
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  date = {2016},
  pages = {3213--3223},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\6PJYH2WS\Cordts et al. - 2016 - The Cityscapes Dataset for Semantic Urban Scene Un.pdf}
}

@online{courbariauxTrainingDeepNeural2015,
  title = {Training Deep Neural Networks with Low Precision Multiplications},
  author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  date = {2015-09-22},
  eprint = {1412.7024},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.7024},
  url = {http://arxiv.org/abs/1412.7024},
  urldate = {2023-09-06},
  abstract = {Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\QVRATDP4\\Courbariaux et al. - 2015 - Training deep neural networks with low precision m.pdf;C\:\\Users\\mospr\\Zotero\\storage\\UICDERYY\\1412.html}
}

@article{cuiJointStructuredPruning2021,
  title = {Joint Structured Pruning and Dense Knowledge Distillation for Efficient Transformer Model Compression},
  author = {Cui, Baiyun and Li, Yingming and Zhang, Zhongfei},
  date = {2021-10-11},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {458},
  pages = {56--69},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.05.084},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221008390},
  urldate = {2023-09-07},
  abstract = {In this paper, we develop a novel Joint Model Compression (referred to as JMC) method by combining structured pruning and dense knowledge distillation techniques to significantly compress original large language model into a deep compressed shallow network. In particular, a new Direct Importance-aware Structured Pruning (referred as DISP) approach is proposed to structurally prune the redundant structures in the Transformer networks directly based on the corresponding parameter matrices in the model. Besides, a Dense Knowledge Distillation (referred to as DKD) method is developed with a many-to-one layer mapping strategy to leverage more comprehensive layer-wise linguistic knowledge for the distillation. Further, the proposed structured pruning and dense knowledge distillation are integrated together to perform the joint compression, which enables us to achieve a significant compression without sacrificing model accuracy. The extensive experimental results across four NLP tasks on seven datasets demonstrate its effectiveness and superiority to the baselines, while maintaining similar performance to original large model with further remarkable benefits for inference-time speedup and memory efficiency.},
  keywords = {Knowledge Distillation,Structured Pruning,Transformer Model Compression}
}

@online{CurriculumTemperatureKnowledge,
  title = {Curriculum {{Temperature}} for {{Knowledge Distillation}} | {{Proceedings}} of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/25236},
  urldate = {2023-10-05}
}

@online{CVPR2018Open,
  title = {{{CVPR}} 2018 {{Open Access Repository}}},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.html},
  urldate = {2023-10-05},
  file = {C:\Users\mospr\Zotero\storage\JYRGYZXA\Zhang_Deep_Mutual_Learning_CVPR_2018_paper.html}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {C:\Users\mospr\Zotero\storage\ADGSLBBG\5206848.html}
}

@inproceedings{denilPredictingParametersDeep2013,
  title = {Predicting {{Parameters}} in {{Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc' Aurelio and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2013/hash/7fec306d1e665bc9c748b5d2b99a6e97-Abstract.html},
  urldate = {2023-10-05},
  abstract = {We demonstrate that there is significant redundancy in the parameterization of   several deep learning models.  Given only a few weight values for each feature   it is possible to accurately predict the remaining values.  Moreover, we show   that not only can the parameter values be predicted, but many of them need not   be learned at all.  We train several different architectures by learning only   a small number of weights and predicting the rest.  In the best case we are   able to predict more than 95\% of the weights of a network without any drop in   accuracy.},
  file = {C:\Users\mospr\Zotero\storage\DI5WKH57\Denil et al. - 2013 - Predicting Parameters in Deep Learning.pdf}
}

@online{denilPredictingParametersDeep2014,
  title = {Predicting {{Parameters}} in {{Deep Learning}}},
  author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2014-10-27},
  eprint = {1306.0543},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1306.0543},
  url = {http://arxiv.org/abs/1306.0543},
  urldate = {2023-09-07},
  abstract = {We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95\% of the weights of a network without any drop in accuracy.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\MGSSRLHW\\Denil et al. - 2014 - Predicting Parameters in Deep Learning.pdf;C\:\\Users\\mospr\\Zotero\\storage\\JTQL544C\\1306.html}
}

@online{dettmersLLMInt88bit2022,
  title = {{{LLM}}.Int8(): 8-Bit {{Matrix Multiplication}} for {{Transformers}} at {{Scale}}},
  shorttitle = {{{LLM}}.Int8()},
  author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  date = {2022-11-10},
  eprint = {2208.07339},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.07339},
  url = {http://arxiv.org/abs/2208.07339},
  urldate = {2023-09-06},
  abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\Q4HSKJ3M\\Dettmers et al. - 2022 - LLM.int8() 8-bit Matrix Multiplication for Transf.pdf;C\:\\Users\\mospr\\Zotero\\storage\\B6NAATHA\\2208.html}
}

@online{dettmersSpQRSparseQuantizedRepresentation2023,
  title = {{{SpQR}}: {{A Sparse-Quantized Representation}} for {{Near-Lossless LLM Weight Compression}}},
  shorttitle = {{{SpQR}}},
  author = {Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  date = {2023-06-05},
  eprint = {2306.03078},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.03078},
  url = {http://arxiv.org/abs/2306.03078},
  urldate = {2023-09-05},
  abstract = {Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1\% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15\% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\GCHVUTKN\\Dettmers et al. - 2023 - SpQR A Sparse-Quantized Representation for Near-L.pdf;C\:\\Users\\mospr\\Zotero\\storage\\MJ7XZDSJ\\2306.html}
}

@online{dongHAWQV2HessianAware2019,
  title = {{{HAWQ-V2}}: {{Hessian Aware}} Trace-{{Weighted Quantization}} of {{Neural Networks}}},
  shorttitle = {{{HAWQ-V2}}},
  author = {Dong, Zhen and Yao, Zhewei and Cai, Yaohui and Arfeen, Daiyaan and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
  date = {2019-11-09},
  eprint = {1911.03852},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.03852},
  url = {http://arxiv.org/abs/1911.03852},
  urldate = {2023-09-06},
  abstract = {Quantization is an effective method for reducing memory footprint and inference time of Neural Networks, e.g., for efficient inference in the cloud, especially at the edge. However, ultra low precision quantization could lead to significant degradation in model generalization. A promising method to address this is to perform mixed-precision quantization, where more sensitive layers are kept at higher precision. However, the search space for a mixed-precision quantization is exponential in the number of layers. Recent work has proposed HAWQ, a novel Hessian based framework, with the aim of reducing this exponential search space by using second-order information. While promising, this prior work has three major limitations: (i) HAWQV1 only uses the top Hessian eigenvalue as a measure of sensitivity and do not consider the rest of the Hessian spectrum; (ii) HAWQV1 approach only provides relative sensitivity of different layers and therefore requires a manual selection of the mixed-precision setting; and (iii) HAWQV1 does not consider mixed-precision activation quantization. Here, we present HAWQV2 which addresses these shortcomings. For (i), we perform a theoretical analysis showing that a better sensitivity metric is to compute the average of all of the Hessian eigenvalues. For (ii), we develop a Pareto frontier based method for selecting the exact bit precision of different layers without any manual selection. For (iii), we extend the Hessian analysis to mixed-precision activation quantization. We have found this to be very beneficial for object detection. We show that HAWQV2 achieves new state-of-the-art results for a wide range of tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\SDGQ2YTR\\Dong et al. - 2019 - HAWQ-V2 Hessian Aware trace-Weighted Quantization.pdf;C\:\\Users\\mospr\\Zotero\\storage\\45X9XA93\\1911.html}
}

@inproceedings{dongHAWQV2HessianAware2020,
  title = {{{HAWQ-V2}}: {{Hessian Aware}} Trace-{{Weighted Quantization}} of {{Neural Networks}}},
  shorttitle = {{{HAWQ-V2}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dong, Zhen and Yao, Zhewei and Arfeen, Daiyaan and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  date = {2020},
  volume = {33},
  pages = {18518--18529},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/d77c703536718b95308130ff2e5cf9ee-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Quantization is an effective method for reducing memory footprint and inference time of Neural Networks. However, ultra low precision quantization could lead to significant degradation in model accuracy. A promising method to address this is to perform mixed-precision quantization, where more sensitive layers are kept at higher precision. However, the search space for a mixed-precision quantization is exponential in the number of layers. Recent work has proposed a novel Hessian based framework, with the aim of reducing this exponential search space by using second-order information. While promising, this prior work has three major limitations: (i) they only use a heuristic metric based on top Hessian eigenvalue as a measure of sensitivity and do not consider the rest of the Hessian spectrum; (ii) their approach only provides relative sensitivity of different layers and therefore requires a manual selection of the mixed-precision setting; and (iii) they do not consider mixed-precision activation quantization. Here, we present HAWQ-V2 which addresses these shortcomings. For (i), we theoretically prove that the right sensitivity metric is the average Hessian trace, instead of just top Hessian eigenvalue. For (ii), we develop a Pareto frontier based method for automatic bit precision selection of different layers without any manual intervention. For (iii), we develop the first Hessian based analysis for mixed-precision activation quantization, which is very beneficial for object detection. We show that HAWQ-V2 achieves new state-of-the-art results for a wide range of tasks. In particular, we present quantization results for InceptionV3, ResNet50, and SqueezeNext, all without any manual bit selection. Furthermore, we present results for object detection on Microsoft COCO, where we achieve 2.6 higher mAP than direct uniform quantization and 1.6 higher mAP than the recently proposed method of FQN, with a smaller model size of 17.9MB.},
  file = {C:\Users\mospr\Zotero\storage\UB4YWASD\Dong et al. - 2020 - HAWQ-V2 Hessian Aware trace-Weighted Quantization.pdf}
}

@inproceedings{fabbriQAFactEvalImprovedQABased2022,
  title = {{{QAFactEval}}: {{Improved QA-Based Factual Consistency Evaluation}} for {{Summarization}}},
  shorttitle = {{{QAFactEval}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Fabbri, Alexander and Wu, Chien-Sheng and Liu, Wenhao and Xiong, Caiming},
  date = {2022-07},
  pages = {2587--2601},
  publisher = {{Association for Computational Linguistics}},
  location = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.187},
  url = {https://aclanthology.org/2022.naacl-main.187},
  urldate = {2023-10-08},
  abstract = {Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the components of a QA-based metric, especially question generation and answerability classification, is critical to performance. Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14\% average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric. Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost.},
  eventtitle = {{{NAACL-HLT}} 2022},
  file = {C:\Users\mospr\Zotero\storage\GJM9KFHY\Fabbri et al. - 2022 - QAFactEval Improved QA-Based Factual Consistency .pdf}
}

@online{fanEvaluatingFactualConsistency2023,
  title = {Evaluating {{Factual Consistency}} of {{Texts}} with {{Semantic Role Labeling}}},
  author = {Fan, Jing and Aumiller, Dennis and Gertz, Michael},
  date = {2023-05-22},
  eprint = {2305.13309},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.13309},
  url = {http://arxiv.org/abs/2305.13309},
  urldate = {2023-10-07},
  abstract = {Automated evaluation of text generation systems has recently seen increasing attention, particularly checking whether generated text stays truthful to input sources. Existing methods frequently rely on an evaluation using task-specific language models, which in turn allows for little interpretability of generated scores. We introduce SRLScore, a reference-free evaluation metric designed with text summarization in mind. Our approach generates fact tuples constructed from Semantic Role Labels, applied to both input and summary texts. A final factuality score is computed by an adjustable scoring mechanism, which allows for easy adaption of the method across domains. Correlation with human judgments on English summarization datasets shows that SRLScore is competitive with state-of-the-art methods and exhibits stable generalization across datasets without requiring further training or hyperparameter tuning. We experiment with an optional co-reference resolution step, but find that the performance boost is mostly outweighed by the additional compute required. Our metric is available online at https://github.com/heyjing/SRLScore.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\TB9ALNWC\\Fan et al. - 2023 - Evaluating Factual Consistency of Texts with Seman.pdf;C\:\\Users\\mospr\\Zotero\\storage\\LD9X7877\\2305.html}
}

@online{fangDepGraphAnyStructural2023,
  title = {{{DepGraph}}: {{Towards Any Structural Pruning}}},
  shorttitle = {{{DepGraph}}},
  author = {Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  date = {2023-03-23},
  eprint = {2301.12900},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.12900},
  url = {http://arxiv.org/abs/2301.12900},
  urldate = {2023-09-07},
  abstract = {Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and \{fully automatic\} method, \textbackslash emph\{Dependency Graph\} (DepGraph), to explicitly model the dependency between layers and comprehensively group coupled parameters for pruning. In this work, we extensively evaluate our method on several architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and Vision transformer for images, GAT for graph, DGCNN for 3D point cloud, alongside LSTM for language, and demonstrate that, even with a simple norm-based criterion, the proposed method consistently yields gratifying performances.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\DFE39QAJ\\Fang et al. - 2023 - DepGraph Towards Any Structural Pruning.pdf;C\:\\Users\\mospr\\Zotero\\storage\\GT8G87CE\\2301.html}
}

@inproceedings{fangDepGraphAnyStructural2023a,
  title = {{{DepGraph}}: {{Towards Any Structural Pruning}}},
  shorttitle = {{{DepGraph}}},
  author = {Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  date = {2023},
  pages = {16091--16101},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\8SJIBTP9\Fang et al. - 2023 - DepGraph Towards Any Structural Pruning.pdf}
}

@online{fanTrainingQuantizationNoise2021,
  title = {Training with {{Quantization Noise}} for {{Extreme Model Compression}}},
  author = {Fan, Angela and Stock, Pierre and Graham, Benjamin and Grave, Edouard and Gribonval, Remi and Jegou, Herve and Joulin, Armand},
  date = {2021-02-28},
  eprint = {2004.07320},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2004.07320},
  url = {http://arxiv.org/abs/2004.07320},
  urldate = {2023-09-06},
  abstract = {We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training, where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator. In this paper, we extend this approach to work beyond int8 fixed-point quantization with extreme compression methods where the approximations introduced by STE are severe, such as Product Quantization. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5\% accuracy on MNLI by compressing RoBERTa to 14MB and 80.0 top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3MB.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\N7P6WN63\\Fan et al. - 2021 - Training with Quantization Noise for Extreme Model.pdf;C\:\\Users\\mospr\\Zotero\\storage\\ARYPSBSS\\2004.html}
}

@online{feldmanTrappingLLMHallucinations2023,
  title = {Trapping {{LLM Hallucinations Using Tagged Context Prompts}}},
  author = {Feldman, Philip and Foulds, James R. and Pan, Shimei},
  date = {2023-06-09},
  eprint = {2306.06085},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.06085},
  url = {http://arxiv.org/abs/2306.06085},
  urldate = {2023-10-07},
  abstract = {Recent advances in large language models (LLMs), such as ChatGPT, have led to highly sophisticated conversation agents. However, these models suffer from "hallucinations," where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with AI-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when LLMs perform outside their domain knowledge, and ensuring users receive accurate information. We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated URLs as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within contexts impacted model responses and were able to eliminate hallucinations in responses with 98.88\% effectiveness.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,I.2.7,K.4.2},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\26BYEN8E\\Feldman et al. - 2023 - Trapping LLM Hallucinations Using Tagged Context P.pdf;C\:\\Users\\mospr\\Zotero\\storage\\7I2DUUKY\\2306.html}
}

@inproceedings{frankleLotteryTicketHypothesis2018,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=rJl-b3RcF7},
  urldate = {2023-10-05},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\D9FJTVT2\Frankle and Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf}
}

@online{frankleLotteryTicketHypothesis2019,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  date = {2019-03-04},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.03635},
  url = {http://arxiv.org/abs/1803.03635},
  urldate = {2023-09-07},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\VPHHV2JS\\Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf;C\:\\Users\\mospr\\Zotero\\storage\\8PD95WFS\\1803.html}
}

@online{frankleStabilizingLotteryTicket2020,
  title = {Stabilizing the {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
  date = {2020-07-20},
  eprint = {1903.01611},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1903.01611},
  url = {http://arxiv.org/abs/1903.01611},
  urldate = {2023-09-07},
  abstract = {Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the "lottery ticket hypothesis" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1\% to 7\% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80\% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork "stability," finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\T9W2Q85Z\\Frankle et al. - 2020 - Stabilizing the Lottery Ticket Hypothesis.pdf;C\:\\Users\\mospr\\Zotero\\storage\\NM84UFJA\\1903.html}
}

@online{frantarGPTQAccuratePostTraining2023,
  title = {{{GPTQ}}: {{Accurate Post-Training Quantization}} for {{Generative Pre-trained Transformers}}},
  shorttitle = {{{GPTQ}}},
  author = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  date = {2023-03-22},
  eprint = {2210.17323},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.17323},
  url = {http://arxiv.org/abs/2210.17323},
  urldate = {2023-09-06},
  abstract = {Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\77CH5696\\Frantar et al. - 2023 - GPTQ Accurate Post-Training Quantization for Gene.pdf;C\:\\Users\\mospr\\Zotero\\storage\\WBC2Z38P\\2210.html}
}

@article{frantarOptimalBrainCompression2022,
  title = {Optimal {{Brain Compression}}: {{A Framework}} for {{Accurate Post-Training Quantization}} and {{Pruning}}},
  shorttitle = {Optimal {{Brain Compression}}},
  author = {Frantar, Elias and Alistarh, Dan},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {4475--4488},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/1caf09c9f4e6b0150b06a07e77f2710c-Abstract-Conference.html},
  urldate = {2023-10-05},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\EZBD2MFN\Frantar and Alistarh - 2022 - Optimal Brain Compression A Framework for Accurat.pdf}
}

@online{frantarOptimalBrainCompression2023,
  title = {Optimal {{Brain Compression}}: {{A Framework}} for {{Accurate Post-Training Quantization}} and {{Pruning}}},
  shorttitle = {Optimal {{Brain Compression}}},
  author = {Frantar, Elias and Singh, Sidak Pal and Alistarh, Dan},
  date = {2023-01-08},
  eprint = {2208.11580},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.11580},
  url = {http://arxiv.org/abs/2208.11580},
  urldate = {2023-09-06},
  abstract = {We consider the problem of model compression for deep neural networks (DNNs) in the challenging one-shot/post-training setting, in which we are given an accurate trained model, and must compress it without any retraining, based only on a small amount of calibration input data. This problem has become popular in view of the emerging software and hardware support for executing models compressed via pruning and/or quantization with speedup, and well-performing solutions have been proposed independently for both compression approaches. In this paper, we introduce a new compression framework which covers both weight pruning and quantization in a unified setting, is time- and space-efficient, and considerably improves upon the practical performance of existing post-training methods. At the technical level, our approach is based on an exact and efficient realization of the classical Optimal Brain Surgeon (OBS) framework of [LeCun, Denker, and Solla, 1990] extended to also cover weight quantization at the scale of modern DNNs. From the practical perspective, our experimental results show that it can improve significantly upon the compression-accuracy trade-offs of existing post-training methods, and that it can enable the accurate compound application of both pruning and quantization in a post-training setting.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\99FS8RXU\\Frantar et al. - 2023 - Optimal Brain Compression A Framework for Accurat.pdf;C\:\\Users\\mospr\\Zotero\\storage\\FFS5ZJU9\\2208.html}
}

@online{frantarOptimalBrainCompression2023a,
  title = {Optimal {{Brain Compression}}: {{A Framework}} for {{Accurate Post-Training Quantization}} and {{Pruning}}},
  shorttitle = {Optimal {{Brain Compression}}},
  author = {Frantar, Elias and Singh, Sidak Pal and Alistarh, Dan},
  date = {2023-01-08},
  eprint = {2208.11580},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.11580},
  url = {http://arxiv.org/abs/2208.11580},
  urldate = {2023-09-06},
  abstract = {We consider the problem of model compression for deep neural networks (DNNs) in the challenging one-shot/post-training setting, in which we are given an accurate trained model, and must compress it without any retraining, based only on a small amount of calibration input data. This problem has become popular in view of the emerging software and hardware support for executing models compressed via pruning and/or quantization with speedup, and well-performing solutions have been proposed independently for both compression approaches. In this paper, we introduce a new compression framework which covers both weight pruning and quantization in a unified setting, is time- and space-efficient, and considerably improves upon the practical performance of existing post-training methods. At the technical level, our approach is based on an exact and efficient realization of the classical Optimal Brain Surgeon (OBS) framework of [LeCun, Denker, and Solla, 1990] extended to also cover weight quantization at the scale of modern DNNs. From the practical perspective, our experimental results show that it can improve significantly upon the compression-accuracy trade-offs of existing post-training methods, and that it can enable the accurate compound application of both pruning and quantization in a post-training setting.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\SFKVHUK5\\Frantar et al. - 2023 - Optimal Brain Compression A Framework for Accurat.pdf;C\:\\Users\\mospr\\Zotero\\storage\\QY7DVB3J\\2208.html}
}

@online{galitskyTruthOMeterCollaboratingLLM2023,
  title = {Truth-{{O-Meter}}: {{Collaborating}} with {{LLM}} in {{Fighting}} Its {{Hallucinations}}},
  shorttitle = {Truth-{{O-Meter}}},
  author = {Galitsky, Boris A.},
  date = {2023-07-25},
  number = {2023071723},
  eprint = {2023071723},
  eprinttype = {Preprints},
  doi = {10.20944/preprints202307.1723.v1},
  url = {https://www.preprints.org/manuscript/202307.1723/v1},
  urldate = {2023-10-07},
  abstract = {A text obtained by a Large Language Model (LLM) such as GPT4 usually has issues in terms of incorrectness and hallucinations. We build a fact-checking system 'Truth-O-Meter' which identifies wrong facts, comparing the generation results with the web and other sources of information, and suggests corrections. Text mining and web mining techniques are leveraged to identify correct corresponding sentences; also, the syntactic and semantic generalization procedure adopted to the content improvement task. To handle inconsistent sources while fact-checking, we rely on an argumentation analysis in the form of defeasible logic programming. We compare our fact checking engine with competitive approach based on reinforcement learning on top of LLM or token-based hallucination detection. It is observed that LLM content can be substantially improved for factual correctness and meaningfulness.},
  langid = {english},
  pubstate = {preprint},
  keywords = {fact-checking,hallucination,Large Language Model,multiple inconsistent sources},
  file = {C:\Users\mospr\Zotero\storage\QIB38X3V\Galitsky - 2023 - Truth-O-Meter Collaborating with LLM in Fighting .pdf}
}

@online{gholamiSurveyQuantizationMethods2021,
  title = {A {{Survey}} of {{Quantization Methods}} for {{Efficient Neural Network Inference}}},
  author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
  date = {2021-03-01},
  doi = {10.48550/arXiv.2103.13630},
  url = {https://ui.adsabs.harvard.edu/abs/2021arXiv210313630G},
  urldate = {2023-09-06},
  abstract = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.},
  organization = {{arXiv e-prints}},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ADS Bibcode: 2021arXiv210313630G},
  file = {C:\Users\mospr\Zotero\storage\353TPEQA\Gholami et al. - 2021 - A Survey of Quantization Methods for Efficient Neu.pdf}
}

@online{gouCRITICLargeLanguage2023,
  title = {{{CRITIC}}: {{Large Language Models Can Self-Correct}} with {{Tool-Interactive Critiquing}}},
  shorttitle = {{{CRITIC}}},
  author = {Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Yang, Yujiu and Duan, Nan and Chen, Weizhu},
  date = {2023-05-19},
  url = {https://arxiv.org/abs/2305.11738v2},
  urldate = {2023-10-08},
  abstract = {Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\mospr\Zotero\storage\RWR5PGZV\Gou et al. - 2023 - CRITIC Large Language Models Can Self-Correct wit.pdf}
}

@article{gouKnowledgeDistillationSurvey2021,
  title = {Knowledge {{Distillation}}: {{A Survey}}},
  shorttitle = {Knowledge {{Distillation}}},
  author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
  date = {2021-06-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {129},
  number = {6},
  pages = {1789--1819},
  issn = {1573-1405},
  doi = {10.1007/s11263-021-01453-z},
  url = {https://doi.org/10.1007/s11263-021-01453-z},
  urldate = {2023-09-05},
  abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
  langid = {english},
  keywords = {Deep neural networks,Knowledge distillation,Knowledge transfer,Model compression,Teacher–student architecture},
  file = {C:\Users\mospr\Zotero\storage\RL2BWDS5\Gou et al. - 2021 - Knowledge Distillation A Survey.pdf}
}

@article{gouKnowledgeDistillationSurvey2021a,
  title = {Knowledge {{Distillation}}: {{A Survey}}},
  shorttitle = {Knowledge {{Distillation}}},
  author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen John and Tao, Dacheng},
  date = {2021-06},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {129},
  number = {6},
  eprint = {2006.05525},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {1789--1819},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-021-01453-z},
  url = {http://arxiv.org/abs/2006.05525},
  urldate = {2023-09-05},
  abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CE5RDDDV\\Gou et al. - 2021 - Knowledge Distillation A Survey.pdf;C\:\\Users\\mospr\\Zotero\\storage\\FY6DHDYG\\2006.html}
}

@online{goyalSNaCCoherenceError2022,
  title = {{{SNaC}}: {{Coherence Error Detection}} for {{Narrative Summarization}}},
  shorttitle = {{{SNaC}}},
  author = {Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
  date = {2022-05-19},
  url = {https://arxiv.org/abs/2205.09641v2},
  urldate = {2023-10-08},
  abstract = {Progress in summarizing long texts is inhibited by the lack of appropriate evaluation frameworks. When a long summary must be produced to appropriately cover the facets of that text, that summary needs to present a coherent narrative to be understandable by a reader, but current automatic and human evaluation methods fail to identify gaps in coherence. In this work, we introduce SNaC, a narrative coherence evaluation framework rooted in fine-grained annotations for long summaries. We develop a taxonomy of coherence errors in generated narrative summaries and collect span-level annotations for 6.6k sentences across 150 book and movie screenplay summaries. Our work provides the first characterization of coherence errors generated by state-of-the-art summarization models and a protocol for eliciting coherence judgments from crowd annotators. Furthermore, we show that the collected annotations allow us to train a strong classifier for automatically localizing coherence errors in generated summaries as well as benchmarking past work in coherence modeling. Finally, our SNaC framework can support future work in long document summarization and coherence evaluation, including improved summarization modeling and post-hoc summary correction.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\mospr\Zotero\storage\L9KJAT6E\Goyal et al. - 2022 - SNaC Coherence Error Detection for Narrative Summ.pdf}
}

@inproceedings{goyalSNaCCoherenceError2022a,
  title = {{{SNaC}}: {{Coherence Error Detection}} for {{Narrative Summarization}}},
  shorttitle = {{{SNaC}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
  date = {2022-12},
  pages = {444--463},
  publisher = {{Association for Computational Linguistics}},
  location = {{Abu Dhabi, United Arab Emirates}},
  doi = {10.18653/v1/2022.emnlp-main.29},
  url = {https://aclanthology.org/2022.emnlp-main.29},
  urldate = {2023-10-08},
  abstract = {Progress in summarizing long texts is inhibited by the lack of appropriate evaluation frameworks. A long summary that appropriately covers the facets of that text must also present a coherent narrative, but current automatic and human evaluation methods fail to identify gaps in coherence. In this work, we introduce SNaC, a narrative coherence evaluation framework for fine-grained annotations of long summaries. We develop a taxonomy of coherence errors in generated narrative summaries and collect span-level annotations for 6.6k sentences across 150 book and movie summaries. Our work provides the first characterization of coherence errors generated by state-of-the-art summarization models and a protocol for eliciting coherence judgments from crowdworkers. Furthermore, we show that the collected annotations allow us to benchmark past work in coherence modeling and train a strong classifier for automatically localizing coherence errors in generated summaries. Finally, our SNaC framework can support future work in long document summarization and coherence evaluation, including improved summarization modeling and post-hoc summary correction.},
  eventtitle = {{{EMNLP}} 2022},
  file = {C:\Users\mospr\Zotero\storage\I2BRGE79\Goyal et al. - 2022 - SNaC Coherence Error Detection for Narrative Summ.pdf}
}

@online{guKnowledgeDistillationLarge2023,
  title = {Knowledge {{Distillation}} of {{Large Language Models}}},
  author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  date = {2023-06-14},
  eprint = {2306.08543},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.08543},
  url = {http://arxiv.org/abs/2306.08543},
  urldate = {2023-09-08},
  abstract = {Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge from white-box generative LLMs is still under-explored, which becomes more and more important with the prosperity of LLMs. In this work, we propose MiniLLM that distills smaller language models from generative larger language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. Extensive experiments in the instruction-following setting show that the MiniLLM models generate more precise responses with the higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance. Our method is also scalable for different model families with 120M to 13B parameters. We will release our code and model checkpoints at https://aka.ms/MiniLLM.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\WRLKJCPL\\Gu et al. - 2023 - Knowledge Distillation of Large Language Models.pdf;C\:\\Users\\mospr\\Zotero\\storage\\22UMCNLP\\2306.html}
}

@online{guoDynamicNetworkSurgery2016,
  title = {Dynamic {{Network Surgery}} for {{Efficient DNNs}}},
  author = {Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  date = {2016-11-09},
  eprint = {1608.04493},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.04493},
  url = {http://arxiv.org/abs/1608.04493},
  urldate = {2023-09-07},
  abstract = {Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of \$\textbackslash bm\{108\}\textbackslash times\$ and \$\textbackslash bm\{17.7\}\textbackslash times\$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\GG7M6EQH\\Guo et al. - 2016 - Dynamic Network Surgery for Efficient DNNs.pdf;C\:\\Users\\mospr\\Zotero\\storage\\5ZAGGKKV\\1608.html}
}

@inproceedings{guoDynamicNetworkSurgery2016a,
  title = {Dynamic {{Network Surgery}} for {{Efficient DNNs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  date = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2016/hash/2823f4797102ce1a1aec05359cc16dd9-Abstract.html},
  urldate = {2023-10-05},
  file = {C:\Users\mospr\Zotero\storage\XPMZF4GZ\Guo et al. - 2016 - Dynamic Network Surgery for Efficient DNNs.pdf}
}

@online{guSearchBetterStudents2020,
  title = {Search for {{Better Students}} to {{Learn Distilled Knowledge}}},
  author = {Gu, Jindong and Tresp, Volker},
  date = {2020-01-30},
  eprint = {2001.11612},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2001.11612},
  url = {http://arxiv.org/abs/2001.11612},
  urldate = {2023-09-08},
  abstract = {Knowledge Distillation, as a model compression technique, has received great attention. The knowledge of a well-performed teacher is distilled to a student with a small architecture. The architecture of the small student is often chosen to be similar to their teacher's, with fewer layers or fewer channels, or both. However, even with the same number of FLOPs or parameters, the students with different architecture can achieve different generalization ability. The configuration of a student architecture requires intensive network architecture engineering. In this work, instead of designing a good student architecture manually, we propose to search for the optimal student automatically. Based on L1-norm optimization, a subgraph from the teacher network topology graph is selected as a student, the goal of which is to minimize the KL-divergence between student's and teacher's outputs. We verify the proposal on CIFAR10 and CIFAR100 datasets. The empirical experiments show that the learned student architecture achieves better performance than ones specified manually. We also visualize and understand the architecture of the found student.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\WIZZC3TE\\Gu and Tresp - 2020 - Search for Better Students to Learn Distilled Know.pdf;C\:\\Users\\mospr\\Zotero\\storage\\4N8WMJWW\\2001.html}
}

@article{guSearchBetterStudents2020a,
  title = {Search for {{Better Students}} to {{Learn Distilled Knowledge}}},
  author = {Gu, Jindong and Tresp, Volker},
  date = {2020},
  journaltitle = {ECAI 2020: 24th European Conference on Artificial Intelligence},
  pages = {1159--1165},
  publisher = {{Ludwig-Maximilians-Universität München}},
  issn = {0922-6389},
  doi = {10.3233/FAIA200214},
  url = {https://epub.ub.uni-muenchen.de/89076/},
  urldate = {2023-10-05},
  abstract = {Knowledge Distillation, as a model compression technique, has received great attention. The knowledge of a well-performed teacher is distilled to a student with a small architecture. The architecture of the small student is often chosen to be similar to their teacher's, with fewer layers or fewer channels, or both. However, even with the same number of FLOPs or parameters, the students with different architecture can achieve different generalization ability. The configuration of a student architecture requires intensive network architecture engineering. In this work, instead of designing a good student architecture manually, we propose to search for the optimal student automatically. Based on L1-norm optimization, a subgraph from the teacher network topology graph is selected as a student, the goal of which is to minimize the KL-divergence between student's and teacher's outputs. We verify the proposal on CIFAR10 and CIFAR100 datasets. The empirical experiments show that the learned student architecture achieves better performance than ones specified manually. We also visualize and understand the architecture of the found student.},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\MKDD8FFP\89076.html}
}

@online{hanLearningBothWeights2015,
  title = {Learning Both {{Weights}} and {{Connections}} for {{Efficient Neural Networks}}},
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
  date = {2015-10-30},
  eprint = {1506.02626},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.02626},
  url = {http://arxiv.org/abs/1506.02626},
  urldate = {2023-09-05},
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\8MME6U36\\Han et al. - 2015 - Learning both Weights and Connections for Efficien.pdf;C\:\\Users\\mospr\\Zotero\\storage\\ZVRXIEA4\\1506.html}
}

@inproceedings{hanLearningBothWeights2015a,
  title = {Learning Both {{Weights}} and {{Connections}} for {{Efficient Neural Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
  date = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9×, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13×, from 138 million to 10.3 million, again with no loss of accuracy.},
  file = {C:\Users\mospr\Zotero\storage\FARYQE5Y\Han et al. - 2015 - Learning both Weights and Connections for Efficien.pdf}
}

@inproceedings{hansonComparingBiasesMinimal1988,
  title = {Comparing {{Biases}} for {{Minimal Network Construction}} with {{Back-Propagation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hanson, Stephen and Pratt, Lorien},
  date = {1988},
  volume = {1},
  publisher = {{Morgan-Kaufmann}},
  url = {https://proceedings.neurips.cc/paper/1988/hash/1c9ac0159c94d8d0cbedc973445af2da-Abstract.html},
  urldate = {2023-09-07},
  abstract = {learning},
  file = {C:\Users\mospr\Zotero\storage\YG6E9PMQ\Hanson and Pratt - 1988 - Comparing Biases for Minimal Network Construction .pdf}
}

@inproceedings{hassibiSecondOrderDerivatives1992,
  title = {Second {{Order Derivatives}} for {{Network Pruning}}: {{Optimal Brain Surgeon}}},
  shorttitle = {Second {{Order Derivatives}} for {{Network Pruning}}},
  author = {Hassibi, B. and Stork, D.},
  date = {1992-11-30},
  url = {https://www.semanticscholar.org/paper/Second-Order-Derivatives-for-Network-Pruning%3A-Brain-Hassibi-Stork/a42954d4b9d0ccdf1036e0af46d87a01b94c3516},
  urldate = {2023-09-07},
  abstract = {We investigate the use of information from all second order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Solla, 1990], which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-1 from training data and structural information of the net. OBS permits a 90\%, a 76\%, and a 62\% reduction in weights over backpropagation with weight decay on three benchmark MONK's problems [Thrun et al., 1991]. Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987] used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1560 weights, yielding better generalization.},
  eventtitle = {{{NIPS}}}
}

@online{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.03385},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2023-09-09},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\KV53IRUT\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\mospr\\Zotero\\storage\\7PSCYF46\\1512.html}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {770--778},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\J6E2PNXJ\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@online{heFilterPruningGeometric2019,
  title = {Filter {{Pruning}} via {{Geometric Median}} for {{Deep Convolutional Neural Networks Acceleration}}},
  author = {He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  date = {2019-07-14},
  eprint = {1811.00250},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1811.00250},
  url = {http://arxiv.org/abs/1811.00250},
  urldate = {2023-09-07},
  abstract = {Previous works utilized ''smaller-norm-less-important'' criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1) the norm deviation of the filters should be large; (2) the minimum norm of the filters should be small. To solve this problem, we propose a novel filter pruning method, namely Filter Pruning via Geometric Median (FPGM), to compress the model regardless of those two requirements. Unlike previous methods, FPGM compresses CNN models by pruning filters with redundancy, rather than those with ''relatively less'' importance. When applied to two image classification benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces more than 52\% FLOPs on ResNet-110 with even 2.69\% relative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than 42\% FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the state-of-the-art. Code is publicly available on GitHub: https://github.com/he-y/filter-pruning-geometric-median},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\X2J4NSTH\\He et al. - 2019 - Filter Pruning via Geometric Median for Deep Convo.pdf;C\:\\Users\\mospr\\Zotero\\storage\\44U22LBQ\\1811.html}
}

@inproceedings{heFilterPruningGeometric2019a,
  title = {Filter {{Pruning}} via {{Geometric Median}} for {{Deep Convolutional Neural Networks Acceleration}}},
  author = {He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  date = {2019},
  pages = {4340--4349},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/He_Filter_Pruning_via_Geometric_Median_for_Deep_Convolutional_Neural_Networks_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\XJPTBL3H\He et al. - 2019 - Filter Pruning via Geometric Median for Deep Convo.pdf}
}

@online{heStructuredPruningDeep2023,
  title = {Structured {{Pruning}} for {{Deep Convolutional Neural Networks}}: {{A}} Survey},
  shorttitle = {Structured {{Pruning}} for {{Deep Convolutional Neural Networks}}},
  author = {He, Yang and Xiao, Lingao},
  date = {2023-03-01},
  eprint = {2303.00566},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.00566},
  url = {http://arxiv.org/abs/2303.00566},
  urldate = {2023-09-06},
  abstract = {The remarkable performance of deep Convolutional neural networks (CNNs) is generally attributed to their deeper and wider architectures, which can come with significant computational costs. Pruning neural networks has thus gained interest since it effectively lowers storage and computational costs. In contrast to weight pruning, which results in unstructured models, structured pruning provides the benefit of realistic acceleration by producing models that are friendly to hardware implementation. The special requirements of structured pruning have led to the discovery of numerous new challenges and the development of innovative solutions. This article surveys the recent progress towards structured pruning of deep CNNs. We summarize and compare the state-of-the-art structured pruning techniques with respect to filter ranking methods, regularization methods, dynamic execution, neural architecture search, the lottery ticket hypothesis, and the applications of pruning. While discussing structured pruning algorithms, we briefly introduce the unstructured pruning counterpart to emphasize their differences. Furthermore, we provide insights into potential research opportunities in the field of structured pruning. A curated list of neural network pruning papers can be found at https://github.com/he-y/Awesome-Pruning},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\SNRZJBIR\\He and Xiao - 2023 - Structured Pruning for Deep Convolutional Neural N.pdf;C\:\\Users\\mospr\\Zotero\\storage\\72PAXTE8\\2303.html}
}

@online{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1503.02531},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2023-09-05},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CBLP65WS\\Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;C\:\\Users\\mospr\\Zotero\\storage\\ESEH6WL5\\1503.html}
}

@inproceedings{horowitzComputingEnergyProblem2014,
  title = {1.1 {{Computing}}'s Energy Problem (and What We Can Do about It)},
  booktitle = {2014 {{IEEE International Solid-State Circuits Conference Digest}} of {{Technical Papers}} ({{ISSCC}})},
  author = {Horowitz, Mark},
  date = {2014-02},
  pages = {10--14},
  issn = {2376-8606},
  doi = {10.1109/ISSCC.2014.6757323},
  abstract = {Our challenge is clear: The drive for performance and the end of voltage scaling have made power, and not the number of transistors, the principal factor limiting further improvements in computing performance. Continuing to scale compute performance will require the creation and effective use of new specialized compute engines, and will require the participation of application experts to be successful. If we play our cards right, and develop the tools that allow our customers to become part of the design process, we will create a new wave of innovative and efficient computing devices.},
  eventtitle = {2014 {{IEEE International Solid-State Circuits Conference Digest}} of {{Technical Papers}} ({{ISSCC}})},
  keywords = {CMOS integrated circuits,CMOS technology,Energy efficiency,Hardware,Logic gates,Transistors,Voltage control},
  file = {C:\Users\mospr\Zotero\storage\336LH4P2\6757323.html}
}

@online{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  date = {2017-04-16},
  eprint = {1704.04861},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.04861},
  url = {http://arxiv.org/abs/1704.04861},
  urldate = {2023-09-08},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\YN2TI89Q\\Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;C\:\\Users\\mospr\\Zotero\\storage\\TCQ5I6IN\\1704.html}
}

@inproceedings{huangCoDeNetEfficientDeployment2021,
  title = {{{CoDeNet}}: {{Efficient Deployment}} of {{Input-Adaptive Object Detection}} on {{Embedded FPGAs}}},
  shorttitle = {{{CoDeNet}}},
  booktitle = {The 2021 {{ACM}}/{{SIGDA International Symposium}} on {{Field-Programmable Gate Arrays}}},
  author = {Huang, Qijing and Wang, Dequan and Dong, Zhen and Gao, Yizhao and Cai, Yaohui and Li, Tian and Wu, Bichen and Keutzer, Kurt and Wawrzynek, John},
  date = {2021-02-17},
  series = {{{FPGA}} '21},
  pages = {206--216},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3431920.3439295},
  url = {https://dl.acm.org/doi/10.1145/3431920.3439295},
  urldate = {2023-09-06},
  abstract = {Deploying deep learning models on embedded systems for computer vision tasks has been challenging due to limited compute resources and strict energy budgets. The majority of existing work focuses on accelerating image classification, while other fundamental vision problems, such as object detection, have not been adequately addressed. Compared with image classification, detection problems are more sensitive to the spatial variance of objects, and therefore, require specialized convolutions to aggregate spatial information. To address this need, recent work introduces dynamic deformable convolution to augment regular convolutions. Regular convolutions process a fixed grid of pixels across all the spatial locations in an image, while dynamic deformable convolution may access arbitrary pixels in the image with the access pattern being input-dependent and varying with spatial location. These properties lead to inefficient memory accesses of inputs with existing hardware. In this work, we harness the flexibility of FPGAs to develop a novel object detection pipeline with deformable convolutions. We show the speed-accuracy tradeoffs for a set of algorithm modifications including irregular-access versus limited-range and fixed-shape on a flexible hardware accelerator. We evaluate these algorithmic changes with corresponding hardware optimizations and show a 1.36x and 9.76x speedup respectively for the full and depthwise deformable convolution on hardware with minor accuracy loss. We then co-design a network called CoDeNet with the modified deformable convolution for object detection and quantize the network to 4-bit weights and 8-bit activations. With our high-efficiency implementation, our solution reaches 26.9 frames per second with a tiny model size of 0.76 MB while achieving 61.7 AP50 on the standard object detection dataset, Pascal VOC. With our higher-accuracy implementation, our model gets to 67.1 AP50 on Pascal VOC with only 2.9 MB of parameters--20.9x smaller but 10\% more accurate than Tiny-YOLO.},
  isbn = {978-1-4503-8218-2},
  keywords = {algorithm-hardware codesign,hardware accelerator,object detection},
  file = {C:\Users\mospr\Zotero\storage\U3UEH666\Huang et al. - 2021 - CoDeNet Efficient Deployment of Input-Adaptive Ob.pdf}
}

@online{huangZeroshotFaithfulFactual2023,
  title = {Zero-Shot {{Faithful Factual Error Correction}}},
  author = {Huang, Kung-Hsiang and Chan, Hou Pong and Ji, Heng},
  date = {2023-05-13},
  url = {https://arxiv.org/abs/2305.07982v2},
  urldate = {2023-10-08},
  abstract = {Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. Drawing on humans' ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about input claims, looks for correct answers in the given evidence, and assesses the faithfulness of each correction based on its consistency with the evidence. Our zero-shot framework outperforms fully-supervised approaches, as demonstrated by experiments on the FEVER and SciFact datasets, where our outputs are shown to be more faithful. More importantly, the decomposability nature of our framework inherently provides interpretability. Additionally, to reveal the most suitable metrics for evaluating factual error corrections, we analyze the correlation between commonly used metrics with human judgments in terms of three different dimensions regarding intelligibility and faithfulness.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\mospr\Zotero\storage\FL2BEPT8\Huang et al. - 2023 - Zero-shot Faithful Factual Error Correction.pdf}
}

@inproceedings{hubaraAccuratePostTraining2021,
  title = {Accurate {{Post Training Quantization With Small Calibration Sets}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  date = {2021-07-01},
  pages = {4466--4475},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/hubara21a.html},
  urldate = {2023-09-06},
  abstract = {Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations’ dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer or block separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations’ dynamic ranges. We suggest two flavors for our method, parallel and sequential aim for a fixed and flexible bit-width allocation. For the latter, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\% accuracy degradation — with 4-bit weights and activations in all layers, but first and last. The suggested methods are two orders of magnitude faster than the traditional Quantize Aware Training approach used for lower than 8-bit quantization. We open-sourced our code \textbackslash textit\{https://github.com/papers-submission/CalibTIP\}.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\BP75KNNV\\Hubara et al. - 2021 - Accurate Post Training Quantization With Small Cal.pdf;C\:\\Users\\mospr\\Zotero\\storage\\PHT47DSW\\Hubara et al. - 2021 - Accurate Post Training Quantization With Small Cal.pdf}
}

@online{hubaraImprovingPostTraining2020,
  title = {Improving {{Post Training Neural Quantization}}: {{Layer-wise Calibration}} and {{Integer Programming}}},
  shorttitle = {Improving {{Post Training Neural Quantization}}},
  author = {Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  date = {2020-12-14},
  eprint = {2006.10518},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2006.10518},
  url = {http://arxiv.org/abs/2006.10518},
  urldate = {2023-09-06},
  abstract = {Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\textbackslash\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. We open-sourced our code.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\WCA8RXZ5\\Hubara et al. - 2020 - Improving Post Training Neural Quantization Layer.pdf;C\:\\Users\\mospr\\Zotero\\storage\\9RRIHITI\\2006.html}
}

@article{hubaraImprovingPostTraining2020a,
  title = {Improving {{Post Training Neural Quantization}}: {{Layer-wise Calibration}} and {{Integer Programming}}},
  shorttitle = {Improving {{Post Training Neural Quantization}}},
  author = {Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  date = {2020-10-02},
  url = {https://openreview.net/forum?id=Mf4ZSXMZP7},
  urldate = {2023-10-05},
  abstract = {Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\textbackslash\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\NK5D2YDB\Hubara et al. - 2020 - Improving Post Training Neural Quantization Layer.pdf}
}

@article{jiQAScoreUnsupervisedUnreferenced2022,
  title = {{{QAScore}} -- {{An Unsupervised Unreferenced Metric}} for the {{Question Generation Evaluation}}},
  author = {Ji, Tianbo and Lyu, Chenyang and Jones, Gareth and Zhou, Liting and Graham, Yvette},
  date = {2022-10-24},
  journaltitle = {Entropy},
  shortjournal = {Entropy},
  volume = {24},
  number = {11},
  eprint = {2210.04320},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1514},
  issn = {1099-4300},
  doi = {10.3390/e24111514},
  url = {http://arxiv.org/abs/2210.04320},
  urldate = {2023-10-07},
  abstract = {Question Generation (QG) aims to automate the task of composing questions for a passage with a set of chosen answers found within the passage. In recent years, the introduction of neural generation models has resulted in substantial improvements of automatically generated questions in terms of quality, especially compared to traditional approaches that employ manually crafted heuristics. However, the metrics commonly applied in QG evaluations have been criticized for their low agreement with human judgement. We therefore propose a new reference-free evaluation metric that has the potential to provide a better mechanism for evaluating QG systems, called QAScore. Instead of fine-tuning a language model to maximize its correlation with human judgements, QAScore evaluates a question by computing the cross entropy according to the probability that the language model can correctly generate the masked words in the answer to that question. Furthermore, we conduct a new crowd-sourcing human evaluation experiment for the QG evaluation to investigate how QAScore and other metrics can correlate with human judgements. Experiments show that QAScore obtains a stronger correlation with the results of our proposed human evaluation method compared to existing traditional word-overlap-based metrics such as BLEU and ROUGE, as well as the existing pretrained-model-based metric BERTScore.},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CD59BD8Y\\Ji et al. - 2022 - QAScore -- An Unsupervised Unreferenced Metric for.pdf;C\:\\Users\\mospr\\Zotero\\storage\\UDZ5F32L\\2210.html}
}

@online{kimFeatureFusionOnline2020,
  title = {Feature {{Fusion}} for {{Online Mutual Knowledge Distillation}}},
  author = {Kim, Jangho and Hyun, Minsung and Chung, Inseop and Kwak, Nojun},
  date = {2020-07-21},
  eprint = {1904.09058},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.09058},
  url = {http://arxiv.org/abs/1904.09058},
  urldate = {2023-09-08},
  abstract = {We propose a learning framework named Feature Fusion Learning (FFL) that efficiently trains a powerful classifier through a fusion module which combines the feature maps generated from parallel neural networks. Specifically, we train a number of parallel neural networks as sub-networks, then we combine the feature maps from each sub-network using a fusion module to create a more meaningful feature map. The fused feature map is passed into the fused classifier for overall classification. Unlike existing feature fusion methods, in our framework, an ensemble of sub-network classifiers transfers its knowledge to the fused classifier and then the fused classifier delivers its knowledge back to each sub-network, mutually teaching one another in an online-knowledge distillation manner. This mutually teaching system not only improves the performance of the fused classifier but also obtains performance gain in each sub-network. Moreover, our model is more beneficial because different types of network can be used for each sub-network. We have performed a variety of experiments on multiple datasets such as CIFAR-10, CIFAR-100 and ImageNet and proved that our method is more effective than other alternative methods in terms of performance of both sub-networks and the fused classifier.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\64UPQMWF\\Kim et al. - 2020 - Feature Fusion for Online Mutual Knowledge Distill.pdf;C\:\\Users\\mospr\\Zotero\\storage\\6K33FCQZ\\1904.html}
}

@inproceedings{kimFeatureFusionOnline2021,
  title = {Feature {{Fusion}} for {{Online Mutual Knowledge Distillation}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Kim, Jangho and Hyun, Minsung and Chung, Inseop and Kwak, Nojun},
  date = {2021-01},
  pages = {4619--4625},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9412615},
  url = {https://ieeexplore.ieee.org/abstract/document/9412615},
  urldate = {2023-10-05},
  abstract = {We propose a learning framework named Feature Fusion Learning (FFL) that efficiently trains a powerful classifier through a fusion module which combines the feature maps generated from parallel neural networks and generates meaningful feature maps. Specifically, we train a number of parallel neural networks as sub-networks, then we combine the feature maps from each sub-network using a fusion module to create a more meaningful feature map. The fused feature map is passed into the fused classifier for overall classification. Unlike existing feature fusion methods, in our framework, an ensemble of sub-network classifiers transfers its knowledge to the fused classifier and then the fused classifier delivers its knowledge back to each subnetwork, mutually teaching one another in an online-knowledge distillation manner. This mutually teaching system not only improves the performance of the fused classifier but also obtains performance gain in each sub-network. Moreover, our model is more beneficial than other alternative methods because different types of network can be used for each sub-network. We have performed a variety of experiments on multiple datasets such as CIFAR-10, CIFAR-100 and ImageNet and proved that our method is more effective than other alternative methods in terms of performances of both sub-networks and the fused classifier, and the aspect of generating meaningful feature maps. The code is available at this link1.},
  eventtitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  file = {C:\Users\mospr\Zotero\storage\2RH7IACQ\Kim et al. - 2021 - Feature Fusion for Online Mutual Knowledge Distill.pdf}
}

@inproceedings{kleinQANomQuestionAnswerDriven2020,
  title = {{{QANom}}: {{Question-Answer}} Driven {{SRL}} for {{Nominalizations}}},
  shorttitle = {{{QANom}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Klein, Ayal and Mamou, Jonathan and Pyatkin, Valentina and Stepanov, Daniela and He, Hangfeng and Roth, Dan and Zettlemoyer, Luke and Dagan, Ido},
  date = {2020-12},
  pages = {3069--3083},
  publisher = {{International Committee on Computational Linguistics}},
  location = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.274},
  url = {https://aclanthology.org/2020.coling-main.274},
  urldate = {2023-10-08},
  abstract = {We propose a new semantic scheme for capturing predicate-argument relations for nominalizations, termed QANom. This scheme extends the QA-SRL formalism (He et al., 2015), modeling the relations between nominalizations and their arguments via natural language question-answer pairs. We construct the first QANom dataset using controlled crowdsourcing, analyze its quality and compare it to expertly annotated nominal-SRL annotations, as well as to other QA-driven annotations. In addition, we train a baseline QANom parser for identifying nominalizations and labeling their arguments with question-answer pairs. Finally, we demonstrate the extrinsic utility of our annotations for downstream tasks using both indirect supervision and zero-shot settings.},
  eventtitle = {{{COLING}} 2020},
  file = {C:\Users\mospr\Zotero\storage\FGPCKA5F\Klein et al. - 2020 - QANom Question-Answer driven SRL for Nominalizati.pdf}
}

@online{KnowledgeDistillationOptimization,
  title = {Knowledge {{Distillation}} for {{Optimization}} of {{Quantized Deep Neural Networks}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9195219},
  urldate = {2023-10-05}
}

@online{krishnamoorthiQuantizingDeepConvolutional2018,
  title = {Quantizing Deep Convolutional Networks for Efficient Inference: {{A}} Whitepaper},
  shorttitle = {Quantizing Deep Convolutional Networks for Efficient Inference},
  author = {Krishnamoorthi, Raghuraman},
  date = {2018-06-21},
  eprint = {1806.08342},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1806.08342},
  url = {http://arxiv.org/abs/1806.08342},
  urldate = {2023-09-06},
  abstract = {We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2\% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. Quantization-aware training can provide further improvements, reducing the gap to floating point to 1\% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2\% to 10\%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\8NBWYC5L\\Krishnamoorthi - 2018 - Quantizing deep convolutional networks for efficie.pdf;C\:\\Users\\mospr\\Zotero\\storage\\8AIKKKPR\\1806.html}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2012-12-03},
  series = {{{NIPS}}'12},
  pages = {1097--1105},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.}
}

@article{krizhevskyLearningMultipleLayers2012,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  date = {2012-05-08},
  journaltitle = {University of Toronto},
  shortjournal = {University of Toronto},
  abstract = {April 8, 2009Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it di cult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signi cantly}
}

@inproceedings{kuhnSemanticUncertaintyLinguistic2022,
  title = {Semantic {{Uncertainty}}: {{Linguistic Invariances}} for {{Uncertainty Estimation}} in {{Natural Language Generation}}},
  shorttitle = {Semantic {{Uncertainty}}},
  author = {Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
  date = {2022-09-29},
  url = {https://openreview.net/forum?id=VD-AYtP0dve},
  urldate = {2023-10-07},
  abstract = {We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of "semantic equivalence"—different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy—an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.},
  eventtitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\5PEVUGAZ\Kuhn et al. - 2022 - Semantic Uncertainty Linguistic Invariances for U.pdf}
}

@inproceedings{lazarevichPostTrainingDeepNeural2021,
  title = {Post-{{Training Deep Neural Network Pruning}} via {{Layer-Wise Calibration}}},
  author = {Lazarevich, Ivan and Kozlov, Alexander and Malinin, Nikita},
  date = {2021},
  pages = {798--805},
  url = {https://openaccess.thecvf.com/content/ICCV2021W/LPCV/html/Lazarevich_Post-Training_Deep_Neural_Network_Pruning_via_Layer-Wise_Calibration_ICCVW_2021_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\7XZAZN9V\Lazarevich et al. - 2021 - Post-Training Deep Neural Network Pruning via Laye.pdf}
}

@online{lebedevFastConvNetsUsing2015,
  title = {Fast {{ConvNets Using Group-wise Brain Damage}}},
  author = {Lebedev, Vadim and Lempitsky, Victor},
  date = {2015-12-07},
  eprint = {1506.02515},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.02515},
  url = {http://arxiv.org/abs/1506.02515},
  urldate = {2023-09-07},
  abstract = {We revisit the idea of brain damage, i.e. the pruning of the coefficients of a neural network, and suggest how brain damage can be modified and used to speedup convolutional layers. The approach uses the fact that many efficient implementations reduce generalized convolutions to matrix multiplications. The suggested brain damage process prunes the convolutional kernel tensor in a group-wise fashion by adding group-sparsity regularization to the standard training process. After such group-wise pruning, convolutions can be reduced to multiplications of thinned dense matrices, which leads to speedup. In the comparison on AlexNet, the method achieves very competitive performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\L4H7M47F\\Lebedev and Lempitsky - 2015 - Fast ConvNets Using Group-wise Brain Damage.pdf;C\:\\Users\\mospr\\Zotero\\storage\\5593IWRY\\1506.html}
}

@inproceedings{lebedevFastConvNetsUsing2016,
  title = {Fast {{ConvNets Using Group-Wise Brain Damage}}},
  author = {Lebedev, Vadim and Lempitsky, Victor},
  date = {2016},
  pages = {2554--2564},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Lebedev_Fast_ConvNets_Using_CVPR_2016_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\ITMN6YNZ\Lebedev and Lempitsky - 2016 - Fast ConvNets Using Group-Wise Brain Damage.pdf}
}

@incollection{lecunConvolutionalNetworksImages1995,
  title = {Convolutional Networks for Images, Speech, and Time-Series},
  booktitle = {The Handbook of Brain Theory and Neural Networks},
  author = {Lecun, Yann and Bengio, Yoshua},
  editor = {Arbib, M.A.},
  date = {1995},
  publisher = {{MIT Press}}
}

@inproceedings{lecunOptimalBrainDamage1989,
  title = {Optimal {{Brain Damage}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Denker, John and Solla, Sara},
  date = {1989},
  volume = {2},
  publisher = {{Morgan-Kaufmann}},
  url = {https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html},
  urldate = {2023-09-07},
  abstract = {We  have used  information-theoretic ideas  to derive  a class of prac(cid:173) tical  and  nearly  optimal schemes  for  adapting the size  of a  neural  network.  By  removing  unimportant  weights  from  a  network,  sev(cid:173) eral  improvements  can  be  expected:  better  generalization,  fewer  training examples required,  and improved speed  of learning and/or  classification.  The  basic  idea  is  to  use  second-derivative  informa(cid:173) tion to make a  tradeoff between  network  complexity  and  training  set error.  Experiments confirm  the usefulness  of the methods on a  real-world  application.},
  file = {C:\Users\mospr\Zotero\storage\SCAKFPGH\LeCun et al. - 1989 - Optimal Brain Damage.pdf}
}

@online{leeGraphbasedKnowledgeDistillation2019,
  title = {Graph-Based {{Knowledge Distillation}} by {{Multi-head Attention Network}}},
  author = {Lee, Seunghyun and Song, Byung Cheol},
  date = {2019-07-08},
  eprint = {1907.02226},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1907.02226},
  url = {http://arxiv.org/abs/1907.02226},
  urldate = {2023-09-08},
  abstract = {Knowledge distillation (KD) is a technique to derive optimal performance from a small student network (SN) by distilling knowledge of a large teacher network (TN) and transferring the distilled knowledge to the small SN. Since a role of convolutional neural network (CNN) in KD is to embed a dataset so as to perform a given task well, it is very important to acquire knowledge that considers intra-data relations. Conventional KD methods have concentrated on distilling knowledge in data units. To our knowledge, any KD methods for distilling information in dataset units have not yet been proposed. Therefore, this paper proposes a novel method that enables distillation of dataset-based knowledge from the TN using an attention network. The knowledge of the embedding procedure of the TN is distilled to graph by multi-head attention (MHA), and multi-task learning is performed to give relational inductive bias to the SN. The MHA can provide clear information about the source dataset, which can greatly improves the performance of the SN. Experimental results show that the proposed method is 7.05\% higher than the SN alone for CIFAR100, which is 2.46\% higher than the state-of-the-art.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\LAYRF2ZW\\Lee and Song - 2019 - Graph-based Knowledge Distillation by Multi-head A.pdf;C\:\\Users\\mospr\\Zotero\\storage\\UN95SA82\\1907.html}
}

@online{leeGraphbasedKnowledgeDistillation2019a,
  title = {Graph-Based {{Knowledge Distillation}} by {{Multi-head Attention Network}}},
  author = {Lee, Seunghyun and Song, Byung Cheol},
  date = {2019-07-08},
  eprint = {1907.02226},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1907.02226},
  url = {http://arxiv.org/abs/1907.02226},
  urldate = {2023-09-08},
  abstract = {Knowledge distillation (KD) is a technique to derive optimal performance from a small student network (SN) by distilling knowledge of a large teacher network (TN) and transferring the distilled knowledge to the small SN. Since a role of convolutional neural network (CNN) in KD is to embed a dataset so as to perform a given task well, it is very important to acquire knowledge that considers intra-data relations. Conventional KD methods have concentrated on distilling knowledge in data units. To our knowledge, any KD methods for distilling information in dataset units have not yet been proposed. Therefore, this paper proposes a novel method that enables distillation of dataset-based knowledge from the TN using an attention network. The knowledge of the embedding procedure of the TN is distilled to graph by multi-head attention (MHA), and multi-task learning is performed to give relational inductive bias to the SN. The MHA can provide clear information about the source dataset, which can greatly improves the performance of the SN. Experimental results show that the proposed method is 7.05\% higher than the SN alone for CIFAR100, which is 2.46\% higher than the state-of-the-art.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\FY5B9YB6\\Lee and Song - 2019 - Graph-based Knowledge Distillation by Multi-head A.pdf;C\:\\Users\\mospr\\Zotero\\storage\\7US23UYZ\\1907.html}
}

@inproceedings{leeGraphbasedKnowledgeDistillation2020,
  title = {Graph-Based Knowledge Distillation by Multi-Head Attention Network},
  author = {Lee, Seunghyun and Song, Byung Cheol},
  date = {2020},
  url = {https://inha.elsevierpure.com/en/publications/graph-based-knowledge-distillation-by-multi-head-attention-networ},
  urldate = {2023-10-05},
  eventtitle = {30th {{British Machine Vision Conference}}, {{BMVC}} 2019},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\KEE87X8Q\graph-based-knowledge-distillation-by-multi-head-attention-networ.html}
}

@online{leeSelfsupervisedKnowledgeDistillation2018,
  title = {Self-Supervised {{Knowledge Distillation Using Singular Value Decomposition}}},
  author = {Lee, Seung Hyun and Kim, Dae Ha and Song, Byung Cheol},
  date = {2018-07-18},
  eprint = {1807.06819},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1807.06819},
  url = {http://arxiv.org/abs/1807.06819},
  urldate = {2023-09-08},
  abstract = {To solve deep neural network (DNN)'s huge training dataset and its high computation issue, so-called teacher-student (T-S) DNN which transfers the knowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN has limited range of use, and the knowledge of T-DNN is insufficiently transferred to S-DNN. To improve the quality of the transferred knowledge from T-DNN, we propose a new knowledge distillation using singular value decomposition (SVD). In addition, we define a knowledge transfer as a self-supervised task and suggest a way to continuously receive information from T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of the T-DNN can be up to 1.1\textbackslash\% better than the T-DNN in terms of classification accuracy. Also assuming the same computational cost, our S-DNN outperforms the S-DNN driven by the state-of-the-art distillation with a performance advantage of 1.79\textbackslash\%. code is available on https://github.com/sseung0703/SSKD\textbackslash\_SVD.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\ST7KZYM4\\Lee et al. - 2018 - Self-supervised Knowledge Distillation Using Singu.pdf;C\:\\Users\\mospr\\Zotero\\storage\\NLNV3C4Z\\1807.html}
}

@inproceedings{leeSelfsupervisedKnowledgeDistillation2018a,
  title = {Self-Supervised {{Knowledge Distillation Using Singular Value Decomposition}}},
  author = {Lee, Seung Hyun and Kim, Dae Ha and Song, Byung Cheol},
  date = {2018},
  pages = {335--350},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/SEUNG_HYUN_LEE_Self-supervised_Knowledge_Distillation_ECCV_2018_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  file = {C:\Users\mospr\Zotero\storage\IW6BHU69\Lee et al. - 2018 - Self-supervised Knowledge Distillation Using Singu.pdf}
}

@article{liangPruningQuantizationDeep2021,
  title = {Pruning and Quantization for Deep Neural Network Acceleration: {{A}} Survey},
  shorttitle = {Pruning and Quantization for Deep Neural Network Acceleration},
  author = {Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
  date = {2021-10-21},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {461},
  pages = {370--403},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.07.045},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221010894},
  urldate = {2023-09-06},
  abstract = {Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.},
  keywords = {Convolutional neural network,Low-bit mathematics,Neural network acceleration,Neural network pruning,Neural network quantization},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\DIPTGN3G\\Liang et al. - 2021 - Pruning and quantization for deep neural network a.pdf;C\:\\Users\\mospr\\Zotero\\storage\\RKNS7IGL\\S0925231221010894.html}
}

@online{liCurriculumTemperatureKnowledge2022,
  title = {Curriculum {{Temperature}} for {{Knowledge Distillation}}},
  author = {Li, Zheng and Li, Xiang and Yang, Lingfeng and Zhao, Borui and Song, Renjie and Luo, Lei and Li, Jun and Yang, Jian},
  date = {2022-12-24},
  eprint = {2211.16231},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.16231},
  url = {http://arxiv.org/abs/2211.16231},
  urldate = {2023-10-01},
  abstract = {Most existing distillation methods ignore the flexible role of the temperature in the loss function and fix it as a hyper-parameter that can be decided by an inefficient grid search. In general, the temperature controls the discrepancy between two distributions and can faithfully determine the difficulty level of the distillation task. Keeping a constant temperature, i.e., a fixed level of task difficulty, is usually sub-optimal for a growing student during its progressive learning stages. In this paper, we propose a simple curriculum-based technique, termed Curriculum Temperature for Knowledge Distillation (CTKD), which controls the task difficulty level during the student's learning career through a dynamic and learnable temperature. Specifically, following an easy-to-hard curriculum, we gradually increase the distillation loss w.r.t. the temperature, leading to increased distillation difficulty in an adversarial manner. As an easy-to-use plug-in technique, CTKD can be seamlessly integrated into existing knowledge distillation frameworks and brings general improvements at a negligible additional computation cost. Extensive experiments on CIFAR-100, ImageNet-2012, and MS-COCO demonstrate the effectiveness of our method. Our code is available at https://github.com/zhengli97/CTKD.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\A972QNJZ\\Li et al. - 2022 - Curriculum Temperature for Knowledge Distillation.pdf;C\:\\Users\\mospr\\Zotero\\storage\\FB5CXURF\\2211.html}
}

@online{liFewSampleKnowledge2020,
  title = {Few {{Sample Knowledge Distillation}} for {{Efficient Network Compression}}},
  author = {Li, Tianhong and Li, Jianguo and Liu, Zhuang and Zhang, Changshui},
  date = {2020-03-31},
  eprint = {1812.01839},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1812.01839},
  url = {http://arxiv.org/abs/1812.01839},
  urldate = {2023-09-08},
  abstract = {Deep neural network compression techniques such as pruning and weight tensor decomposition usually require fine-tuning to recover the prediction accuracy when the compression ratio is high. However, conventional fine-tuning suffers from the requirement of a large training set and the time-consuming training procedure. This paper proposes a novel solution for knowledge distillation from label-free few samples to realize both data efficiency and training/processing efficiency. We treat the original network as "teacher-net" and the compressed network as "student-net". A 1x1 convolution layer is added at the end of each layer block of the student-net, and we fit the block-level outputs of the student-net to the teacher-net by estimating the parameters of the added layers. We prove that the added layer can be merged without adding extra parameters and computation cost during inference. Experiments on multiple datasets and network architectures verify the method's effectiveness on student-nets obtained by various network pruning and weight decomposition methods. Our method can recover student-net's accuracy to the same level as conventional fine-tuning methods in minutes while using only 1\% label-free data of the full training data.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\QREWYV9K\\Li et al. - 2020 - Few Sample Knowledge Distillation for Efficient Ne.pdf;C\:\\Users\\mospr\\Zotero\\storage\\Y2D8VBJS\\1812.html}
}

@inproceedings{liFewSampleKnowledge2020a,
  title = {Few {{Sample Knowledge Distillation}} for {{Efficient Network Compression}}},
  author = {Li, Tianhong and Li, Jianguo and Liu, Zhuang and Zhang, Changshui},
  date = {2020},
  pages = {14639--14647},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\Z9EGS73Y\Li et al. - 2020 - Few Sample Knowledge Distillation for Efficient Ne.pdf}
}

@online{liHaluEvalLargeScaleHallucination2023,
  title = {{{HaluEval}}: {{A Large-Scale Hallucination Evaluation Benchmark}} for {{Large Language Models}}},
  shorttitle = {{{HaluEval}}},
  author = {Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  date = {2023-05-22},
  eprint = {2305.11747},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.11747},
  url = {http://arxiv.org/abs/2305.11747},
  urldate = {2023-10-07},
  abstract = {Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, \textbackslash ie content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, \textbackslash ie sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (\textbackslash ie about \$11.4\textbackslash\%\$ user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\Q8M5UAU5\\Li et al. - 2023 - HaluEval A Large-Scale Hallucination Evaluation B.pdf;C\:\\Users\\mospr\\Zotero\\storage\\N2FA4SJ7\\2305.html}
}

@online{liJustClozEFast2022,
  title = {Just {{ClozE}}! {{A Fast}} and {{Simple Method}} for {{Evaluating}} the {{Factual Consistency}} in {{Abstractive Summarization}}},
  author = {Li, Yiyang and Li, Lei and Yang, Qing and Litvak, Marina and Vanetik, Natalia and Hu, Dingxin and Li, Yuze and Zhou, Yanquan and Xu, Dongliang and Zhang, Xuanyu},
  date = {2022-10-06},
  eprint = {2210.02804},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.02804},
  url = {http://arxiv.org/abs/2210.02804},
  urldate = {2023-10-08},
  abstract = {The issue of factual consistency in abstractive summarization has attracted much attention in recent years, and the evaluation of factual consistency between summary and document has become an important and urgent task. Most of the current evaluation metrics are adopted from the question answering (QA). However, the application of QA-based metrics is extremely time-consuming in practice, causing the iteration cycle of abstractive summarization research to be severely prolonged. In this paper, we propose a new method called ClozE to evaluate factual consistency by cloze model, instantiated based on masked language model(MLM), with strong interpretability and substantially higher speed. We demonstrate that ClozE can reduce the evaluation time by nearly 96\$\textbackslash\%\$ relative to QA-based metrics while retaining their interpretability and performance through experiments on six human-annotated datasets and a meta-evaluation benchmark GO FIGURE \textbackslash citep\{gabriel2020go\}. We also implement experiments to further demonstrate more characteristics of ClozE in terms of performance and speed. In addition, we conduct an experimental analysis of the limitations of ClozE, which suggests future research directions. The code and models for ClozE will be released upon the paper acceptance.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\KH9PNKA3\\Li et al. - 2022 - Just ClozE! A Fast and Simple Method for Evaluatin.pdf;C\:\\Users\\mospr\\Zotero\\storage\\6QPLDEG5\\2210.html}
}

@article{liModelCompressionDeep2023,
  title = {Model {{Compression}} for {{Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Model {{Compression}} for {{Deep Neural Networks}}},
  author = {Li, Zhuo and Li, Hengyi and Meng, Lin},
  date = {2023-03},
  journaltitle = {Computers},
  volume = {12},
  number = {3},
  pages = {60},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2073-431X},
  doi = {10.3390/computers12030060},
  url = {https://www.mdpi.com/2073-431X/12/3/60},
  urldate = {2023-09-06},
  abstract = {Currently, with the rapid development of deep learning, deep neural networks (DNNs) have been widely applied in various computer vision tasks. However, in the pursuit of performance, advanced DNN models have become more complex, which has led to a large memory footprint and high computation demands. As a result, the models are difficult to apply in real time. To address these issues, model compression has become a focus of research. Furthermore, model compression techniques play an important role in deploying models on edge devices. This study analyzed various model compression methods to assist researchers in reducing device storage space, speeding up model inference, reducing model complexity and training costs, and improving model deployment. Hence, this paper summarized the state-of-the-art techniques for model compression, including model pruning, parameter quantization, low-rank decomposition, knowledge distillation, and lightweight model design. In addition, this paper discusses research challenges and directions for future work.},
  issue = {3},
  langid = {english},
  keywords = {deep neural networks,knowledge distillation,lightweight model design,low-rank decomposition,model compression,model pruning,parameter quantization},
  file = {C:\Users\mospr\Zotero\storage\V6GJ5NMC\Li et al. - 2023 - Model Compression for Deep Neural Networks A Surv.pdf}
}

@online{linRotatedBinaryNeural2020,
  title = {Rotated {{Binary Neural Network}}},
  author = {Lin, Mingbao and Ji, Rongrong and Xu, Zihan and Zhang, Baochang and Wang, Yan and Wu, Yongjian and Huang, Feiyue and Lin, Chia-Wen},
  date = {2020-10-22},
  eprint = {2009.13055},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2009.13055},
  url = {http://arxiv.org/abs/2009.13055},
  urldate = {2023-09-06},
  abstract = {Binary Neural Network (BNN) shows its predominance in reducing the complexity of deep neural networks. However, it suffers severe performance degradation. One of the major impediments is the large quantization error between the full-precision weight vector and its binary vector. Previous works focus on compensating for the norm gap while leaving the angular bias hardly touched. In this paper, for the first time, we explore the influence of angular bias on the quantization error and then introduce a Rotated Binary Neural Network (RBNN), which considers the angle alignment between the full-precision weight vector and its binarized version. At the beginning of each training epoch, we propose to rotate the full-precision weight vector to its binary vector to reduce the angular bias. To avoid the high complexity of learning a large rotation matrix, we further introduce a bi-rotation formulation that learns two smaller rotation matrices. In the training stage, we devise an adjustable rotated weight vector for binarization to escape the potential local optimum. Our rotation leads to around 50\% weight flips which maximize the information gain. Finally, we propose a training-aware approximation of the sign function for the gradient backward. Experiments on CIFAR-10 and ImageNet demonstrate the superiorities of RBNN over many state-of-the-arts. Our source code, experimental settings, training logs and binary models are available at https://github.com/lmbxmu/RBNN.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\3FRW6DCA\\Lin et al. - 2020 - Rotated Binary Neural Network.pdf;C\:\\Users\\mospr\\Zotero\\storage\\VVLJPQLK\\2009.html}
}

@inproceedings{linRotatedBinaryNeural2020a,
  title = {Rotated {{Binary Neural Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lin, Mingbao and Ji, Rongrong and Xu, Zihan and Zhang, Baochang and Wang, Yan and Wu, Yongjian and Huang, Feiyue and Lin, Chia-Wen},
  date = {2020},
  volume = {33},
  pages = {7474--7485},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/53c5b2affa12eed84dfec9bfd83550b1-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Binary Neural Network (BNN) shows its predominance in reducing the complexity of deep neural networks. However, it suffers severe performance degradation. One of the major impediments is the large quantization error between the full-precision weight vector and its binary vector. Previous works focus on compensating for the norm gap while leaving the angular bias hardly touched. In this paper, for the first time, we explore the influence of angular bias on the quantization error and then introduce a Rotated Binary Neural Network (RBNN), which considers the angle alignment between the full-precision weight vector and its binarized version. At the beginning of each training epoch, we propose to rotate the full-precision weight vector to its binary vector to reduce the angular bias. To avoid the high complexity of learning a large rotation matrix, we further introduce a bi-rotation formulation that learns two smaller rotation matrices. In the training stage, we devise an adjustable rotated weight vector for binarization to escape the potential local optimum. Our rotation leads to around 50\% weight flips which maximize the information gain. Finally, we propose a training-aware approximation of the sign function for the gradient backward. Experiments on CIFAR-10 and ImageNet demonstrate the superiorities of RBNN over many state-of-the-arts. Our source code, experimental settings, training logs and binary models are available at https://github.com/lmbxmu/RBNN.},
  file = {C:\Users\mospr\Zotero\storage\8KJHJEU7\Lin et al. - 2020 - Rotated Binary Neural Network.pdf}
}

@inproceedings{linTruthfulQAMeasuringHow2022,
  title = {{{TruthfulQA}}: {{Measuring How Models Mimic Human Falsehoods}}},
  shorttitle = {{{TruthfulQA}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  date = {2022-05},
  pages = {3214--3252},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.229},
  url = {https://aclanthology.org/2022.acl-long.229},
  urldate = {2023-10-07},
  abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
  eventtitle = {{{ACL}} 2022},
  file = {C:\Users\mospr\Zotero\storage\T7LZXLFF\Lin et al. - 2022 - TruthfulQA Measuring How Models Mimic Human False.pdf}
}

@inproceedings{linTruthfulQAMeasuringHow2022a,
  title = {{{TruthfulQA}}: {{Measuring How Models Mimic Human Falsehoods}}},
  shorttitle = {{{TruthfulQA}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  date = {2022-05},
  pages = {3214--3252},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.229},
  url = {https://aclanthology.org/2022.acl-long.229},
  urldate = {2023-10-08},
  abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
  eventtitle = {{{ACL}} 2022},
  file = {C:\Users\mospr\Zotero\storage\6YW8YZJW\Lin et al. - 2022 - TruthfulQA Measuring How Models Mimic Human False.pdf}
}

@inproceedings{liPruningFiltersEfficient2016,
  title = {Pruning {{Filters}} for {{Efficient ConvNets}}},
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  date = {2016-11-05},
  url = {https://openreview.net/forum?id=rJqFGTslg},
  urldate = {2023-10-05},
  abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34\% and ResNet-110 by up to 38\% on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\H7PAG8YD\Li et al. - 2016 - Pruning Filters for Efficient ConvNets.pdf}
}

@online{liPruningFiltersEfficient2017,
  title = {Pruning {{Filters}} for {{Efficient ConvNets}}},
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  date = {2017-03-10},
  eprint = {1608.08710},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.08710},
  url = {http://arxiv.org/abs/1608.08710},
  urldate = {2023-09-05},
  abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34\% and ResNet-110 by up to 38\% on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\MUD8YHAN\\Li et al. - 2017 - Pruning Filters for Efficient ConvNets.pdf;C\:\\Users\\mospr\\Zotero\\storage\\V4WXUC5E\\1608.html}
}

@online{liSelfCheckerPlugandPlayModules2023,
  title = {Self-{{Checker}}: {{Plug-and-Play Modules}} for {{Fact-Checking}} with {{Large Language Models}}},
  shorttitle = {Self-{{Checker}}},
  author = {Li, Miaoran and Peng, Baolin and Zhang, Zhu},
  date = {2023-05-24},
  url = {https://arxiv.org/abs/2305.14623v1},
  urldate = {2023-10-08},
  abstract = {Fact-checking is an essential task in NLP that is commonly utilized for validating the factual accuracy of claims. Prior work has mainly focused on fine-tuning pre-trained languages models on specific datasets, which can be computationally intensive and time-consuming. With the rapid development of large language models (LLMs), such as ChatGPT and GPT-3, researchers are now exploring their in-context learning capabilities for a wide range of tasks. In this paper, we aim to assess the capacity of LLMs for fact-checking by introducing Self-Checker, a framework comprising a set of plug-and-play modules that facilitate fact-checking by purely prompting LLMs in an almost zero-shot setting. This framework provides a fast and efficient way to construct fact-checking systems in low-resource environments. Empirical results demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking. However, there is still significant room for improvement compared to SOTA fine-tuned models, which suggests that LLM adoption could be a promising approach for future fact-checking research.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\mospr\Zotero\storage\R45U93KE\Li et al. - 2023 - Self-Checker Plug-and-Play Modules for Fact-Check.pdf}
}

@online{liuEfficientNLPStandard2022,
  title = {Towards {{Efficient NLP}}: {{A Standard Evaluation}} and {{A Strong Baseline}}},
  shorttitle = {Towards {{Efficient NLP}}},
  author = {Liu, Xiangyang and Sun, Tianxiang and He, Junliang and Wu, Jiawen and Wu, Lingling and Zhang, Xinyu and Jiang, Hao and Cao, Zhao and Huang, Xuanjing and Qiu, Xipeng},
  date = {2022-04-10},
  eprint = {2110.07038},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.07038},
  url = {http://arxiv.org/abs/2110.07038},
  urldate = {2023-09-09},
  abstract = {Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention on model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depict the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\BKQTCJIQ\\Liu et al. - 2022 - Towards Efficient NLP A Standard Evaluation and A.pdf;C\:\\Users\\mospr\\Zotero\\storage\\AJEH93VF\\2110.html}
}

@inproceedings{liuEfficientNLPStandard2022a,
  title = {Towards {{Efficient NLP}}: {{A Standard Evaluation}} and {{A Strong Baseline}}},
  shorttitle = {Towards {{Efficient NLP}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Liu, Xiangyang and Sun, Tianxiang and He, Junliang and Wu, Jiawen and Wu, Lingling and Zhang, Xinyu and Jiang, Hao and Cao, Zhao and Huang, Xuanjing and Qiu, Xipeng},
  date = {2022-07},
  pages = {3288--3303},
  publisher = {{Association for Computational Linguistics}},
  location = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.240},
  url = {https://aclanthology.org/2022.naacl-main.240},
  urldate = {2023-10-05},
  abstract = {Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depicting the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.},
  eventtitle = {{{NAACL-HLT}} 2022},
  file = {C:\Users\mospr\Zotero\storage\N5XQYBTQ\Liu et al. - 2022 - Towards Efficient NLP A Standard Evaluation and A.pdf}
}

@online{liuPruningAlgorithmsAccelerate2020,
  title = {Pruning {{Algorithms}} to {{Accelerate Convolutional Neural Networks}} for {{Edge Applications}}: {{A Survey}}},
  shorttitle = {Pruning {{Algorithms}} to {{Accelerate Convolutional Neural Networks}} for {{Edge Applications}}},
  author = {Liu, Jiayi and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
  date = {2020-05-08},
  eprint = {2005.04275},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2005.04275},
  url = {http://arxiv.org/abs/2005.04275},
  urldate = {2023-09-06},
  abstract = {With the general trend of increasing Convolutional Neural Network (CNN) model sizes, model compression and acceleration techniques have become critical for the deployment of these models on edge devices. In this paper, we provide a comprehensive survey on Pruning, a major compression strategy that removes non-critical or redundant neurons from a CNN model. The survey covers the overarching motivation for pruning, different strategies and criteria, their advantages and drawbacks, along with a compilation of major pruning techniques. We conclude the survey with a discussion on alternatives to pruning and current challenges for the model compression community.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\XLAKTB9G\\Liu et al. - 2020 - Pruning Algorithms to Accelerate Convolutional Neu.pdf;C\:\\Users\\mospr\\Zotero\\storage\\F25SWL84\\2005.html}
}

@inproceedings{liuRethinkingValueNetwork2018,
  title = {Rethinking the {{Value}} of {{Network Pruning}}},
  author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=rJlnB3C5Ym},
  urldate = {2023-10-05},
  abstract = {Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned ``important'' weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited ``important'' weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the "Lottery Ticket Hypothesis" (Frankle \& Carbin 2019), and find that with optimal learning rate, the "winning ticket" initialization as used in Frankle \& Carbin (2019) does not bring improvement over random initialization.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\E78EVTTY\Liu et al. - 2018 - Rethinking the Value of Network Pruning.pdf}
}

@online{liuRethinkingValueNetwork2019,
  title = {Rethinking the {{Value}} of {{Network Pruning}}},
  author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  date = {2019-03-05},
  eprint = {1810.05270},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.05270},
  url = {http://arxiv.org/abs/1810.05270},
  urldate = {2023-09-07},
  abstract = {Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned "important" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited "important" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the "Lottery Ticket Hypothesis" (Frankle \& Carbin 2019), and find that with optimal learning rate, the "winning ticket" initialization as used in Frankle \& Carbin (2019) does not bring improvement over random initialization.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\3WZHWY6K\\Liu et al. - 2019 - Rethinking the Value of Network Pruning.pdf;C\:\\Users\\mospr\\Zotero\\storage\\5G4NI9HR\\1810.html}
}

@online{longpreEntityBasedKnowledgeConflicts2022,
  title = {Entity-{{Based Knowledge Conflicts}} in {{Question Answering}}},
  author = {Longpre, Shayne and Perisetla, Kartik and Chen, Anthony and Ramesh, Nikhil and DuBois, Chris and Singh, Sameer},
  date = {2022-01-11},
  eprint = {2109.05052},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.05052},
  url = {http://arxiv.org/abs/2109.05052},
  urldate = {2023-10-07},
  abstract = {Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge, which minimizes hallucination, and improves out-of-distribution generalization by 4\%-7\%. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e., time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\QHM92BI9\\Longpre et al. - 2022 - Entity-Based Knowledge Conflicts in Question Answe.pdf;C\:\\Users\\mospr\\Zotero\\storage\\9BDIH44K\\2109.html}
}

@online{maLLMPrunerStructuralPruning2023,
  title = {{{LLM-Pruner}}: {{On}} the {{Structural Pruning}} of {{Large Language Models}}},
  shorttitle = {{{LLM-Pruner}}},
  author = {Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  date = {2023-06-14},
  eprint = {2305.11627},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.11627},
  url = {http://arxiv.org/abs/2305.11627},
  urldate = {2023-09-07},
  abstract = {Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\FKK9CW4X\\Ma et al. - 2023 - LLM-Pruner On the Structural Pruning of Large Lan.pdf;C\:\\Users\\mospr\\Zotero\\storage\\99SIVQ5N\\2305.html}
}

@online{manakulSelfCheckGPTZeroResourceBlackBox2023,
  title = {{{SelfCheckGPT}}: {{Zero-Resource Black-Box Hallucination Detection}} for {{Generative Large Language Models}}},
  shorttitle = {{{SelfCheckGPT}}},
  author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark J. F.},
  date = {2023-05-07},
  eprint = {2303.08896},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.08896},
  url = {http://arxiv.org/abs/2303.08896},
  urldate = {2023-10-07},
  abstract = {Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that in sentence hallucination detection, our approach has AUC-PR scores comparable to or better than grey-box methods, while SelfCheckGPT is best at passage factuality assessment.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\L2VLFKHH\\Manakul et al. - 2023 - SelfCheckGPT Zero-Resource Black-Box Hallucinatio.pdf;C\:\\Users\\mospr\\Zotero\\storage\\G7T8KEHV\\2303.html}
}

@online{mckinstryDiscoveringLowPrecisionNetworks2019,
  title = {Discovering {{Low-Precision Networks Close}} to {{Full-Precision Networks}} for {{Efficient Embedded Inference}}},
  author = {McKinstry, Jeffrey L. and Esser, Steven K. and Appuswamy, Rathinakumar and Bablani, Deepika and Arthur, John V. and Yildiz, Izzet B. and Modha, Dharmendra S.},
  date = {2019-02-24},
  eprint = {1809.04191},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1809.04191},
  url = {http://arxiv.org/abs/1809.04191},
  urldate = {2023-09-06},
  abstract = {To realize the promise of ubiquitous embedded deep network inference, it is essential to seek limits of energy and area efficiency. To this end, low-precision networks offer tremendous promise because both energy and area scale down quadratically with the reduction in precision. Here we demonstrate ResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on the ImageNet classification benchmark that, at 8-bit precision exceed the accuracy of the full-precision baseline networks after one epoch of finetuning, thereby leveraging the availability of pretrained models. We also demonstrate ResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match the accuracy of the full-precision baseline networks -- the highest scores to date. Surprisingly, the weights of the low-precision networks are very close (in cosine similarity) to the weights of the corresponding baseline networks, making training from scratch unnecessary. We find that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this noise. The number of iterations required by SGD to achieve a given training error is related to the square of (a) the distance of the initial solution from the final plus (b) the maximum variance of the gradient estimates. Therefore, we (a) reduce solution distance by starting with pretrained fp32 precision baseline networks and fine-tuning, and (b) combat gradient noise introduced by quantization by training longer and reducing learning rates. Sensitivity analysis indicates that these simple techniques, coupled with proper activation function range calibration to take full advantage of the limited precision, are sufficient to discover low-precision networks, if they exist, close to fp32 precision baseline networks. The results herein provide evidence that 4-bits suffice for classification.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\48JX75XJ\\McKinstry et al. - 2019 - Discovering Low-Precision Networks Close to Full-P.pdf;C\:\\Users\\mospr\\Zotero\\storage\\3IBQQ3LS\\1809.html}
}

@inproceedings{mckinstryDiscoveringLowPrecisionNetworks2019a,
  title = {Discovering {{Low-Precision Networks Close}} to {{Full-Precision Networks}} for {{Efficient Inference}}},
  booktitle = {2019 {{Fifth Workshop}} on {{Energy Efficient Machine Learning}} and {{Cognitive Computing}} - {{NeurIPS Edition}} ({{EMC2-NIPS}})},
  author = {McKinstry, Jeffrey L. and Esser, Steven K. and Appuswamy, Rathinakumar and Bablani, Deepika and Arthur, John V. and Yildiz, Izzet B. and Modha, Dharmendra S.},
  date = {2019-12},
  pages = {6--9},
  doi = {10.1109/EMC2-NIPS53020.2019.00009},
  url = {https://ieeexplore.ieee.org/abstract/document/9463517},
  urldate = {2023-10-05},
  abstract = {To realize the promise of ubiquitous embedded deep network inference, it is essential to seek limits of energy and area efficiency. Low-precision networks offer promise as energy and area scale down quadratically with precision. We demonstrate 8- and 4-bit networks that meet or exceed the accuracy of their full-precision versions on the ImageNet classification benchmark. We hypothesize that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this. The number of iterations required by SGD to achieve a given training error is related to the square of (a) the distance of the initial solution from the final and (b) the maximum variance of the gradient estimates. Accordingly, we reduce solution distance by starting with pretrained fp32 baseline networks, and combat noise introduced by quantizing weights and activations during training by training longer and reducing learning rates. Sensitivity analysis indicates that these techniques, coupled with activation function range calibration, are sufficient to discover low-precision networks close to fp32 precision baseline networks. Our results provide evidence that 4-bits suffice for classification.},
  eventtitle = {2019 {{Fifth Workshop}} on {{Energy Efficient Machine Learning}} and {{Cognitive Computing}} - {{NeurIPS Edition}} ({{EMC2-NIPS}})},
  file = {C:\Users\mospr\Zotero\storage\SJGWA5B8\McKinstry et al. - 2019 - Discovering Low-Precision Networks Close to Full-P.pdf}
}

@inproceedings{mengConditionalTeacherStudentLearning2019,
  title = {Conditional {{Teacher-Student Learning}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Meng, Zhong and Li, Jinyu and Zhao, Yong and Gong, Yifan},
  date = {2019-05},
  eprint = {1904.12399},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  pages = {6445--6449},
  doi = {10.1109/ICASSP.2019.8683438},
  url = {http://arxiv.org/abs/1904.12399},
  urldate = {2023-10-01},
  abstract = {The teacher-student (T/S) learning has been shown to be effective for a variety of problems such as domain adaptation and model compression. One shortcoming of the T/S learning is that a teacher model, not always perfect, sporadically produces wrong guidance in form of posterior probabilities that misleads the student model towards a suboptimal performance. To overcome this problem, we propose a conditional T/S learning scheme, in which a "smart" student model selectively chooses to learn from either the teacher model or the ground truth labels conditioned on whether the teacher can correctly predict the ground truth. Unlike a naive linear combination of the two knowledge sources, the conditional learning is exclusively engaged with the teacher model when the teacher model's prediction is correct, and otherwise backs off to the ground truth. Thus, the student model is able to learn effectively from the teacher and even potentially surpass the teacher. We examine the proposed learning scheme on two tasks: domain adaptation on CHiME-3 dataset and speaker adaptation on Microsoft short message dictation dataset. The proposed method achieves 9.8\% and 12.8\% relative word error rate reductions, respectively, over T/S learning for environment adaptation and speaker-independent model for speaker adaptation.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\F3PISQZ5\\Meng et al. - 2019 - Conditional Teacher-Student Learning.pdf;C\:\\Users\\mospr\\Zotero\\storage\\Y7Q4UZCP\\1904.html}
}

@online{MethodsPruningDeep,
  title = {Methods for {{Pruning Deep Neural Networks}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9795013},
  urldate = {2023-10-05}
}

@inproceedings{migaczNvidia8bitInference,
  title = {Nvidia 8-Bit Inference with Tensorrt.},
  author = {Migacz, Szymon},
  eventtitle = {{{GPU Technology Conference}}, 2017}
}

@online{minFActScoreFinegrainedAtomic2023,
  title = {{{FActScore}}: {{Fine-grained Atomic Evaluation}} of {{Factual Precision}} in {{Long Form Text Generation}}},
  shorttitle = {{{FActScore}}},
  author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  date = {2023-05-23},
  url = {https://arxiv.org/abs/2305.14251v1},
  urldate = {2023-10-08},
  abstract = {Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FActScore (Factual precision in Atomicity Score), a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FActScores of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58\%). Since human evaluation is costly, we also introduce an automated model that estimates FActScore, using retrieval and a strong language model, with less than a 2\% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost \$26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\mospr\Zotero\storage\SSHD6JDL\Min et al. - 2023 - FActScore Fine-grained Atomic Evaluation of Factu.pdf}
}

@online{mirzadehImprovedKnowledgeDistillation2019,
  title = {Improved {{Knowledge Distillation}} via {{Teacher Assistant}}},
  author = {Mirzadeh, Seyed-Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  date = {2019-12-16},
  eprint = {1902.03393},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1902.03393},
  url = {http://arxiv.org/abs/1902.03393},
  urldate = {2023-09-08},
  abstract = {Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\3XF9EVTM\\Mirzadeh et al. - 2019 - Improved Knowledge Distillation via Teacher Assist.pdf;C\:\\Users\\mospr\\Zotero\\storage\\FC49RB8U\\1902.html}
}

@article{mirzadehImprovedKnowledgeDistillation2020,
  title = {Improved {{Knowledge Distillation}} via {{Teacher Assistant}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {5191--5198},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i04.5963},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/5963},
  urldate = {2023-10-05},
  abstract = {Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.},
  issue = {04},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\YRJULJT5\Mirzadeh et al. - 2020 - Improved Knowledge Distillation via Teacher Assist.pdf}
}

@online{mishraApprenticeUsingKnowledge2017,
  title = {Apprentice: {{Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy}}},
  shorttitle = {Apprentice},
  author = {Mishra, Asit and Marr, Debbie},
  date = {2017-11-15},
  eprint = {1711.05852},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.05852},
  url = {http://arxiv.org/abs/1711.05852},
  urldate = {2023-09-08},
  abstract = {Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems - the models (often deep networks or wide networks or both) are compute and memory intensive. Low-precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the combination of these two techniques and show that the performance of low-precision networks can be significantly improved by using knowledge distillation techniques. Our approach, Apprentice, achieves state-of-the-art accuracies using ternary precision and 4-bit precision for variants of ResNet architecture on ImageNet dataset. We present three schemes using which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CKXV7YSH\\Mishra and Marr - 2017 - Apprentice Using Knowledge Distillation Technique.pdf;C\:\\Users\\mospr\\Zotero\\storage\\6B4AI5BB\\1711.html}
}

@online{miyashitaConvolutionalNeuralNetworks2016,
  title = {Convolutional {{Neural Networks}} Using {{Logarithmic Data Representation}}},
  author = {Miyashita, Daisuke and Lee, Edward H. and Murmann, Boris},
  date = {2016-03-16},
  eprint = {1603.01025},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1603.01025},
  url = {http://arxiv.org/abs/1603.01025},
  urldate = {2023-09-06},
  abstract = {Recent advances in convolutional neural networks have considered model complexity and hardware efficiency to enable deployment onto embedded systems and mobile devices. For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance. However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance. In this paper we propose a new data representation that enables state-of-the-art networks to be encoded to 3 bits with negligible loss in classification performance. To perform this, we take advantage of the fact that the weights and activations in a trained network naturally have non-uniform distributions. Using non-uniform, base-2 logarithmic representation to encode weights, communicate activations, and perform dot-products enables networks to 1) achieve higher classification accuracies than fixed-point at the same resolution and 2) eliminate bulky digital multipliers. Finally, we propose an end-to-end training procedure that uses log representation at 5-bits, which achieves higher final test accuracy than linear at 5-bits.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\TXBP8UTP\\Miyashita et al. - 2016 - Convolutional Neural Networks using Logarithmic Da.pdf;C\:\\Users\\mospr\\Zotero\\storage\\Z2M9357B\\1603.html}
}

@online{mobahiSelfDistillationAmplifiesRegularization2020,
  title = {Self-{{Distillation Amplifies Regularization}} in {{Hilbert Space}}},
  author = {Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter L.},
  date = {2020-10-26},
  eprint = {2002.05715},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2002.05715},
  url = {http://arxiv.org/abs/2002.05715},
  urldate = {2023-09-09},
  abstract = {Knowledge distillation introduced in the deep learning context is a method to transfer knowledge from one architecture to another. In particular, when the architectures are identical, this is called self-distillation. The idea is to feed in predictions of the trained model as new target values for retraining (and iterate this loop possibly a few times). It has been empirically observed that the self-distilled model often achieves higher accuracy on held out data. Why this happens, however, has been a mystery: the self-distillation dynamics does not receive any new information about the task and solely evolves by looping over training. To the best of our knowledge, there is no rigorous understanding of this phenomenon. This work provides the first theoretical analysis of self-distillation. We focus on fitting a nonlinear function to training data, where the model space is Hilbert space and fitting is subject to \$\textbackslash ell\_2\$ regularization in this function space. We show that self-distillation iterations modify regularization by progressively limiting the number of basis functions that can be used to represent the solution. This implies (as we also verify empirically) that while a few rounds of self-distillation may reduce over-fitting, further rounds may lead to under-fitting and thus worse performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\SGGVRZZW\\Mobahi et al. - 2020 - Self-Distillation Amplifies Regularization in Hilb.pdf;C\:\\Users\\mospr\\Zotero\\storage\\4SSZLKS7\\2002.html}
}

@inproceedings{mobahiSelfDistillationAmplifiesRegularization2020a,
  title = {Self-{{Distillation Amplifies Regularization}} in {{Hilbert Space}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter},
  date = {2020},
  volume = {33},
  pages = {3351--3361},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/2288f691b58edecadcc9a8691762b4fd-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Knowledge distillation introduced in the deep learning context is a method to transfer knowledge from one architecture to another. In particular, when the architectures are identical, this is called self-distillation. The idea is to feed in predictions of the trained model as new target values for retraining (and iterate this loop possibly a few times). It has been empirically observed that the self-distilled model often achieves higher accuracy on held out data. Why this happens, however, has been a mystery: the self-distillation dynamics does not receive any new information about the task and solely evolves by looping over training. To the best of our knowledge, there is no rigorous understanding of why this happens. This work provides the first theoretical analysis of self-distillation. We focus on fitting a nonlinear function to training data, where the model space is Hilbert space and fitting is subject to L2 regularization in this function space. We show that self-distillation iterations modify regularization by progressively limiting the number of basis functions that can be used to represent the solution. This implies (as we also verify empirically) that while a few rounds of self-distillation may reduce over-fitting, further rounds may lead to under-fitting and thus worse performance.},
  file = {C:\Users\mospr\Zotero\storage\KS8NW3DN\Mobahi et al. - 2020 - Self-Distillation Amplifies Regularization in Hilb.pdf}
}

@online{muhlgayGeneratingBenchmarksFactuality2023,
  title = {Generating {{Benchmarks}} for {{Factuality Evaluation}} of {{Language Models}}},
  author = {Muhlgay, Dor and Ram, Ori and Magar, Inbal and Levine, Yoav and Ratner, Nir and Belinkov, Yonatan and Abend, Omri and Leyton-Brown, Kevin and Shashua, Amnon and Shoham, Yoav},
  date = {2023-07-13},
  eprint = {2307.06908},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.06908},
  url = {http://arxiv.org/abs/2307.06908},
  urldate = {2023-10-07},
  abstract = {Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators. We make our data and code publicly available in https://github.com/AI21Labs/factor.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\BDKYMHIQ\\Muhlgay et al. - 2023 - Generating Benchmarks for Factuality Evaluation of.pdf;C\:\\Users\\mospr\\Zotero\\storage\\RJTC2HEQ\\2307.html}
}

@online{nagelAdaptiveRoundingPostTraining2020,
  title = {Up or {{Down}}? {{Adaptive Rounding}} for {{Post-Training Quantization}}},
  shorttitle = {Up or {{Down}}?},
  author = {Nagel, Markus and Amjad, Rana Ali and family=Baalen, given=Mart, prefix=van, useprefix=true and Louizos, Christos and Blankevoort, Tijmen},
  date = {2020-06-30},
  eprint = {2004.10568},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2004.10568},
  url = {http://arxiv.org/abs/2004.10568},
  urldate = {2023-09-06},
  abstract = {When quantizing neural networks, assigning each floating-point weight to its nearest fixed-point value is the predominant approach. We find that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. We start by theoretically analyzing the rounding problem for a pre-trained neural network. By approximating the task loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a layer-wise local loss and propose to optimize this loss with a soft relaxation. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1\%.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\SFH222W5\\Nagel et al. - 2020 - Up or Down Adaptive Rounding for Post-Training Qu.pdf;C\:\\Users\\mospr\\Zotero\\storage\\EJKYV6BH\\2004.html}
}

@inproceedings{nagelAdaptiveRoundingPostTraining2020a,
  title = {Up or {{Down}}? {{Adaptive Rounding}} for {{Post-Training Quantization}}},
  shorttitle = {Up or {{Down}}?},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Nagel, Markus and Amjad, Rana Ali and Baalen, Mart Van and Louizos, Christos and Blankevoort, Tijmen},
  date = {2020-11-21},
  pages = {7197--7206},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/nagel20a.html},
  urldate = {2023-10-05},
  abstract = {When quantizing neural networks, assigning each floating-point weight to its nearest fixed-point value is the predominant approach. We find that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. We start by theoretically analyzing the rounding problem for a pre-trained neural network. By approximating the task loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a layer-wise local loss and propose to optimize this loss with a soft relaxation. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1\%.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CGK3ZQXP\\Nagel et al. - 2020 - Up or Down Adaptive Rounding for Post-Training Qu.pdf;C\:\\Users\\mospr\\Zotero\\storage\\SUY3W7R4\\Nagel et al. - 2020 - Up or Down Adaptive Rounding for Post-Training Qu.pdf}
}

@online{parkLUTGEMMQuantizedMatrix2023,
  title = {{{LUT-GEMM}}: {{Quantized Matrix Multiplication}} Based on {{LUTs}} for {{Efficient Inference}} in {{Large-Scale Generative Language Models}}},
  shorttitle = {{{LUT-GEMM}}},
  author = {Park, Gunho and Park, Baeseong and Kim, Minsub and Lee, Sungjae and Kim, Jeonghoon and Kwon, Beomseok and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  date = {2023-04-15},
  eprint = {2206.09557},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.09557},
  url = {http://arxiv.org/abs/2206.09557},
  urldate = {2023-09-06},
  abstract = {The recent advancements in self-supervised learning, combined with the Transformer architecture, have enabled natural language processing (NLP) to achieve remarkably low perplexity. However, powerful NLP models necessitate increasing model size, leading to substantial computational and memory requirements. In this paper, we introduce an efficient inference framework tailored for large-scale generative language models. To reduce the model size, we employ a weight-only quantization strategy while preserving full precision for activations. As a result, we attain sub-4-bit quantization for each weight through non-uniform or uniform quantization techniques. Our proposed kernel, called LUT-GEMM, then accelerates quantized matrix multiplications, offering a flexible balance between compression ratio and accuracy. Unlike earlier matrix multiplication kernels that accommodated weight-only quantization, LUT-GEMM efficiently eliminates the resource-demanding dequantization process for both uniform and non-uniform quantization methods. By reducing the latency of individual GPUs and the overall inference process for large-scale language models, LUT-GEMM provides significant performance improvements in inference. The impact of LUT-GEMM is facilitated by implementing high compression ratios through low-bit quantization and efficient LUT-based operations, which decreases the number of required GPUs. For the OPT-175B model with 3-bit quantization, we show that LUT-GEMM accelerates the latency for generating each token by 2.1x compared to OPTQ, which requires costly dequantization. Consequently, LUT-GEMM enables inference of the OPT-175B model on a single GPU without noticeable degradation in accuracy or performance, while the non-quantized OPT-175B model requires a minimum of 8 GPUs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\J74KQ39J\\Park et al. - 2023 - LUT-GEMM Quantized Matrix Multiplication based on.pdf;C\:\\Users\\mospr\\Zotero\\storage\\AK7C8Z9E\\2206.html}
}

@online{parkRelationalKnowledgeDistillation2019,
  title = {Relational {{Knowledge Distillation}}},
  author = {Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  date = {2019-05-01},
  eprint = {1904.05068},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.05068},
  url = {http://arxiv.org/abs/1904.05068},
  urldate = {2023-09-08},
  abstract = {Knowledge distillation aims at transferring knowledge acquired in one model (a teacher) to another model (a student) that is typically smaller. Previous approaches can be expressed as a form of training the student to mimic output activations of individual data examples represented by the teacher. We introduce a novel approach, dubbed relational knowledge distillation (RKD), that transfers mutual relations of data examples instead. For concrete realizations of RKD, we propose distance-wise and angle-wise distillation losses that penalize structural differences in relations. Experiments conducted on different tasks show that the proposed method improves educated student models with a significant margin. In particular for metric learning, it allows students to outperform their teachers' performance, achieving the state of the arts on standard benchmark datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\3Y6Q96R5\\Park et al. - 2019 - Relational Knowledge Distillation.pdf;C\:\\Users\\mospr\\Zotero\\storage\\KQZXS224\\1904.html}
}

@inproceedings{parkRelationalKnowledgeDistillation2019a,
  title = {Relational {{Knowledge Distillation}}},
  author = {Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  date = {2019},
  pages = {3967--3976},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\LX6GGTMJ\Park et al. - 2019 - Relational Knowledge Distillation.pdf}
}

@online{passalisHeterogeneousKnowledgeDistillation2020,
  title = {Heterogeneous {{Knowledge Distillation}} Using {{Information Flow Modeling}}},
  author = {Passalis, Nikolaos and Tzelepi, Maria and Tefas, Anastasios},
  date = {2020-05-02},
  eprint = {2005.00727},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.00727},
  url = {http://arxiv.org/abs/2005.00727},
  urldate = {2023-09-08},
  abstract = {Knowledge Distillation (KD) methods are capable of transferring the knowledge encoded in a large and complex teacher into a smaller and faster student. Early methods were usually limited to transferring the knowledge only between the last layers of the networks, while latter approaches were capable of performing multi-layer KD, further increasing the accuracy of the student. However, despite their improved performance, these methods still suffer from several limitations that restrict both their efficiency and flexibility. First, existing KD methods typically ignore that neural networks undergo through different learning phases during the training process, which often requires different types of supervision for each one. Furthermore, existing multi-layer KD methods are usually unable to effectively handle networks with significantly different architectures (heterogeneous KD). In this paper we propose a novel KD method that works by modeling the information flow through the various layers of the teacher model and then train a student model to mimic this information flow. The proposed method is capable of overcoming the aforementioned limitations by using an appropriate supervision scheme during the different phases of the training process, as well as by designing and training an appropriate auxiliary teacher model that acts as a proxy model capable of "explaining" the way the teacher works to the student. The effectiveness of the proposed method is demonstrated using four image datasets and several different evaluation setups.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\4XYFW2RE\\Passalis et al. - 2020 - Heterogeneous Knowledge Distillation using Informa.pdf;C\:\\Users\\mospr\\Zotero\\storage\\MHKC8EYZ\\2005.html}
}

@inproceedings{passalisHeterogeneousKnowledgeDistillation2020a,
  title = {Heterogeneous {{Knowledge Distillation Using Information Flow Modeling}}},
  author = {Passalis, Nikolaos and Tzelepi, Maria and Tefas, Anastasios},
  date = {2020},
  pages = {2339--2348},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Passalis_Heterogeneous_Knowledge_Distillation_Using_Information_Flow_Modeling_CVPR_2020_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\6QM4UB7E\Passalis et al. - 2020 - Heterogeneous Knowledge Distillation Using Informa.pdf}
}

@inproceedings{passalisLearningDeepRepresentations2018,
  title = {Learning {{Deep Representations}} with {{Probabilistic Knowledge Transfer}}},
  author = {Passalis, Nikolaos and Tefas, Anastasios},
  date = {2018},
  pages = {268--284},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  file = {C:\Users\mospr\Zotero\storage\F9KRDKTD\Passalis and Tefas - 2018 - Learning Deep Representations with Probabilistic K.pdf}
}

@online{passalisLearningDeepRepresentations2019,
  title = {Learning {{Deep Representations}} with {{Probabilistic Knowledge Transfer}}},
  author = {Passalis, Nikolaos and Tefas, Anastasios},
  date = {2019-03-20},
  eprint = {1803.10837},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1803.10837},
  url = {http://arxiv.org/abs/1803.10837},
  urldate = {2023-09-08},
  abstract = {Knowledge Transfer (KT) techniques tackle the problem of transferring the knowledge from a large and complex neural network into a smaller and faster one. However, existing KT methods are tailored towards classification tasks and they cannot be used efficiently for other representation learning tasks. In this paper a novel knowledge transfer technique, that is capable of training a student model that maintains the same amount of mutual information between the learned representation and a set of (possible unknown) labels as the teacher model, is proposed. Apart from outperforming existing KT techniques, the proposed method allows for overcoming several limitations of existing methods providing new insight into KT as well as novel KT applications, ranging from knowledge transfer from handcrafted feature extractors to \{cross-modal\} KT from the textual modality into the representation extracted from the visual modality of the data.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\WJFZ9ZC9\\Passalis and Tefas - 2019 - Learning Deep Representations with Probabilistic K.pdf;C\:\\Users\\mospr\\Zotero\\storage\\XCHAM8N2\\1803.html}
}

@online{passbanALPKDAttentionBasedLayer2020,
  title = {{{ALP-KD}}: {{Attention-Based Layer Projection}} for {{Knowledge Distillation}}},
  shorttitle = {{{ALP-KD}}},
  author = {Passban, Peyman and Wu, Yimeng and Rezagholizadeh, Mehdi and Liu, Qun},
  date = {2020-12-27},
  eprint = {2012.14022},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.14022},
  url = {http://arxiv.org/abs/2012.14022},
  urldate = {2023-09-08},
  abstract = {Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher's supervision for internal components. Motivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer's significance into consideration, then performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\X8HU8QYP\\Passban et al. - 2020 - ALP-KD Attention-Based Layer Projection for Knowl.pdf;C\:\\Users\\mospr\\Zotero\\storage\\NKJKQ2JD\\2012.html}
}

@article{passbanALPKDAttentionBasedLayer2021,
  title = {{{ALP-KD}}: {{Attention-Based Layer Projection}} for {{Knowledge Distillation}}},
  shorttitle = {{{ALP-KD}}},
  author = {Passban, Peyman and Wu, Yimeng and Rezagholizadeh, Mehdi and Liu, Qun},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {15},
  pages = {13657--13665},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i15.17610},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17610},
  urldate = {2023-10-05},
  abstract = {Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher’s supervision for internal components. Motivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer’s significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques.},
  issue = {15},
  langid = {english},
  keywords = {Language Models},
  file = {C:\Users\mospr\Zotero\storage\PEC522S4\Passban et al. - 2021 - ALP-KD Attention-Based Layer Projection for Knowl.pdf}
}

@inproceedings{phuongDistillationBasedTrainingMultiExit2019,
  title = {Distillation-{{Based Training}} for {{Multi-Exit Architectures}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Phuong, Mary and Lampert, Christoph},
  date = {2019-10},
  pages = {1355--1364},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2019.00144},
  abstract = {Multi-exit architectures, in which a stack of processing layers is interleaved with early output layers, allow the processing of a test example to stop early and thus save computation time and/or energy. In this work, we propose a new training procedure for multi-exit architectures based on the principle of knowledge distillation. The method encourages early exits to mimic later, more accurate exits, by matching their probability outputs. Experiments on CIFAR100 and ImageNet show that distillation-based training significantly improves the accuracy of early exits while maintaining state-of-the-art accuracy for late ones. The method is particularly beneficial when training data is limited and also allows a straight-forward extension to semi-supervised learning, i.e. make use also of unlabeled data at training time. Moreover, it takes only a few lines to implement and imposes almost no computational overhead at training time, and none at all at test time.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {Computational modeling,Computer architecture,Entropy,Predictive models,Task analysis,Training,Training data},
  file = {C:\Users\mospr\Zotero\storage\GJGL2L2G\9009834.html}
}

@online{polinoModelCompressionDistillation2018,
  title = {Model Compression via Distillation and Quantization},
  author = {Polino, Antonio and Pascanu, Razvan and Alistarh, Dan},
  date = {2018-02-15},
  eprint = {1802.05668},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.05668},
  url = {http://arxiv.org/abs/1802.05668},
  urldate = {2023-09-05},
  abstract = {Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method, differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model. We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\RVS8R4PM\\Polino et al. - 2018 - Model compression via distillation and quantizatio.pdf;C\:\\Users\\mospr\\Zotero\\storage\\RK58CML9\\1802.html}
}

@article{polinoModelCompressionDistillation2018a,
  title = {Model Compression via Distillation and Quantization},
  author = {Polino, Antonio and Pascanu, Razvan and Alistarh, Dan-Adrian},
  date = {2018},
  journaltitle = {6th International Conference on Learning Representations},
  url = {https://research-explorer.ista.ac.at/record/7812},
  urldate = {2023-10-05},
  abstract = {Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\QY42M76Z\Polino et al. - 2018 - Model compression via distillation and quantizatio.pdf}
}

@online{puSummarizationAlmostDead2023,
  title = {Summarization Is ({{Almost}}) {{Dead}}},
  author = {Pu, Xiao and Gao, Mingqi and Wan, Xiaojun},
  date = {2023-09-18},
  url = {https://arxiv.org/abs/2309.09558v1},
  urldate = {2023-10-08},
  abstract = {How well can large language models (LLMs) generate summaries? We develop new datasets and conduct human evaluation experiments to evaluate the zero-shot generation capability of LLMs across five distinct summarization tasks. Our findings indicate a clear preference among human evaluators for LLM-generated summaries over human-written summaries and summaries generated by fine-tuned models. Specifically, LLM-generated summaries exhibit better factual consistency and fewer instances of extrinsic hallucinations. Due to the satisfactory performance of LLMs in summarization tasks (even surpassing the benchmark of reference summaries), we believe that most conventional works in the field of text summarization are no longer necessary in the era of LLMs. However, we recognize that there are still some directions worth exploring, such as the creation of novel datasets with higher quality and more reliable evaluation methods.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\mospr\Zotero\storage\ARU2N3KP\Pu et al. - 2023 - Summarization is (Almost) Dead.pdf}
}

@inproceedings{reddiMLPerfInferenceBenchmark2020,
  title = {{{MLPerf Inference Benchmark}}},
  booktitle = {2020 {{ACM}}/{{IEEE}} 47th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
  date = {2020-05},
  pages = {446--459},
  doi = {10.1109/ISCA45697.2020.00045},
  abstract = {Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark’s flexibility and adaptability.},
  eventtitle = {2020 {{ACM}}/{{IEEE}} 47th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  keywords = {Benchmarking,Inference,Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\UPG9Q28G\\Reddi et al. - 2020 - MLPerf Inference Benchmark.pdf;C\:\\Users\\mospr\\Zotero\\storage\\4IM6FEIV\\9138989.html}
}

@inproceedings{reutherSurveyBenchmarkingMachine2019,
  title = {Survey and {{Benchmarking}} of {{Machine Learning Accelerators}}},
  booktitle = {2019 {{IEEE High Performance Extreme Computing Conference}} ({{HPEC}})},
  author = {Reuther, Albert and Michaleas, Peter and Jones, Michael and Gadepally, Vijay and Samsi, Siddharth and Kepner, Jeremy},
  date = {2019-09},
  eprint = {1908.11348},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--9},
  doi = {10.1109/HPEC.2019.8916327},
  url = {http://arxiv.org/abs/1908.11348},
  urldate = {2023-09-09},
  abstract = {Advances in multicore processors and accelerators have opened the flood gates to greater exploration and application of machine learning techniques to a variety of applications. These advances, along with breakdowns of several trends including Moore's Law, have prompted an explosion of processors and accelerators that promise even greater computational and machine learning capabilities. These processors and accelerators are coming in many forms, from CPUs and GPUs to ASICs, FPGAs, and dataflow accelerators. This paper surveys the current state of these processors and accelerators that have been publicly announced with performance and power consumption numbers. The performance and power values are plotted on a scatter graph and a number of dimensions and observations from the trends on this plot are discussed and analyzed. For instance, there are interesting trends in the plot regarding power consumption, numerical precision, and inference versus training. We then select and benchmark two commercially-available low size, weight, and power (SWaP) accelerators as these processors are the most interesting for embedded and mobile machine learning inference applications that are most applicable to the DoD and other SWaP constrained users. We determine how they actually perform with real-world images and neural network models, compare those results to the reported performance and power consumption values and evaluate them against an Intel CPU that is used in some embedded applications.},
  keywords = {B.8,C.4,Computer Science - Performance},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\J2CJI24R\\Reuther et al. - 2019 - Survey and Benchmarking of Machine Learning Accele.pdf;C\:\\Users\\mospr\\Zotero\\storage\\8EBJZP3N\\1908.html}
}

@online{rokhComprehensiveSurveyModel2023,
  title = {A {{Comprehensive Survey}} on {{Model Quantization}} for {{Deep Neural Networks}} in {{Image Classification}}},
  author = {Rokh, Babak and Azarpeyvand, Ali and Khanteymoori, Alireza},
  date = {2023-07-30},
  eprint = {2205.07877},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.07877},
  url = {http://arxiv.org/abs/2205.07877},
  urldate = {2023-09-06},
  abstract = {Recent advancements in machine learning achieved by Deep Neural Networks (DNNs) have been significant. While demonstrating high accuracy, DNNs are associated with a huge number of parameters and computation, which leads to high memory usage and energy consumption. As a result, deploying of DNNs on devices with constrained hardware resources poses significant challenges. To overcome this, various compression techniques have been widely employed to optimize DNN accelerators. A promising approach is quantization, in which the full-precision values are stored in low bit-width precision. Quantization not only reduces memory requirements but also replaces high-cost operations with low-cost ones. DNN quantization offers flexibility and efficiency in hardware design, making it a widely adopted technique in various methods. Since quantization has been extensively utilized in previous works, there is a need for an integrated report that provides an understanding, analysis, and comparison of different quantization approaches. Consequently, we present a comprehensive survey of quantization concepts and methods, with a focus on image classification. We describe clustering-based quantization methods and explore the use of a scale factor parameter for approximating full-precision values. Moreover, we thoroughly review the training of quantized DNN, including the use of straight-through estimator and quantized regularization. We explain the replacement of floating-point operations with low-cost bitwise operations in a quantized DNN and the sensitivity of different layers in quantization. Furthermore, we highlight the evaluation metrics for quantized methods and important benchmarks in image classification task. We also present the accuracy of the state-of-the-art methods on CIFAR-10 and ImageNet.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\VH6RGX32\\Rokh et al. - 2023 - A Comprehensive Survey on Model Quantization for D.pdf;C\:\\Users\\mospr\\Zotero\\storage\\ZP3ZN7WY\\2205.html}
}

@article{rokhComprehensiveSurveyModel2023a,
  title = {A {{Comprehensive Survey}} on {{Model Quantization}} for {{Deep Neural Networks}} in {{Image Classification}}},
  author = {Rokh, Babak and Azarpeyvand, Ali and Khanteymoori, Alireza},
  date = {2023-09-11},
  journaltitle = {ACM Transactions on Intelligent Systems and Technology},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  issn = {2157-6904},
  doi = {10.1145/3623402},
  url = {https://dl.acm.org/doi/10.1145/3623402},
  urldate = {2023-10-05},
  abstract = {Recent advancements in machine learning achieved by Deep Neural Networks (DNNs) have been significant. While demonstrating high accuracy, DNNs are associated with a huge number of parameters and computations, which leads to high memory usage and energy consumption. As a result, deploying DNNs on devices with constrained hardware resources poses significant challenges. To overcome this, various compression techniques have been widely employed to optimize DNN accelerators. A promising approach is quantization, in which the full-precision values are stored in low bit-width precision. Quantization not only reduces memory requirements but also replaces high-cost operations with low-cost ones. DNN quantization offers flexibility and efficiency in hardware design, making it a widely adopted technique in various methods. Since quantization has been extensively utilized in previous works, there is a need for an integrated report that provides an understanding, analysis, and comparison of different quantization approaches. Consequently, we present a comprehensive survey of quantization concepts and methods, with a focus on image classification. We describe clustering-based quantization methods and explore the use of a scale factor parameter for approximating full-precision values. Moreover, we thoroughly review the training of a quantized DNN, including the use of a straight-through estimator and quantized regularization. We explain the replacement of floating-point operations with low-cost bitwise operations in a quantized DNN and the sensitivity of different layers in quantization. Furthermore, we highlight the evaluation metrics for quantized methods and important benchmarks in the image classification task. We also present the accuracy of the state-of-the-art methods on CIFAR-10 and ImageNet. This paper attempts to make the readers familiar with the basic and advanced concepts of quantization, introduce important works in DNN quantization, and highlight challenges for future research in this field.},
  keywords = {Deep neural network acceleration,Discrete neural network optimization,Image classification,Model compression,Quantization},
  annotation = {Just Accepted},
  file = {C:\Users\mospr\Zotero\storage\NGP2D4K5\Rokh et al. - 2023 - A Comprehensive Survey on Model Quantization for D.pdf}
}

@online{shenQBERTHessianBased2019,
  title = {Q-{{BERT}}: {{Hessian Based Ultra Low Precision Quantization}} of {{BERT}}},
  shorttitle = {Q-{{BERT}}},
  author = {Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
  date = {2019-09-24},
  eprint = {1909.05840},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1909.05840},
  url = {http://arxiv.org/abs/1909.05840},
  urldate = {2023-09-06},
  abstract = {Transformer based architectures have become de-facto models used for a range of Natural Language Processing tasks. In particular, the BERT based models achieved significant accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However, BERT based models have a prohibitive memory footprint and latency. As a result, deploying BERT based models in resource constrained environments has become a challenging task. In this work, we perform an extensive analysis of fine-tuned BERT models using second order Hessian information, and we use our results to propose a novel method for quantizing BERT models to ultra low precision. In particular, we propose a new group-wise quantization scheme, and we use a Hessian based mix-precision method to compress the model further. We extensively test our proposed method on BERT downstream tasks of SST-2, MNLI, CoNLL-03, and SQuAD. We can achieve comparable performance to baseline with at most \$2.3\textbackslash\%\$ performance degradation, even with ultra-low precision quantization down to 2 bits, corresponding up to \$13\textbackslash times\$ compression of the model parameters, and up to \$4\textbackslash times\$ compression of the embedding table as well as activations. Among all tasks, we observed the highest performance loss for BERT fine-tuned on SQuAD. By probing into the Hessian based analysis as well as visualization, we show that this is related to the fact that current training/fine-tuning strategy of BERT does not converge for SQuAD.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\S722TF4T\\Shen et al. - 2019 - Q-BERT Hessian Based Ultra Low Precision Quantiza.pdf;C\:\\Users\\mospr\\Zotero\\storage\\DX4C8CXZ\\1909.html}
}

@article{shenQBERTHessianBased2020,
  title = {Q-{{BERT}}: {{Hessian Based Ultra Low Precision Quantization}} of {{BERT}}},
  shorttitle = {Q-{{BERT}}},
  author = {Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {8815--8821},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i05.6409},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/6409},
  urldate = {2023-10-05},
  abstract = {Transformer based architectures have become de-facto models used for a range of Natural Language Processing tasks. In particular, the BERT based models achieved significant accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However, BERT based models have a prohibitive memory footprint and latency. As a result, deploying BERT based models in resource constrained environments has become a challenging task. In this work, we perform an extensive analysis of fine-tuned BERT models using second order Hessian information, and we use our results to propose a novel method for quantizing BERT models to ultra low precision. In particular, we propose a new group-wise quantization scheme, and we use Hessian-based mix-precision method to compress the model further. We extensively test our proposed method on BERT downstream tasks of SST-2, MNLI, CoNLL-03, and SQuAD. We can achieve comparable performance to baseline with at most 2.3\% performance degradation, even with ultra-low precision quantization down to 2 bits, corresponding up to 13× compression of the model parameters, and up to 4× compression of the embedding table as well as activations. Among all tasks, we observed the highest performance loss for BERT fine-tuned on SQuAD. By probing into the Hessian based analysis as well as visualization, we show that this is related to the fact that current training/fine-tuning strategy of BERT does not converge for SQuAD.},
  issue = {05},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\97VQTVXT\Shen et al. - 2020 - Q-BERT Hessian Based Ultra Low Precision Quantiza.pdf}
}

@article{shinFixedPointPerformanceAnalysis2015,
  title = {Fixed-{{Point Performance Analysis}} of {{Recurrent Neural Networks}}},
  author = {Shin, Sungho and Hwang, Kyuyeon and Sung, Wonyong},
  date = {2015-07},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {32},
  number = {4},
  eprint = {1512.01322},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {158--158},
  issn = {1053-5888},
  doi = {10.1109/MSP.2015.2411564},
  url = {http://arxiv.org/abs/1512.01322},
  urldate = {2023-09-06},
  abstract = {Recurrent neural networks have shown excellent performance in many applications, however they require increased complexity in hardware or software based implementations. The hardware complexity can be much lowered by minimizing the word-length of weights and signals. This work analyzes the fixed-point performance of recurrent neural networks using a retrain based quantization method. The quantization sensitivity of each layer in RNNs is studied, and the overall fixed-point optimization results minimizing the capacity of weights while not sacrificing the performance are presented. A language model and a phoneme recognition examples are used.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\LTKV6PWP\\Shin et al. - 2015 - Fixed-Point Performance Analysis of Recurrent Neur.pdf;C\:\\Users\\mospr\\Zotero\\storage\\5BZZRHAI\\1512.html}
}

@online{shinKnowledgeDistillationOptimization2019,
  title = {Knowledge Distillation for Optimization of Quantized Deep Neural Networks},
  author = {Shin, Sungho and Boo, Yoonho and Sung, Wonyong},
  date = {2019-10-23},
  eprint = {1909.01688},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1909.01688},
  url = {http://arxiv.org/abs/1909.01688},
  urldate = {2023-09-08},
  abstract = {Knowledge distillation (KD) is a very popular method for model size reduction. Recently, the technique is exploited for quantized deep neural networks (QDNNs) training as a way to restore the performance sacrificed by word-length reduction. KD, however, employs additional hyper-parameters, such as temperature, coefficient, and the size of teacher network for QDNN training. We analyze the effect of these hyper-parameters for QDNN optimization with KD. We find that these hyper-parameters are inter-related, and also introduce a simple and effective technique that reduces \textbackslash textit\{coefficient\} during training. With KD employing the proposed hyper-parameters, we achieve the test accuracy of 92.7\% and 67.0\% on Resnet20 with 2-bit ternary weights for CIFAR-10 and CIFAR-100 data sets, respectively.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\A4ME3BKJ\\Shin et al. - 2019 - Knowledge distillation for optimization of quantiz.pdf;C\:\\Users\\mospr\\Zotero\\storage\\XKNCQWIP\\1909.html}
}

@online{srinivasDatafreeParameterPruning2015,
  title = {Data-Free Parameter Pruning for {{Deep Neural Networks}}},
  author = {Srinivas, Suraj and Babu, R. Venkatesh},
  date = {2015-07-22},
  eprint = {1507.06149},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1507.06149},
  url = {http://arxiv.org/abs/1507.06149},
  urldate = {2023-09-07},
  abstract = {Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85\textbackslash\% of the total parameters in an MNIST-trained network, and about 35\textbackslash\% for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\V3EH4KL5\\Srinivas and Babu - 2015 - Data-free parameter pruning for Deep Neural Networ.pdf;C\:\\Users\\mospr\\Zotero\\storage\\K7X2GYGL\\1507.html}
}

@inproceedings{stockTrainingQuantizationNoise2021,
  title = {Training with {{Quantization Noise}} for {{Extreme Model Compression}}},
  booktitle = {{{ICLR}} 2021 - {{International Conference}} on {{Learning Representations}}},
  author = {Stock, Pierre and Fan, Angela and Graham, Benjamin and Grave, Edouard and Gribonval, Rémi and Jegou, Herve and Joulin, Armand},
  date = {2021-05},
  location = {{Vienna, Austria}},
  url = {https://inria.hal.science/hal-03136442},
  urldate = {2023-10-05},
  abstract = {We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training, where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator. In this paper, we extend this approach to work with extreme compression methods where the approximations introduced by STE are severe. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5\% accuracy on MNLI by compressing RoBERTa to 14 MB and 80.0\% top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3 MB.},
  keywords = {Compression,Efficiency,Product Quantization}
}

@online{sungResiliencyDeepNeural2016,
  title = {Resiliency of {{Deep Neural Networks}} under {{Quantization}}},
  author = {Sung, Wonyong and Shin, Sungho and Hwang, Kyuyeon},
  date = {2016-01-07},
  eprint = {1511.06488},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1511.06488},
  url = {http://arxiv.org/abs/1511.06488},
  urldate = {2023-09-06},
  abstract = {The complexity of deep neural network algorithms for hardware implementation can be much lowered by optimizing the word-length of weights and signals. Direct quantization of floating-point weights, however, does not show good performance when the number of bits assigned is small. Retraining of quantized networks has been developed to relieve this problem. In this work, the effects of retraining are analyzed for a feedforward deep neural network (FFDNN) and a convolutional neural network (CNN). The network complexity is controlled to know their effects on the resiliency of quantized networks by retraining. The complexity of the FFDNN is controlled by varying the unit size in each hidden layer and the number of layers, while that of the CNN is done by modifying the feature map configuration. We find that the performance gap between the floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks exists with a fair amount in 'complexity limited' networks, but the discrepancy almost vanishes in fully complex networks whose capability is limited by the training data, rather than by the number of connections. This research shows that highly complex DNNs have the capability of absorbing the effects of severe weight quantization through retraining, but connection limited networks are less resilient. This paper also presents the effective compression ratio to guide the trade-off between the network size and the precision when the hardware resource is limited.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\INGDQEED\\Sung et al. - 2016 - Resiliency of Deep Neural Networks under Quantizat.pdf;C\:\\Users\\mospr\\Zotero\\storage\\UTUGQ3RK\\1511.html}
}

@online{sunSimpleEffectivePruning2023,
  title = {A {{Simple}} and {{Effective Pruning Approach}} for {{Large Language Models}}},
  author = {Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},
  date = {2023-06-20},
  eprint = {2306.11695},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.11695},
  url = {http://arxiv.org/abs/2306.11695},
  urldate = {2023-09-07},
  abstract = {As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prune weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method on LLaMA across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and competes favorably against recent methods involving intensive weight update. Code is available at https://github.com/locuslab/wanda.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\4TCBWK3N\\Sun et al. - 2023 - A Simple and Effective Pruning Approach for Large .pdf;C\:\\Users\\mospr\\Zotero\\storage\\EVLQFTLS\\2306.html}
}

@online{SurveyAutomatedFactChecking,
  title = {A {{Survey}} on {{Automated Fact-Checking}} | {{Transactions}} of the {{Association}} for {{Computational Linguistics}} | {{MIT Press}}},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00454/109469/A-Survey-on-Automated-Fact-Checking},
  urldate = {2023-10-08},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\38AVNHVP\\A Survey on Automated Fact-Checking  Transactions.pdf;C\:\\Users\\mospr\\Zotero\\storage\\6AYPGSYJ\\A-Survey-on-Automated-Fact-Checking.html}
}

@online{tamEvaluatingFactualConsistency2022,
  title = {Evaluating the {{Factual Consistency}} of {{Large Language Models Through Summarization}}},
  author = {Tam, Derek and Mascarenhas, Anisha and Zhang, Shiyue and Kwan, Sarah and Bansal, Mohit and Raffel, Colin},
  date = {2022-11-15},
  eprint = {2211.08412},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.08412},
  url = {http://arxiv.org/abs/2211.08412},
  urldate = {2023-10-08},
  abstract = {While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\textbackslash{} the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\E3SL4BJD\\Tam et al. - 2022 - Evaluating the Factual Consistency of Large Langua.pdf;C\:\\Users\\mospr\\Zotero\\storage\\5YWB7DQN\\2211.html}
}

@inproceedings{tamEvaluatingFactualConsistency2023,
  title = {Evaluating the {{Factual Consistency}} of {{Large Language Models Through News Summarization}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
  author = {Tam, Derek and Mascarenhas, Anisha and Zhang, Shiyue and Kwan, Sarah and Bansal, Mohit and Raffel, Colin},
  date = {2023-07},
  pages = {5220--5255},
  publisher = {{Association for Computational Linguistics}},
  location = {{Toronto, Canada}},
  doi = {10.18653/v1/2023.findings-acl.322},
  url = {https://aclanthology.org/2023.findings-acl.322},
  urldate = {2023-10-08},
  abstract = {While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of pasted macro `BENCHMARK', we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries.},
  eventtitle = {Findings 2023},
  file = {C:\Users\mospr\Zotero\storage\5PMFCYTZ\Tam et al. - 2023 - Evaluating the Factual Consistency of Large Langua.pdf}
}

@article{tavaraParallelComputingSupport2019,
  title = {Parallel {{Computing}} of {{Support Vector Machines}}: {{A Survey}}},
  shorttitle = {Parallel {{Computing}} of {{Support Vector Machines}}},
  author = {Tavara, Shirin},
  date = {2019-01-28},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {51},
  number = {6},
  pages = {123:1--123:38},
  issn = {0360-0300},
  doi = {10.1145/3280989},
  url = {https://dl.acm.org/doi/10.1145/3280989},
  urldate = {2023-09-05},
  abstract = {The immense amount of data created by digitalization requires parallel computing for machine-learning methods. While there are many parallel implementations for support vector machines (SVMs), there is no clear suggestion for every application scenario. Many factor—including optimization algorithm, problem size and dimension, kernel function, parallel programming stack, and hardware architecture—impact the efficiency of implementations. It is up to the user to balance trade-offs, particularly between computation time and classification accuracy. In this survey, we review the state-of-the-art implementations of SVMs, their pros and cons, and suggest possible avenues for future research.},
  keywords = {CPU parallelism,data movement,decomposition,Dual optimization,GPU parallelism,primal optimization,speedup},
  file = {C:\Users\mospr\Zotero\storage\LTPICCBU\Tavara - 2019 - Parallel Computing of Support Vector Machines A S.pdf}
}

@online{tianContrastiveRepresentationDistillation2022,
  title = {Contrastive {{Representation Distillation}}},
  author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  date = {2022-01-24},
  eprint = {1910.10699},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1910.10699},
  url = {http://arxiv.org/abs/1910.10699},
  urldate = {2023-09-05},
  abstract = {Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: http://github.com/HobbitLong/RepDistiller.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\8VZQUMD7\\Tian et al. - 2022 - Contrastive Representation Distillation.pdf;C\:\\Users\\mospr\\Zotero\\storage\\CNVTWBKS\\1910.html}
}

@online{umapathiMedHALTMedicalDomain2023,
  title = {Med-{{HALT}}: {{Medical Domain Hallucination Test}} for {{Large Language Models}}},
  shorttitle = {Med-{{HALT}}},
  author = {Umapathi, Logesh Kumar and Pal, Ankit and Sankarasubbu, Malaikannan},
  date = {2023-07-28},
  eprint = {2307.15343},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2307.15343},
  url = {http://arxiv.org/abs/2307.15343},
  urldate = {2023-10-07},
  abstract = {This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\28GHMEEX\\Umapathi et al. - 2023 - Med-HALT Medical Domain Hallucination Test for La.pdf;C\:\\Users\\mospr\\Zotero\\storage\\IW4MUNN9\\2307.html}
}

@online{vaderaMethodsPruningDeep2021,
  title = {Methods for {{Pruning Deep Neural Networks}}},
  author = {Vadera, Sunil and Ameen, Salem},
  date = {2021-07-30},
  eprint = {2011.00241},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.00241},
  url = {http://arxiv.org/abs/2011.00241},
  urldate = {2023-09-07},
  abstract = {This paper presents a survey of methods for pruning deep neural networks. It begins by categorising over 150 studies based on the underlying approach used and then focuses on three categories: methods that use magnitude based pruning, methods that utilise clustering to identify redundancy, and methods that use sensitivity analysis to assess the effect of pruning. Some of the key influencing studies within these categories are presented to highlight the underlying approaches and results achieved. Most studies present results which are distributed in the literature as new architectures, algorithms and data sets have developed with time, making comparison across different studied difficult. The paper therefore provides a resource for the community that can be used to quickly compare the results from many different methods on a variety of data sets, and a range of architectures, including AlexNet, ResNet, DenseNet and VGG. The resource is illustrated by comparing the results published for pruning AlexNet and ResNet50 on ImageNet and ResNet56 and VGG16 on the CIFAR10 data to reveal which pruning methods work well in terms of retaining accuracy whilst achieving good compression rates. The paper concludes by identifying some promising directions for future research.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\EW6L79B5\\Vadera and Ameen - 2021 - Methods for Pruning Deep Neural Networks.pdf;C\:\\Users\\mospr\\Zotero\\storage\\6CDFQMNV\\2011.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2023-10-05},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {C:\Users\mospr\Zotero\storage\X5H93BIT\Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-09-07},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\ABBSWAD9\\Vaswani et al. - 2023 - Attention Is All You Need.pdf;C\:\\Users\\mospr\\Zotero\\storage\\6ZZJIC7I\\1706.html}
}

@online{voitaAnalyzingMultiHeadSelfAttention2019,
  title = {Analyzing {{Multi-Head Self-Attention}}: {{Specialized Heads Do}} the {{Heavy Lifting}}, the {{Rest Can Be Pruned}}},
  shorttitle = {Analyzing {{Multi-Head Self-Attention}}},
  author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  date = {2019-06-07},
  eprint = {1905.09418},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1905.09418},
  url = {http://arxiv.org/abs/1905.09418},
  urldate = {2023-09-07},
  abstract = {Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\JVBACF4R\\Voita et al. - 2019 - Analyzing Multi-Head Self-Attention Specialized H.pdf;C\:\\Users\\mospr\\Zotero\\storage\\G363G3WG\\1905.html}
}

@online{vuFreshLLMsRefreshingLarge2023,
  title = {{{FreshLLMs}}: {{Refreshing Large Language Models}} with {{Search Engine Augmentation}}},
  shorttitle = {{{FreshLLMs}}},
  author = {Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and Luong, Thang},
  date = {2023-10-05},
  url = {https://arxiv.org/abs/2310.03214v1},
  urldate = {2023-10-08},
  abstract = {Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\mospr\Zotero\storage\4VFTFGUP\Vu et al. - 2023 - FreshLLMs Refreshing Large Language Models with S.pdf}
}

@incollection{walawalkarOnlineEnsembleModel2020,
  title = {Online {{Ensemble Model Compression}} Using {{Knowledge Distillation}}},
  author = {Walawalkar, Devesh and Shen, Zhiqiang and Savvides, Marios},
  date = {2020},
  volume = {12364},
  eprint = {2011.07449},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {18--35},
  doi = {10.1007/978-3-030-58529-7_2},
  url = {http://arxiv.org/abs/2011.07449},
  urldate = {2023-09-08},
  abstract = {This paper presents a novel knowledge distillation based model compression framework consisting of a student ensemble. It enables distillation of simultaneously learnt ensemble knowledge onto each of the compressed student models. Each model learns unique representations from the data distribution due to its distinct architecture. This helps the ensemble generalize better by combining every model's knowledge. The distilled students and ensemble teacher are trained simultaneously without requiring any pretrained weights. Moreover, our proposed method can deliver multi-compressed students with single training, which is efficient and flexible for different scenarios. We provide comprehensive experiments using state-of-the-art classification models to validate our framework's effectiveness. Notably, using our framework a 97\% compressed ResNet110 student model managed to produce a 10.64\% relative accuracy gain over its individual baseline training on CIFAR100 dataset. Similarly a 95\% compressed DenseNet-BC(k=12) model managed a 8.17\% relative accuracy gain.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\QHHRX8XA\\Walawalkar et al. - 2020 - Online Ensemble Model Compression using Knowledge .pdf;C\:\\Users\\mospr\\Zotero\\storage\\DZHG93WF\\2011.html}
}

@online{wangHAQHardwareAwareAutomated2019,
  title = {{{HAQ}}: {{Hardware-Aware Automated Quantization}} with {{Mixed Precision}}},
  shorttitle = {{{HAQ}}},
  author = {Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  date = {2019-04-06},
  eprint = {1811.08886},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1811.08886},
  url = {http://arxiv.org/abs/1811.08886},
  urldate = {2023-09-06},
  abstract = {Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\ZKE6J4QI\\Wang et al. - 2019 - HAQ Hardware-Aware Automated Quantization with Mi.pdf;C\:\\Users\\mospr\\Zotero\\storage\\LM55ZW9B\\1811.html}
}

@inproceedings{wangHAQHardwareAwareAutomated2019a,
  title = {{{HAQ}}: {{Hardware-Aware Automated Quantization With Mixed Precision}}},
  shorttitle = {{{HAQ}}},
  author = {Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  date = {2019},
  pages = {8612--8620},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\27UFSLPN\Wang et al. - 2019 - HAQ Hardware-Aware Automated Quantization With Mi.pdf}
}

@article{wangProgressiveBlockwiseKnowledge2018,
  title = {Progressive {{Blockwise Knowledge Distillation}} for {{Neural Network Acceleration}}},
  author = {Wang, Hui and Zhao, Hanbin and Li, Xi and Tan, Xu},
  date = {2018},
  pages = {2769--2775},
  url = {https://www.ijcai.org/proceedings/2018/384},
  urldate = {2023-09-08},
  abstract = {Electronic proceedings of IJCAI 2018}
}

@inproceedings{wangProgressiveBlockwiseKnowledge2018a,
  title = {Progressive Blockwise Knowledge Distillation for Neural Network Acceleration},
  booktitle = {Proceedings of the 27th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Wang, Hui and Zhao, Hanbin and Li, Xi and Tan, Xu},
  date = {2018-07-13},
  series = {{{IJCAI}}'18},
  pages = {2769--2775},
  publisher = {{AAAI Press}},
  location = {{Stockholm, Sweden}},
  abstract = {As an important and challenging problem in machine learning and computer vision, neural network acceleration essentially aims to enhance the computational efficiency without sacrificing the model accuracy too much. In this paper, we propose a progressive blockwise learning scheme for teacher-student model distillation at the subnetwork block level. The proposed scheme is able to distill the knowledge of the entire teacher network by locally extracting the knowledge of each block in terms of progressive blockwise function approximation. Furthermore, we propose a structure design criterion for the student subnetwork block, which is able to effectively preserve the original receptive field from the teacher network. Experimental results demonstrate the effectiveness of the proposed scheme against the state-of-the-art approaches.},
  isbn = {978-0-9992411-2-7}
}

@online{wangSelfConsistencyImprovesChain2023,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  date = {2023-03-07},
  eprint = {2203.11171},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.11171},
  url = {http://arxiv.org/abs/2203.11171},
  urldate = {2023-10-07},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\JDQRS983\\Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf;C\:\\Users\\mospr\\Zotero\\storage\\RMU92P6D\\2203.html}
}

@inproceedings{wangStructuredPruningLarge2020,
  title = {Structured {{Pruning}} of {{Large Language Models}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Wang, Ziheng and Wohlwend, Jeremy and Lei, Tao},
  date = {2020},
  eprint = {1910.04732},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {6151--6162},
  doi = {10.18653/v1/2020.emnlp-main.496},
  url = {http://arxiv.org/abs/1910.04732},
  urldate = {2023-09-07},
  abstract = {Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\X3ZLTKK5\\Wang et al. - 2020 - Structured Pruning of Large Language Models.pdf;C\:\\Users\\mospr\\Zotero\\storage\\4VDQDYTE\\1910.html}
}

@inproceedings{wangSuperGLUEStickierBenchmark2019,
  title = {{{SuperGLUE}}: {{A Stickier Benchmark}} for {{General-Purpose Language Understanding Systems}}},
  shorttitle = {{{SuperGLUE}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
  urldate = {2023-10-05},
  abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at https://super.gluebenchmark.com.},
  file = {C:\Users\mospr\Zotero\storage\G3A4R6NE\Wang et al. - 2019 - SuperGLUE A Stickier Benchmark for General-Purpos.pdf}
}

@online{wangSuperGLUEStickierBenchmark2020,
  title = {{{SuperGLUE}}: {{A Stickier Benchmark}} for {{General-Purpose Language Understanding Systems}}},
  shorttitle = {{{SuperGLUE}}},
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  date = {2020-02-12},
  eprint = {1905.00537},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1905.00537},
  url = {http://arxiv.org/abs/1905.00537},
  urldate = {2023-09-09},
  abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\SB8KVS5U\\Wang et al. - 2020 - SuperGLUE A Stickier Benchmark for General-Purpos.pdf;C\:\\Users\\mospr\\Zotero\\storage\\TQ6PZ4MA\\1905.html}
}

@online{wenLearningStructuredSparsity2016,
  title = {Learning {{Structured Sparsity}} in {{Deep Neural Networks}}},
  author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  date = {2016-10-18},
  eprint = {1608.03665},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1608.03665},
  url = {http://arxiv.org/abs/1608.03665},
  urldate = {2023-09-05},
  abstract = {High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25\% to 92.60\%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around \textasciitilde 1\%. Open source code is in https://github.com/wenwei202/caffe/tree/scnn},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6,I.5.1,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\IW2287DH\\Wen et al. - 2016 - Learning Structured Sparsity in Deep Neural Networ.pdf;C\:\\Users\\mospr\\Zotero\\storage\\9454X2BD\\1608.html}
}

@inproceedings{wenLearningStructuredSparsity2016a,
  title = {Learning {{Structured Sparsity}} in {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  date = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/41bfd20a38bb1b0bec75acf0845530a7-Abstract.html},
  urldate = {2023-10-05},
  abstract = {High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNN’s evaluation. Experimental results show that SSL achieves on average 5.1X and 3.1X speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth reduces a 20-layer Deep Residual Network (ResNet) to 18 layers while improves the accuracy from 91.25\% to 92.60\%, which is still higher than that of original ResNet with 32 layers. For AlexNet, SSL reduces the error by \textasciitilde 1\%.},
  file = {C:\Users\mospr\Zotero\storage\ZL9YRLYK\Wen et al. - 2016 - Learning Structured Sparsity in Deep Neural Networ.pdf}
}

@online{wuExtremeCompressionPretrained2022,
  title = {Extreme {{Compression}} for {{Pre-trained Transformers Made Simple}} and {{Efficient}}},
  author = {Wu, Xiaoxia and Yao, Zhewei and Zhang, Minjia and Li, Conglong and He, Yuxiong},
  date = {2022-06-03},
  eprint = {2206.01859},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.01859},
  url = {http://arxiv.org/abs/2206.01859},
  urldate = {2023-09-06},
  abstract = {Extreme compression, particularly ultra-low bit precision (binary/ternary) quantization, has been proposed to fit large NLP models on resource-constraint devices. However, to preserve the accuracy for such aggressive compression schemes, cutting-edge methods usually introduce complicated compression pipelines, e.g., multi-stage expensive knowledge distillation with extensive hyperparameter tuning. Also, they oftentimes focus less on smaller transformer models that have already been heavily compressed via knowledge distillation and lack a systematic study to show the effectiveness of their methods. In this paper, we perform a very comprehensive systematic study to measure the impact of many key hyperparameters and training strategies from previous works. As a result, we find out that previous baselines for ultra-low bit precision quantization are significantly under-trained. Based on our study, we propose a simple yet effective compression pipeline for extreme compression, named XTC. XTC demonstrates that (1) we can skip the pre-training knowledge distillation to obtain a 5-layer BERT while achieving better performance than previous state-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization plus layer reduction is able to reduce the model size by 50x, resulting in new state-of-the-art results on GLUE tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\VA9ELQUC\\Wu et al. - 2022 - Extreme Compression for Pre-trained Transformers M.pdf;C\:\\Users\\mospr\\Zotero\\storage\\233SVWQJ\\2206.html}
}

@online{wuQuantizedConvolutionalNeural2016,
  title = {Quantized {{Convolutional Neural Networks}} for {{Mobile Devices}}},
  author = {Wu, Jiaxiang and Leng, Cong and Wang, Yuhang and Hu, Qinghao and Cheng, Jian},
  date = {2016-05-15},
  eprint = {1512.06473},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.06473},
  url = {http://arxiv.org/abs/1512.06473},
  urldate = {2023-09-06},
  abstract = {Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4\textasciitilde 6x speed-up and 15\textasciitilde 20x compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\WB24RXHZ\\Wu et al. - 2016 - Quantized Convolutional Neural Networks for Mobile.pdf;C\:\\Users\\mospr\\Zotero\\storage\\65RRCVY5\\1512.html}
}

@inproceedings{wuQuantizedConvolutionalNeural2016a,
  title = {Quantized {{Convolutional Neural Networks}} for {{Mobile Devices}}},
  author = {Wu, Jiaxiang and Leng, Cong and Wang, Yuhang and Hu, Qinghao and Cheng, Jian},
  date = {2016},
  pages = {4820--4828},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Wu_Quantized_Convolutional_Neural_CVPR_2016_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\JB62SLEY\Wu et al. - 2016 - Quantized Convolutional Neural Networks for Mobile.pdf}
}

@article{wuXTCExtremeCompression2022,
  title = {{{XTC}}: {{Extreme Compression}} for {{Pre-trained Transformers Made Simple}} and {{Efficient}}},
  shorttitle = {{{XTC}}},
  author = {Wu, Xiaoxia and Yao, Zhewei and Zhang, Minjia and Li, Conglong and He, Yuxiong},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {3217--3231},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/1579d5d8edacd85ac1a86aea28bdf32d-Abstract-Conference.html},
  urldate = {2023-10-05},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\UI5TNNFP\Wu et al. - 2022 - XTC Extreme Compression for Pre-trained Transform.pdf}
}

@inproceedings{xuConvolutionalNeuralNetwork2020,
  title = {Convolutional {{Neural Network Pruning}}: {{A Survey}}},
  shorttitle = {Convolutional {{Neural Network Pruning}}},
  booktitle = {2020 39th {{Chinese Control Conference}} ({{CCC}})},
  author = {Xu, Sheng and Huang, Anran and Chen, Lei and Zhang, Baochang},
  date = {2020-07},
  pages = {7458--7463},
  issn = {1934-1768},
  doi = {10.23919/CCC50068.2020.9189610},
  abstract = {Deep convolutional neural networks have enabled remarkable progress over the last years on a variety of visual tasks, such as image recognition, speech recognition, and machine translation. These tasks contribute many to machine intelligence. However, developments of deep convolutional neural networks to a machine terminal remains challenging due to massive number of parameters and float operations that a typical model contains. Therefore, there is growing interest in convolutional neural network pruning. Existing work in this field of research can be categorized according to three dimensions: pruning method, training strategy, estimation criterion.},
  eventtitle = {2020 39th {{Chinese Control Conference}} ({{CCC}})},
  keywords = {Computational modeling,Computer architecture,convolutional neural networks,Convolutional neural networks,Estimation,estimation criterion,Kernel,machine intelligence,pruning method,Task analysis,Training,training strategy},
  file = {C:\Users\mospr\Zotero\storage\2ZW8UHKP\9189610.html}
}

@online{xuCriticalEvaluationEvaluations2023,
  title = {A {{Critical Evaluation}} of {{Evaluations}} for {{Long-form Question Answering}}},
  author = {Xu, Fangyuan and Song, Yixiao and Iyyer, Mohit and Choi, Eunsol},
  date = {2023-05-29},
  eprint = {2305.18201},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.18201},
  url = {http://arxiv.org/abs/2305.18201},
  urldate = {2023-10-07},
  abstract = {Long-form question answering (LFQA) enables answering a wide range of questions, but its flexibility poses enormous challenges for evaluation. We perform the first targeted study of the evaluation of long-form answers, covering both human and automatic evaluation practices. We hire domain experts in seven areas to provide preference judgments over pairs of answers, along with free-form justifications for their choices. We present a careful analysis of experts' evaluation, which focuses on new aspects such as the comprehensiveness of the answer. Next, we examine automatic text generation metrics, finding that no existing metrics are predictive of human preference judgments. However, some metrics correlate with fine-grained aspects of answers (e.g., coherence). We encourage future work to move away from a single "overall score" of the answer and adopt a multi-faceted evaluation, targeting aspects such as factuality and completeness. We publicly release all of our annotations and code to spur future work into LFQA evaluation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\F79XAKNC\\Xu et al. - 2023 - A Critical Evaluation of Evaluations for Long-form.pdf;C\:\\Users\\mospr\\Zotero\\storage\\UQ5Q35JR\\2305.html}
}

@online{yangQuantizationNetworks2019,
  title = {Quantization {{Networks}}},
  author = {Yang, Jiwei and Shen, Xu and Xing, Jun and Tian, Xinmei and Li, Houqiang and Deng, Bing and Huang, Jianqiang and Hua, Xiansheng},
  date = {2019-11-27},
  eprint = {1911.09464},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1911.09464},
  url = {http://arxiv.org/abs/1911.09464},
  urldate = {2023-09-06},
  abstract = {Although deep neural networks are highly effective, their high computational and memory costs severely challenge their applications on portable devices. As a consequence, low-bit quantization, which converts a full-precision neural network into a low-bitwidth integer version, has been an active and promising research topic. Existing methods formulate the low-bit quantization of networks as an approximation or optimization problem. Approximation-based methods confront the gradient mismatch problem, while optimization-based methods are only suitable for quantizing weights and could introduce high computational cost in the training stage. In this paper, we propose a novel perspective of interpreting and implementing neural network quantization by formulating low-bit quantization as a differentiable non-linear function (termed quantization function). The proposed quantization function can be learned in a lossless and end-to-end manner and works for any weights and activations of neural networks in a simple and uniform way. Extensive experiments on image classification and object detection tasks show that our quantization networks outperform the state-of-the-art methods. We believe that the proposed method will shed new insights on the interpretation of neural network quantization. Our code is available at https://github.com/aliyun/alibabacloud-quantization-networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\KHA9PWV3\\Yang et al. - 2019 - Quantization Networks.pdf;C\:\\Users\\mospr\\Zotero\\storage\\NXWNW5CK\\1911.html}
}

@inproceedings{yangQuantizationNetworks2019a,
  title = {Quantization {{Networks}}},
  author = {Yang, Jiwei and Shen, Xu and Xing, Jun and Tian, Xinmei and Li, Houqiang and Deng, Bing and Huang, Jianqiang and Hua, Xian-sheng},
  date = {2019},
  pages = {7308--7316},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Quantization_Networks_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\DQULL2GQ\Yang et al. - 2019 - Quantization Networks.pdf}
}

@online{yangSnapshotDistillationTeacherStudent2018,
  title = {Snapshot {{Distillation}}: {{Teacher-Student Optimization}} in {{One Generation}}},
  shorttitle = {Snapshot {{Distillation}}},
  author = {Yang, Chenglin and Xie, Lingxi and Su, Chi and Yuille, Alan L.},
  date = {2018-11-30},
  eprint = {1812.00123},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.00123},
  url = {http://arxiv.org/abs/1812.00123},
  urldate = {2023-09-08},
  abstract = {Optimizing a deep neural network is a fundamental task in computer vision, yet direct training methods often suffer from over-fitting. Teacher-student optimization aims at providing complementary cues from a model trained previously, but these approaches are often considerably slow due to the pipeline of training a few generations in sequence, i.e., time complexity is increased by several times. This paper presents snapshot distillation (SD), the first framework which enables teacher-student optimization in one generation. The idea of SD is very simple: instead of borrowing supervision signals from previous generations, we extract such information from earlier epochs in the same generation, meanwhile make sure that the difference between teacher and student is sufficiently large so as to prevent under-fitting. To achieve this goal, we implement SD in a cyclic learning rate policy, in which the last snapshot of each cycle is used as the teacher for all iterations in the next cycle, and the teacher signal is smoothed to provide richer information. In standard image classification benchmarks such as CIFAR100 and ILSVRC2012, SD achieves consistent accuracy gain without heavy computational overheads. We also verify that models pre-trained with SD transfers well to object detection and semantic segmentation in the PascalVOC dataset.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\L4KHVMQB\\Yang et al. - 2018 - Snapshot Distillation Teacher-Student Optimizatio.pdf;C\:\\Users\\mospr\\Zotero\\storage\\Q65ZG4UL\\1812.html}
}

@inproceedings{yangSnapshotDistillationTeacherStudent2019,
  title = {Snapshot {{Distillation}}: {{Teacher-Student Optimization}} in {{One Generation}}},
  shorttitle = {Snapshot {{Distillation}}},
  author = {Yang, Chenglin and Xie, Lingxi and Su, Chi and Yuille, Alan L.},
  date = {2019},
  pages = {2859--2868},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Snapshot_Distillation_Teacher-Student_Optimization_in_One_Generation_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\BK678KHJ\Yang et al. - 2019 - Snapshot Distillation Teacher-Student Optimizatio.pdf}
}

@online{yaoHAWQV3DyadicNeural2021,
  title = {{{HAWQV3}}: {{Dyadic Neural Network Quantization}}},
  shorttitle = {{{HAWQV3}}},
  author = {Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael W. and Keutzer, Kurt},
  date = {2021-06-23},
  eprint = {2011.10680},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.10680},
  url = {http://arxiv.org/abs/2011.10680},
  urldate = {2023-09-06},
  abstract = {Current low-precision quantization algorithms often have the hidden cost of conversion back and forth from floating point to quantized integer values. This hidden cost limits the latency improvement realized by quantizing Neural Networks. To address this, we present HAWQV3, a novel mixed-precision integer-only quantization framework. The contributions of HAWQV3 are the following: (i) An integer-only inference where the entire computational graph is performed only with integer multiplication, addition, and bit shifting, without any floating point operations or even integer division; (ii) A novel hardware-aware mixed-precision quantization method where the bit-precision is calculated by solving an integer linear programming problem that balances the trade-off between model perturbation and other constraints, e.g., memory footprint and latency; (iii) Direct hardware deployment and open source contribution for 4-bit uniform/mixed-precision quantization in TVM, achieving an average speed up of \$1.45\textbackslash times\$ for uniform 4-bit, as compared to uniform 8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed methods on ResNet18/50 and InceptionV3, for various model compression levels with/without mixed precision. For ResNet50, our INT8 quantization achieves an accuracy of \$77.58\textbackslash\%\$, which is \$2.68\textbackslash\%\$ higher than prior integer-only work, and our mixed-precision INT4/8 quantization can reduce INT8 latency by \$23\textbackslash\%\$ and still achieve \$76.73\textbackslash\%\$ accuracy. Our framework and the TVM implementation have been open sourced.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\LXNKVG5J\\Yao et al. - 2021 - HAWQV3 Dyadic Neural Network Quantization.pdf;C\:\\Users\\mospr\\Zotero\\storage\\FZDHV5J4\\2011.html}
}

@online{yaoHAWQV3DyadicNeural2021a,
  title = {{{HAWQV3}}: {{Dyadic Neural Network Quantization}}},
  shorttitle = {{{HAWQV3}}},
  author = {Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael W. and Keutzer, Kurt},
  date = {2021-06-23},
  eprint = {2011.10680},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.10680},
  url = {http://arxiv.org/abs/2011.10680},
  urldate = {2023-09-06},
  abstract = {Current low-precision quantization algorithms often have the hidden cost of conversion back and forth from floating point to quantized integer values. This hidden cost limits the latency improvement realized by quantizing Neural Networks. To address this, we present HAWQV3, a novel mixed-precision integer-only quantization framework. The contributions of HAWQV3 are the following: (i) An integer-only inference where the entire computational graph is performed only with integer multiplication, addition, and bit shifting, without any floating point operations or even integer division; (ii) A novel hardware-aware mixed-precision quantization method where the bit-precision is calculated by solving an integer linear programming problem that balances the trade-off between model perturbation and other constraints, e.g., memory footprint and latency; (iii) Direct hardware deployment and open source contribution for 4-bit uniform/mixed-precision quantization in TVM, achieving an average speed up of \$1.45\textbackslash times\$ for uniform 4-bit, as compared to uniform 8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed methods on ResNet18/50 and InceptionV3, for various model compression levels with/without mixed precision. For ResNet50, our INT8 quantization achieves an accuracy of \$77.58\textbackslash\%\$, which is \$2.68\textbackslash\%\$ higher than prior integer-only work, and our mixed-precision INT4/8 quantization can reduce INT8 latency by \$23\textbackslash\%\$ and still achieve \$76.73\textbackslash\%\$ accuracy. Our framework and the TVM implementation have been open sourced.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\YDIBA8HZ\\Yao et al. - 2021 - HAWQV3 Dyadic Neural Network Quantization.pdf;C\:\\Users\\mospr\\Zotero\\storage\\EXV3G3UT\\2011.html}
}

@inproceedings{yaoHAWQV3DyadicNeural2021b,
  title = {{{HAWQ-V3}}: {{Dyadic Neural Network Quantization}}},
  shorttitle = {{{HAWQ-V3}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael and Keutzer, Kurt},
  date = {2021-07-01},
  pages = {11875--11886},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/yao21a.html},
  urldate = {2023-10-05},
  abstract = {Current low-precision quantization algorithms often have the hidden cost of conversion back and forth from floating point to quantized integer values. This hidden cost limits the latency improvement realized by quantizing Neural Networks. To address this, we present HAWQ-V3, a novel mixed-precision integer-only quantization framework. The contributions of HAWQ-V3 are the following: (i) An integer-only inference where the entire computational graph is performed only with integer multiplication, addition, and bit shifting, without any floating point operations or even integer division; (ii) A novel hardware-aware mixed-precision quantization method where the bit-precision is calculated by solving an integer linear programming problem that balances the trade-off between model perturbation and other constraints, e.g., memory footprint and latency; (iii) Direct hardware deployment and open source contribution for 4-bit uniform/mixed-precision quantization in TVM, achieving an average speed up of 1.45x for uniform 4-bit, as compared to uniform 8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed methods on ResNet18/50 and InceptionV3, for various model compression levels with/without mixed precision. For ResNet50, our INT8 quantization achieves an accuracy of 77.58\%, which is 2.68\% higher than prior integer-only work, and our mixed-precision INT4/8 quantization can reduce INT8 latency by 23\% and still achieve 76.73\% accuracy. Our framework and the TVM implementation have been open sourced (HAWQ, 2020).},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\4YGHLSDI\\Yao et al. - 2021 - HAWQ-V3 Dyadic Neural Network Quantization.pdf;C\:\\Users\\mospr\\Zotero\\storage\\TE5Q7HCU\\Yao et al. - 2021 - HAWQ-V3 Dyadic Neural Network Quantization.pdf}
}

@online{yaoZeroQuantEfficientAffordable2022,
  title = {{{ZeroQuant}}: {{Efficient}} and {{Affordable Post-Training Quantization}} for {{Large-Scale Transformers}}},
  shorttitle = {{{ZeroQuant}}},
  author = {Yao, Zhewei and Aminabadi, Reza Yazdani and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  date = {2022-06-03},
  eprint = {2206.01861},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.01861},
  url = {http://arxiv.org/abs/2206.01861},
  urldate = {2023-09-06},
  abstract = {How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements. In this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an end-to-end quantization and inference pipeline with three main components: (1) a fine-grained hardware-friendly quantization scheme for both weight and activations; (2) a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data; (3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead. As such, we are able to show that: (1) ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference; (2) ZeroQuant plus LKD affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model; (3) ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J6B and GPT-NeoX20, for which our INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\VQQ8VUHE\\Yao et al. - 2022 - ZeroQuant Efficient and Affordable Post-Training .pdf;C\:\\Users\\mospr\\Zotero\\storage\\6AHAI7XC\\2206.html}
}

@article{yaoZeroQuantEfficientAffordable2022a,
  title = {{{ZeroQuant}}: {{Efficient}} and {{Affordable Post-Training Quantization}} for {{Large-Scale Transformers}}},
  shorttitle = {{{ZeroQuant}}},
  author = {Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {27168--27183},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html},
  urldate = {2023-10-05},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\NJHZDA9M\Yao et al. - 2022 - ZeroQuant Efficient and Affordable Post-Training .pdf}
}

@online{yueAutomaticEvaluationAttribution2023,
  title = {Automatic {{Evaluation}} of {{Attribution}} by {{Large Language Models}}},
  author = {Yue, Xiang and Wang, Boshi and Zhang, Kai and Chen, Ziru and Su, Yu and Sun, Huan},
  date = {2023-05-10},
  eprint = {2305.06311},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.06311},
  url = {http://arxiv.org/abs/2305.06311},
  urldate = {2023-10-07},
  abstract = {A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support their claims. However, evaluating the attribution, i.e., verifying whether the generated statement is indeed fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution by LLMs. We begin by providing a definition of attribution and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks, such as question answering, fact-checking, natural language inference, and summarization. To facilitate the evaluation, we manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on the curated test set and simulated test examples from existing benchmark questions highlight both promising signals as well as remaining challenges for the automatic evaluation of attribution. We hope our testbed, modeling methodology, and insights will help lay the foundation for future studies on this important problem.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\IA8NMFET\\Yue et al. - 2023 - Automatic Evaluation of Attribution by Large Langu.pdf;C\:\\Users\\mospr\\Zotero\\storage\\8FCYMEM8\\2305.html}
}

@article{yuWidthDepthPruning2022,
  title = {Width \& {{Depth Pruning}} for {{Vision Transformers}}},
  author = {Yu, Fang and Huang, Kun and Wang, Meng and Cheng, Yuan and Chu, Wei and Cui, Li},
  date = {2022-06-28},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {3},
  pages = {3143--3151},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i3.20222},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/20222},
  urldate = {2023-09-07},
  abstract = {Transformer models have demonstrated their promising potential and achieved excellent performance on a series of computer vision tasks. However, the huge computational cost of vision transformers hinders their deployment and application to edge devices. Recent works have proposed to find and remove the unimportant units of vision transformers. Despite achieving remarkable results, these methods take one dimension of network width into consideration and ignore network depth, which is another important dimension for pruning vision transformers. Therefore, we propose a Width \& Depth Pruning (WDPruning) framework that reduces both width and depth dimensions simultaneously. Specifically, for width pruning, a set of learnable pruning-related parameters is used to adaptively adjust the width of transformer. For depth pruning, we introduce several shallow classifiers by using the intermediate information of the transformer blocks, which allows images to be classified by shallow classifiers instead of the deeper classifiers. In the inference period, all of the blocks after shallow classifiers can be dropped so they don’t bring additional parameters and computation. Experimental results on benchmark datasets demonstrate that the proposed method can significantly reduce the computational costs of mainstream vision transformers such as DeiT and Swin Transformer with a minor accuracy drop. In particular, on ILSVRC-12, we achieve over 22\% pruning ratio of FLOPs by compressing DeiT-Base, even with an increase of 0.14\% Top-1 accuracy.},
  issue = {3},
  langid = {english},
  keywords = {Computer Vision (CV)},
  file = {C:\Users\mospr\Zotero\storage\2E5ZBPSL\Yu et al. - 2022 - Width & Depth Pruning for Vision Transformers.pdf}
}

@article{zhangAdversarialCodistillationLearning2021,
  title = {Adversarial Co-Distillation Learning for Image Recognition},
  author = {Zhang, Haoran and Hu, Zhenzhen and Qin, Wei and Xu, Mingliang and Wang, Meng},
  date = {2021-03-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {111},
  pages = {107659},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2020.107659},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320320304623},
  urldate = {2023-09-08},
  abstract = {Knowledge distillation is an effective way to transfer the knowledge from a pre-trained teacher model to a student model. Co-distillation, as an online variant of distillation, further accelerates the training process and paves a new way to explore the “dark knowledge” by training n models in parallel. In this paper, we explore the “divergent examples”, which can make the classifiers have different predictions and thus induce the “dark knowledge”, and we propose a novel approach named Adversarial Co-distillation Networks (ACNs) to enhance the “dark knowledge” by generating extra divergent examples. Note that we do not involve any extra dataset, and we only utilize the standard training set to train the entire framework. ACNs are end-to-end frameworks composed of two parts: an adversarial phase consisting of Generative Adversarial Networks (GANs) to generate the divergent examples and a co-distillation phase consisting of multiple classifiers to learn the divergent examples. These two phases are learned in an iterative and adversarial way. To guarantee the quality of the divergent examples and the stability of ACNs, we further design “Weakly Residual Connection” module and “Restricted Adversarial Search” module to assist in the training process. Extensive experiments with various deep architectures on different datasets well demonstrate the effectiveness of our approach.},
  keywords = {Data augmentation,Divergent examples,Generative adversarial nets,Image classification,Knowledge distillation},
  file = {C:\Users\mospr\Zotero\storage\LT6XEV3R\S0031320320304623.html}
}

@online{zhangBeYourOwn2019,
  title = {Be {{Your Own Teacher}}: {{Improve}} the {{Performance}} of {{Convolutional Neural Networks}} via {{Self Distillation}}},
  shorttitle = {Be {{Your Own Teacher}}},
  author = {Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
  date = {2019-05-17},
  eprint = {1905.08094},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1905.08094},
  url = {http://arxiv.org/abs/1905.08094},
  urldate = {2023-09-08},
  abstract = {Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65\%, varying from 0.61\% in ResNeXt as minimum to 4.07\% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices.Our codes will be released on github soon.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\ALRBFY9K\\Zhang et al. - 2019 - Be Your Own Teacher Improve the Performance of Co.pdf;C\:\\Users\\mospr\\Zotero\\storage\\DQCAJ7HD\\1905.html}
}

@inproceedings{zhangBeYourOwn2019a,
  title = {Be {{Your Own Teacher}}: {{Improve}} the {{Performance}} of {{Convolutional Neural Networks}} via {{Self Distillation}}},
  shorttitle = {Be {{Your Own Teacher}}},
  author = {Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
  date = {2019},
  pages = {3713--3722},
  url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  file = {C:\Users\mospr\Zotero\storage\DTGLZWY3\Zhang et al. - 2019 - Be Your Own Teacher Improve the Performance of Co.pdf}
}

@online{zhangDeepMutualLearning2017,
  title = {Deep {{Mutual Learning}}},
  author = {Zhang, Ying and Xiang, Tao and Hospedales, Timothy M. and Lu, Huchuan},
  date = {2017-06-01},
  eprint = {1706.00384},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.00384},
  url = {http://arxiv.org/abs/1706.00384},
  urldate = {2023-09-08},
  abstract = {Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, that is better suited to low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy where, rather than one way transfer between a static pre-defined teacher and a student, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on CIFAR-100 recognition and Market-1501 person re-identification benchmarks. Surprisingly, it is revealed that no prior powerful teacher network is necessary -- mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CZFU4IIC\\Zhang et al. - 2017 - Deep Mutual Learning.pdf;C\:\\Users\\mospr\\Zotero\\storage\\4I39K8PW\\1706.html}
}

@online{zhangFastHumanPose2019,
  title = {Fast {{Human Pose Estimation}}},
  author = {Zhang, Feng and Zhu, Xiatian and Ye, Mao},
  date = {2019-04-02},
  eprint = {1811.05419},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1811.05419},
  url = {http://arxiv.org/abs/1811.05419},
  urldate = {2023-09-08},
  abstract = {Existing human pose estimation approaches often only consider how to improve the model generalisation performance, but putting aside the significant efficiency problem. This leads to the development of heavy models with poor scalability and cost-effectiveness in practical use. In this work, we investigate the under-studied but practically critical pose model efficiency problem. To this end, we present a new Fast Pose Distillation (FPD) model learning strategy. Specifically, the FPD trains a lightweight pose neural network architecture capable of executing rapidly with low computational cost. It is achieved by effectively transferring the pose structure knowledge of a strong teacher network. Extensive evaluations demonstrate the advantages of our FPD method over a broad range of state-of-the-art pose estimation approaches in terms of model cost-effectiveness on two standard benchmark datasets, MPII Human Pose and Leeds Sports Pose.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\WRIRS95M\\Zhang et al. - 2019 - Fast Human Pose Estimation.pdf;C\:\\Users\\mospr\\Zotero\\storage\\YK6HSU8U\\1811.html}
}

@inproceedings{zhangFastHumanPose2019a,
  title = {Fast {{Human Pose Estimation}}},
  author = {Zhang, Feng and Zhu, Xiatian and Ye, Mao},
  date = {2019},
  pages = {3517--3526},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\3ASHSL5B\Zhang et al. - 2019 - Fast Human Pose Estimation.pdf}
}

@software{zhangLlmhallucinationsurvey2023,
  title = {Llm-Hallucination-Survey},
  author = {Zhang, Yue},
  date = {2023-10-07T09:49:23Z},
  origdate = {2023-07-10T07:24:36Z},
  url = {https://github.com/HillZhang1999/llm-hallucination-survey},
  urldate = {2023-10-07},
  abstract = {Reading list of hallucination in LLMs. Check out our new survey paper: "Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models"},
  keywords = {awesome-list,large-language-models,reading-list}
}

@online{zhangLQNetsLearnedQuantization2018,
  title = {{{LQ-Nets}}: {{Learned Quantization}} for {{Highly Accurate}} and {{Compact Deep Neural Networks}}},
  shorttitle = {{{LQ-Nets}}},
  author = {Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
  date = {2018-07-26},
  eprint = {1807.10029},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1807.10029},
  url = {http://arxiv.org/abs/1807.10029},
  urldate = {2023-09-06},
  abstract = {Although weight and activation quantization is an effective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using fixed, handcrafted quantization schemes such as uniform or logarithmic quantization. Our method for learning the quantizers applies to both network weights and activations with arbitrary-bit precision, and our quantizers are easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets show that our method works consistently well for various network structures such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous quantization methods in terms of accuracy by an appreciable margin. Code available at https://github.com/Microsoft/LQ-Nets},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\IJ8D98P6\\Zhang et al. - 2018 - LQ-Nets Learned Quantization for Highly Accurate .pdf;C\:\\Users\\mospr\\Zotero\\storage\\XNYIBL7J\\1807.html}
}

@inproceedings{zhangLQNetsLearnedQuantization2018a,
  title = {{{LQ-Nets}}: {{Learned Quantization}} for {{Highly Accurate}} and {{Compact Deep Neural Networks}}},
  shorttitle = {{{LQ-Nets}}},
  author = {Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
  date = {2018},
  pages = {365--382},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  file = {C:\Users\mospr\Zotero\storage\KLJHV3K7\Zhang et al. - 2018 - LQ-Nets Learned Quantization for Highly Accurate .pdf}
}

@online{zhangSirenSongAI2023,
  title = {Siren's {{Song}} in the {{AI Ocean}}: {{A Survey}} on {{Hallucination}} in {{Large Language Models}}},
  shorttitle = {Siren's {{Song}} in the {{AI Ocean}}},
  author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
  date = {2023-09-24},
  eprint = {2309.01219},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.01219},
  url = {http://arxiv.org/abs/2309.01219},
  urldate = {2023-10-07},
  abstract = {While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\P2B2FPWZ\\Zhang et al. - 2023 - Siren's Song in the AI Ocean A Survey on Hallucin.pdf;C\:\\Users\\mospr\\Zotero\\storage\\3PXDICGI\\2309.html}
}

@online{zhaoContrastiveKnowledgeTransfer2023,
  title = {A {{Contrastive Knowledge Transfer Framework}} for {{Model Compression}} and {{Transfer Learning}}},
  author = {Zhao, Kaiqi and Chen, Yitao and Zhao, Ming},
  date = {2023-03-13},
  eprint = {2303.07599},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.07599},
  url = {http://arxiv.org/abs/2303.07599},
  urldate = {2023-09-05},
  abstract = {Knowledge Transfer (KT) achieves competitive performance and is widely used for image classification tasks in model compression and transfer learning. Existing KT works transfer the information from a large model ("teacher") to train a small model ("student") by minimizing the difference of their conditionally independent output distributions. However, these works overlook the high-dimension structural knowledge from the intermediate representations of the teacher, which leads to limited effectiveness, and they are motivated by various heuristic intuitions, which makes it difficult to generalize. This paper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which enables the transfer of sufficient structural knowledge from the teacher to the student by optimizing multiple contrastive objectives across the intermediate representations between them. Also, CKTF provides a generalized agreement to existing KT techniques and increases their performance significantly by deriving them as specific cases of CKTF. The extensive evaluation shows that CKTF consistently outperforms the existing KT works by 0.04\% to 11.59\% in model compression and by 0.4\% to 4.75\% in transfer learning on various models and datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\3JWPJL4C\\Zhao et al. - 2023 - A Contrastive Knowledge Transfer Framework for Mod.pdf;C\:\\Users\\mospr\\Zotero\\storage\\FXNLRV5H\\2303.html}
}

@inproceedings{zhaoContrastiveKnowledgeTransfer2023a,
  title = {A {{Contrastive Knowledge Transfer Framework}} for {{Model Compression}} and {{Transfer Learning}}},
  booktitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhao, Kaiqi and Chen, Yitao and Zhao, Ming},
  date = {2023-06},
  pages = {1--5},
  doi = {10.1109/ICASSP49357.2023.10095744},
  url = {https://ieeexplore.ieee.org/abstract/document/10095744},
  urldate = {2023-10-05},
  abstract = {Knowledge Transfer (KT) achieves competitive performance and is widely used for image classification tasks in model compression and transfer learning. Existing KT works transfer the information from a large model ("teacher") to train a small model ("student") by minimizing the difference of their conditionally independent output distributions. However, these works overlook the high-dimension structural knowledge from the intermediate representations of the teacher, which leads to limited effectiveness, and they are motivated by various heuristic intuitions, which makes it difficult to generalize. This paper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which enables the transfer of sufficient structural knowledge from the teacher to the student by optimizing multiple contrastive objectives across the intermediate representations between them. Also, CKTF provides a generalized agreement to existing KT techniques and increases their performance significantly by deriving them as specific cases of CKTF. The extensive evaluation shows that CKTF consistently outperforms the existing KT works by 0.04\% to 11.59\% in model compression and by 0.4\% to 4.75\% in transfer learning on various models and datasets.},
  eventtitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C:\Users\mospr\Zotero\storage\SACWIK9M\Zhao et al. - 2023 - A Contrastive Knowledge Transfer Framework for Mod.pdf}
}

@online{zhaoLLMCalibrationAutomatic2023,
  title = {{{LLM Calibration}} and {{Automatic Hallucination Detection}} via {{Pareto Optimal Self-supervision}}},
  author = {Zhao, Theodore and Wei, Mu and Preston, J. Samuel and Poon, Hoifung},
  date = {2023-07-06},
  eprint = {2306.16564},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2306.16564},
  url = {http://arxiv.org/abs/2306.16564},
  urldate = {2023-10-07},
  abstract = {Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in significant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\HEU8DI9J\\Zhao et al. - 2023 - LLM Calibration and Automatic Hallucination Detect.pdf;C\:\\Users\\mospr\\Zotero\\storage\\B5GI7BYI\\2306.html}
}

@inproceedings{zhaoVariationalConvolutionalNeural2019,
  title = {Variational {{Convolutional Neural Network Pruning}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhao, Chenglong and Ni, Bingbing and Zhang, Jian and Zhao, Qiwei and Zhang, Wenjun and Tian, Qi},
  date = {2019-06},
  pages = {2775--2784},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00289},
  abstract = {We propose a variational Bayesian scheme for pruning convolutional neural networks in channel level. This idea is motivated by the fact that deterministic value based pruning methods are inherently improper and unstable. In a nutshell, variational technique is introduced to estimate distribution of a newly proposed parameter, called channel saliency, based on this, redundant channels can be removed from model via a simple criterion. The advantages are two-fold: 1) Our method conducts channel pruning without desire of re-training stage, thus improving the computation efficiency. 2) Our method is implemented as a stand-alone module, called variational pruning layer, which can be straightforwardly inserted into off-the-shelf deep learning packages, without any special network design. Extensive experimental results well demonstrate the effectiveness of our method: For CIFAR-10, we perform channel removal on different CNN models up to 74\% reduction, which results in significant size reduction and computation saving. For ImageNet, about 40\% channels of ResNet-50 are removed without compromising accuracy.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Computer Vision Theory,Deep Learning},
  file = {C:\Users\mospr\Zotero\storage\Q5USH4KY\8954234.html}
}

@online{zhouHULKEnergyEfficiency2020,
  title = {{{HULK}}: {{An Energy Efficiency Benchmark Platform}} for {{Responsible Natural Language Processing}}},
  shorttitle = {{{HULK}}},
  author = {Zhou, Xiyou and Chen, Zhiyu and Jin, Xiaoyong and Wang, William Yang},
  date = {2020-02-13},
  eprint = {2002.05829},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2002.05829},
  url = {http://arxiv.org/abs/2002.05829},
  urldate = {2023-09-09},
  abstract = {Computation-intensive pretrained models have been taking the lead of many natural language processing benchmarks such as GLUE. However, energy efficiency in the process of model training and inference becomes a critical bottleneck. We introduce HULK, a multi-task energy efficiency benchmarking platform for responsible natural language processing. With HULK, we compare pretrained models' energy efficiency from the perspectives of time and cost. Baseline benchmarking results are provided for further analysis. The fine-tuning efficiency of different pretrained models can differ a lot among different tasks and fewer parameter number does not necessarily imply better efficiency. We analyzed such phenomenon and demonstrate the method of comparing the multi-task efficiency of pretrained models. Our platform is available at https://sites.engineering.ucsb.edu/\textasciitilde xiyou/hulk/.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\JLBTRW3T\\Zhou et al. - 2020 - HULK An Energy Efficiency Benchmark Platform for .pdf;C\:\\Users\\mospr\\Zotero\\storage\\UZT3P5HD\\2002.html}
}

@online{zhuangStructuredBinaryNeural2018,
  title = {Structured {{Binary Neural Networks}} for {{Accurate Image Classification}} and {{Semantic Segmentation}}},
  author = {Zhuang, Bohan and Shen, Chunhua and Tan, Mingkui and Liu, Lingqiao and Reid, Ian},
  date = {2018-11-26},
  eprint = {1811.10413},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1811.10413},
  url = {http://arxiv.org/abs/1811.10413},
  urldate = {2023-09-06},
  abstract = {In this paper, we propose to train convolutional neural networks (CNNs) with both binarized weights and activations, leading to quantized models specifically\vphantom\{\} for mobile devices with limited power capacity and computation resources. Previous works on quantizing CNNs seek to approximate the floating-point information using a set of discrete values, which we call value approximation, but typically assume the same architecture as the full-precision networks. In this paper, however, we take a novel 'structure approximation' view for quantization---it is very likely that a different architecture may be better for best performance. In particular, we propose a `network decomposition' strategy, named \textbackslash textbf\{Group-Net\}, in which we divide the network into groups. In this way, each full-precision group can be effectively reconstructed by aggregating a set of homogeneous binary branches. In addition, we learn effective connections among groups to improve the representational capability. Moreover, the proposed Group-Net shows strong generalization to other tasks. For instance, we extend Group-Net for highly accurate semantic segmentation by embedding rich context into the binary structure. Experiments on both classification and semantic segmentation tasks demonstrate the superior performance of the proposed methods over various popular architectures. In particular, we outperform the previous best binary neural networks in terms of accuracy and major computation savings.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\2IDYE5BR\\Zhuang et al. - 2018 - Structured Binary Neural Networks for Accurate Ima.pdf;C\:\\Users\\mospr\\Zotero\\storage\\X5NAXE58\\1811.html}
}

@inproceedings{zhuangStructuredBinaryNeural2019,
  title = {Structured {{Binary Neural Networks}} for {{Accurate Image Classification}} and {{Semantic Segmentation}}},
  author = {Zhuang, Bohan and Shen, Chunhua and Tan, Mingkui and Liu, Lingqiao and Reid, Ian},
  date = {2019},
  pages = {413--422},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhuang_Structured_Binary_Neural_Networks_for_Accurate_Image_Classification_and_Semantic_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\N8DNXDX8\Zhuang et al. - 2019 - Structured Binary Neural Networks for Accurate Ima.pdf}
}

@online{zhuPruneNotPrune2017,
  title = {To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},
  shorttitle = {To Prune, or Not to Prune},
  author = {Zhu, Michael and Gupta, Suyog},
  date = {2017-11-13},
  eprint = {1710.01878},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1710.01878},
  url = {http://arxiv.org/abs/1710.01878},
  urldate = {2023-09-05},
  abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\KQ4HIQZN\\Zhu and Gupta - 2017 - To prune, or not to prune exploring the efficacy .pdf;C\:\\Users\\mospr\\Zotero\\storage\\A4IKDXHX\\1710.html}
}

@article{zhuPruneNotPrune2018,
  title = {To {{Prune}}, or {{Not}} to {{Prune}}: {{Exploring}} the {{Efficacy}} of {{Pruning}} for {{Model Compression}}},
  shorttitle = {To {{Prune}}, or {{Not}} to {{Prune}}},
  author = {Zhu, Michael H. and Gupta, Suyog},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=S1lN69AT-},
  urldate = {2023-10-05},
  abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\4SCC7SSP\Zhu and Gupta - 2018 - To Prune, or Not to Prune Exploring the Efficacy .pdf}
}

@online{zhuSurveyModelCompression2023,
  title = {A {{Survey}} on {{Model Compression}} for {{Large Language Models}}},
  author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  date = {2023-08-17},
  eprint = {2308.07633},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.07633},
  url = {http://arxiv.org/abs/2308.07633},
  urldate = {2023-09-06},
  abstract = {Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of compressed LLMs. By providing insights into the latest developments and practical implications, this survey serves as an invaluable resource for both researchers and practitioners. As LLMs continue to evolve, this survey aims to facilitate enhanced efficiency and real-world applicability, establishing a foundation for future advancements in the field.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\6DSWIABA\\Zhu et al. - 2023 - A Survey on Model Compression for Large Language M.pdf;C\:\\Users\\mospr\\Zotero\\storage\\VRGW2SGT\\2308.html}
}

@online{zhuVisionTransformerPruning2021,
  title = {Vision {{Transformer Pruning}}},
  author = {Zhu, Mingjian and Tang, Yehui and Han, Kai},
  date = {2021-08-14},
  eprint = {2104.08500},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.08500},
  url = {http://arxiv.org/abs/2104.08500},
  urldate = {2023-09-07},
  abstract = {Vision transformer has achieved competitive performance on a variety of computer vision applications. However, their storage, run-time memory, and computational demands are hindering the deployment to mobile devices. Here we present a vision transformer pruning approach, which identifies the impacts of dimensions in each layer of transformer and then executes pruning accordingly. By encouraging dimension-wise sparsity in the transformer, important dimensions automatically emerge. A great number of dimensions with small importance scores can be discarded to achieve a high pruning ratio without significantly compromising accuracy. The pipeline for vision transformer pruning is as follows: 1) training with sparsity regularization; 2) pruning dimensions of linear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of the proposed algorithm are well evaluated and analyzed on ImageNet dataset to demonstrate the effectiveness of our proposed method.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\K3DT7H3F\\Zhu et al. - 2021 - Vision Transformer Pruning.pdf;C\:\\Users\\mospr\\Zotero\\storage\\V9U4P6SL\\2104.html}
}
