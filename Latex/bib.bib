@article{shah2022supranormal,
  title={Supranormal left ventricular ejection fraction, stroke volume, and cardiovascular risk: findings from population-based cohort studies},
  author={Shah, Sonia and Segar, Matthew W and Kondamudi, Nitin and Ayers, Colby and Chandra, Alvin and Matulevicius, Susan and Agusala, Kartik and Peshock, Ron and Abbara, Suhny and Michos, Erin D and others},
  journal={Heart Failure},
  volume={10},
  number={8},
  pages={583--594},
  year={2022},
  publisher={American College of Cardiology Foundation Washington DC}
}

@online{shinKnowledgeDistillationOptimization2019,
	title = {Knowledge Distillation for Optimization of Quantized Deep Neural Networks},
	author = {Shin, Sungho and Boo, Yoonho and Sung, Wonyong},
	date = {2019-10-23},
	eprint = {1909.01688},
	eprinttype = {arxiv},
	eprintclass = {cs, stat},
	doi = {10.48550/arXiv.1909.01688},
	url = {http://arxiv.org/abs/1909.01688},
	urldate = {2023-09-08},
	abstract = {Knowledge distillation (KD) is a very popular method for model size reduction. Recently, the technique is exploited for quantized deep neural networks (QDNNs) training as a way to restore the performance sacrificed by word-length reduction. KD, however, employs additional hyper-parameters, such as temperature, coefficient, and the size of teacher network for QDNN training. We analyze the effect of these hyper-parameters for QDNN optimization with KD. We find that these hyper-parameters are inter-related, and also introduce a simple and effective technique that reduces \textbackslash textit\{coefficient\} during training. With KD employing the proposed hyper-parameters, we achieve the test accuracy of 92.7\% and 67.0\% on Resnet20 with 2-bit ternary weights for CIFAR-10 and CIFAR-100 data sets, respectively.},
	pubstate = {preprint},
	keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
	file = {C\:\\Users\\mospr\\Zotero\\storage\\A4ME3BKJ\\Shin et al. - 2019 - Knowledge distillation for optimization of quantiz.pdf;C\:\\Users\\mospr\\Zotero\\storage\\XKNCQWIP\\1909.html}
}
@online{anilLargeScaleDistributed2018,
  title = {Large Scale Distributed Neural Network Training through Online Distillation},
  author = {Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre and Ormandi, Robert and Dahl, George E. and Hinton, Geoffrey E.},
  date = {2018-04-01},
  doi = {10.48550/arXiv.1804.03235},
  url = {https://ui.adsabs.harvard.edu/abs/2018arXiv180403235A},
  urldate = {2023-10-05},
  abstract = {Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing \$6\textbackslash times 10\^\{11\}\$ tokens and based on the Common Crawl repository of web data.},
  organization = {{arXiv e-prints}},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ADS Bibcode: 2018arXiv180403235A},
  file = {C:\Users\mospr\Zotero\storage\BV42K38E\Anil et al. - 2018 - Large scale distributed neural network training th.pdf}
}

@inproceedings{anilLargeScaleDistributed2018a,
  title = {Large Scale Distributed Neural Network Training through Online Distillation},
  author = {Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre Tachard and Ormandi, Robert and Dahl, George and Hinton, Geoffrey},
  date = {2018},
  url = {https://openreview.net/pdf?id=rkr1UDeC-},
  urldate = {2023-10-05}
}

@online{anilLargeScaleDistributed2020,
  title = {Large Scale Distributed Neural Network Training through Online Distillation},
  author = {Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre and Ormandi, Robert and Dahl, George E. and Hinton, Geoffrey E.},
  date = {2020-08-20},
  eprint = {1804.03235},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1804.03235},
  url = {http://arxiv.org/abs/1804.03235},
  urldate = {2023-09-08},
  abstract = {Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing \$6\textbackslash times 10\^\{11\}\$ tokens and based on the Common Crawl repository of web data.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\BEMU4QIZ\\Anil et al. - 2020 - Large scale distributed neural network training th.pdf;C\:\\Users\\mospr\\Zotero\\storage\\W2BBPW2K\\1804.html}
}


@inproceedings{baDeepNetsReally2014,
  title = {Do {{Deep Nets Really Need}} to Be {{Deep}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ba, Jimmy and Caruana, Rich},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.},
  file = {C:\Users\mospr\Zotero\storage\YTT25NH2\Ba and Caruana - 2014 - Do Deep Nets Really Need to be Deep.pdf}
}

@inproceedings{bannerPosttraining4bitQuantization2019,
  title = {Post Training 4-Bit Quantization of Convolutional Networks for Rapid-Deployment},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Banner, Ron and Nahshan, Yury and Soudry, Daniel},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/c0a62e133894cdce435bcb4a5df1db2d-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Convolutional neural networks require significant memory bandwidth and storage for intermediate computations, apart from substantial computing resources. Neural network quantization has significant benefits in reducing the amount of intermediate results, but it often requires the full datasets and time-consuming fine tuning to recover the accuracy lost after quantization. This paper introduces the first practical 4-bit post training quantization approach: it does not involve training the quantized model (fine-tuning), nor it requires the availability of the full dataset. We target the quantization of both activations and weights and suggest three complementary methods for minimizing quantization error at the tensor level, two of whom obtain a closed-form analytical solution. Combining these methods, our approach achieves accuracy that is just a few percents less the state-of-the-art baseline across a wide range of convolutional models. The source code to replicate all experiments is available on GitHub: \textbackslash url\{https://github.com/submission2019/cnn-quantization\}.},
  file = {C:\Users\mospr\Zotero\storage\865M3I9M\Banner et al. - 2019 - Post training 4-bit quantization of convolutional .pdf}
}



@online{bengioEstimatingPropagatingGradients2013,
  title = {Estimating or {{Propagating Gradients Through Stochastic Neurons}} for {{Conditional Computation}}},
  author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
  year = {2013},
  eprint = {1308.3432},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1308.3432},
  url = {http://arxiv.org/abs/1308.3432},
  urldate = {2023-09-06},
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{\textbackslash em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\5UYYRCX3\\Bengio et al. - 2013 - Estimating or Propagating Gradients Through Stocha.pdf;C\:\\Users\\mospr\\Zotero\\storage\\9VV6HHHD\\1308.html}
}


@inproceedings{beyerKnowledgeDistillationGood2022,
  title = {Knowledge {{Distillation}}: {{A Good Teacher Is Patient}} and {{Consistent}}},
  shorttitle = {Knowledge {{Distillation}}},
  author = {Beyer, Lucas and Zhai, Xiaohua and Royer, Amélie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander},
  year = {2022},
  pages = {10925--10934},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Beyer_Knowledge_Distillation_A_Good_Teacher_Is_Patient_and_Consistent_CVPR_2022_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\YD9V4I5E\Beyer et al. - 2022 - Knowledge Distillation A Good Teacher Is Patient .pdf}
}

@inproceedings{bhalgatLSQImprovingLowbit2020,
	title={Lsq+: Improving low-bit quantization through learnable offsets and better initialization},
	author={Bhalgat, Yash and Lee, Jinwon and Nagel, Markus and Blankevoort, Tijmen and Kwak, Nojun},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
	pages={696--697},
	year={2020}
}


@article{blalockWhatStateNeural2020,
	title={What is the state of neural network pruning?},
	author={Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
	journal={Proceedings of machine learning and systems},
	volume={2},
	pages={129--146},
	year={2020}
}


@article{chenCrossLayerDistillationSemantic2021,
  title = {Cross-{{Layer Distillation}} with {{Semantic Calibration}}},
  author = {Chen, Defang and Mei, Jian-Ping and Zhang, Yuan and Wang, Can and Wang, Zhe and Feng, Yan and Chen, Chun},
  year = {2021},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {8},
  pages = {7028--7036},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i8.16865},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/16865},
  urldate = {2023-10-05},
  abstract = {Recently proposed knowledge distillation approaches based on feature-map transfer validate that intermediate layers of a teacher model can serve as effective targets for training a student model to obtain better generalization ability. Existing studies mainly focus on particular representation forms for knowledge transfer between manually specified pairs of teacher-student intermediate layers. However, semantics of intermediate layers may vary in different networks and manual association of layers might lead to negative regularization caused by semantic mismatch between certain teacher-student layer pairs. To address this problem, we propose Semantic Calibration for Cross-layer Knowledge Distillation (SemCKD), which automatically assigns proper target layers of the teacher model for each student layer with an attention mechanism. With a learned attention distribution, each student layer distills knowledge contained in multiple layers rather than a single fixed intermediate layer from the teacher model for appropriate cross-layer supervision in training. Consistent improvements over state-of-the-art approaches are observed in extensive experiments with various network architectures for teacher and student models, demonstrating the effectiveness and flexibility of the proposed attention based soft layer association mechanism for cross-layer distillation.},
  issue = {8},
  langid = {english},
  keywords = {Learning \& Optimization for CV},
  file = {C:\Users\mospr\Zotero\storage\APCPUBSU\Chen et al. - 2021 - Cross-Layer Distillation with Semantic Calibration.pdf}
}

@article{chengModelCompressionAcceleration2018,
  title = {Model {{Compression}} and {{Acceleration}} for {{Deep Neural Networks}}: {{The Principles}}, {{Progress}}, and {{Challenges}}},
  shorttitle = {Model {{Compression}} and {{Acceleration}} for {{Deep Neural Networks}}},
  author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  year = {2018},
  booktitle = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {126--136},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2765695},
  abstract = {In recent years, deep neural networks (DNNs) have received increased attention, have been applied to different applications, and achieved dramatic accuracy improvements in many tasks. These works rely on deep networks with millions or even billions of parameters, and the availability of graphics processing units (GPUs) with very high computation capability plays a key role in their success. For example, Krizhevsky et al. achieved breakthrough results in the 2012 ImageNet Challenge using a network containing 60 million parameters with five convolutional layers and three fully connected layers. Usually, it takes two to three days to train the whole model on the ImagetNet data set with an NVIDIA K40 machine. In another example, the top face-verification results from the Labeled Faces in the Wild (LFW) data set were obtained with networks containing hundreds of millions of parameters, using a mix of convolutional, locally connected, and fully connected layers. It is also very time-consuming to train such a model to obtain a reasonable performance. In architectures that only rely on fully connected layers, the number of parameters can grow to billions.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {Computational modeling,Convolution,Convolutional codes,Machine learning,Neural networks,Quantization (signal),Training data},
  file = {C:\Users\mospr\Downloads\Model_Compression_and_Acceleration_for_Deep_Neural_Networks_The_Principles_Progress_and_Challenges.pdf}
}

@online{chengSurveyModelCompression2020,
  title = {A {{Survey}} of {{Model Compression}} and {{Acceleration}} for {{Deep Neural Networks}}},
  author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  year = {2020},
  eprint = {1710.09282},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1710.09282},
  url = {http://arxiv.org/abs/1710.09282},
  urldate = {2023-09-05},
  abstract = {Deep neural networks (DNNs) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. During the past five years, tremendous progress has been made in this area. In this paper, we review the recent techniques for compacting and accelerating DNN models. In general, these techniques are divided into four categories: parameter pruning and quantization, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and quantization are described first, after that the other techniques are introduced. For each category, we also provide insightful analysis about the performance, related applications, advantages, and drawbacks. Then we go through some very recent successful methods, for example, dynamic capacity networks and stochastic depths networks. After that, we survey the evaluation matrices, the main datasets used for evaluating the model performance, and recent benchmark efforts. Finally, we conclude this paper, discuss remaining the challenges and possible directions for future work.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\RDN44QRU\\Cheng et al. - 2020 - A Survey of Model Compression and Acceleration for.pdf;C\:\\Users\\mospr\\Zotero\\storage\\WJVG3SRV\\1710.html}
}

@inproceedings{chenLearningEfficientObject2017,
  title = {Learning Efficient Object Detection Models with Knowledge Distillation},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
  year = {2017},
  series = {{{NIPS}}'17},
  pages = {742--751},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {Despite significant accuracy improvement in convolutional neural networks (CNN) based object detectors, they often require prohibitive runtimes to process an image for real-time applications. State-of-the-art models often use very deep networks with a large number of floating point operations. Efforts such as model compression learn compact models with fewer number of parameters, but with much reduced accuracy. In this work, we propose a new framework to learn compact and fast object detection networks with improved accuracy using knowledge distillation [20] and hint learning [34]. Although knowledge distillation has demonstrated excellent improvements for simpler classification setups, the complexity of detection poses new challenges in the form of regression, region proposals and less voluminous labels. We address this through several innovations such as a weighted cross-entropy loss to address class imbalance, a teacher bounded loss to handle the regression component and adaptation layers to better learn from intermediate teacher distributions. We conduct comprehensive empirical evaluation with different distillation configurations over multiple datasets including PASCAL, KITTI, ILSVRC and MS-COCO. Our results show consistent improvement in accuracy-speed trade-offs for modern multi-class detection models.},
  isbn = {978-1-5108-6096-4},
  file = {C:\Users\mospr\Zotero\storage\BW7388AA\Chen et al. - 2017 - Learning efficient object detection models with kn.pdf}
}


@article{chenLearningStudentNetworks2021,
  title = {Learning {{Student Networks}} via {{Feature Embedding}}},
  author = {Chen, Hanting and Wang, Yunhe and Xu, Chang and Xu, Chao and Tao, Dacheng},
  year = {2021},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {1},
  pages = {25--35},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.2970494},
  url = {https://ieeexplore.ieee.org/abstract/document/9007474},
  urldate = {2023-10-05},
  abstract = {Deep convolutional neural networks have been widely used in numerous applications, but their demanding storage and computational resource requirements prevent their applications on mobile devices. Knowledge distillation aims to optimize a portable student network by taking the knowledge from a well-trained heavy teacher network. Traditional teacher-student-based methods used to rely on additional fully connected layers to bridge intermediate layers of teacher and student networks, which brings in a large number of auxiliary parameters. In contrast, this article aims to propagate information from teacher to student without introducing new variables that need to be optimized. We regard the teacher-student paradigm from a new perspective of feature embedding. By introducing the locality preserving loss, the student network is encouraged to generate the low-dimensional features that could inherit intrinsic properties of their corresponding high-dimensional features from the teacher network. The resulting portable network, thus, can naturally maintain the performance as that of the teacher network. Theoretical analysis is provided to justify the lower computation complexity of the proposed method. Experiments on benchmark data sets and well-trained networks suggest that the proposed algorithm is superior to state-of-the-art teacher-student learning methods in terms of computational and storage complexity.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  file = {C:\Users\mospr\Zotero\storage\LTXMCJYG\Chen et al. - 2021 - Learning Student Networks via Feature Embedding.pdf}
}

@inproceedings{choiLimitNetworkQuantization2016,
  title = {Towards the {{Limit}} of {{Network Quantization}}},
  author = {Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
  year = {2016},
  url = {https://openreview.net/forum?id=rJ8uNptgl},
  urldate = {2023-10-05},
  abstract = {Network quantization is one of network compression techniques to reduce the redundancy of deep neural networks. It reduces the number of distinct network parameter values by quantization in order to save the storage for them. In this paper, we design network quantization schemes that minimize the performance loss due to quantization given a compression ratio constraint. We analyze the quantitative relation of quantization errors to the neural network loss function and identify that the Hessian-weighted distortion measure is locally the right objective function for the optimization of network quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize. When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm. Finally, using the simple uniform quantization followed by Huffman coding, we show from our experiments that the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet, 32-layer ResNet and AlexNet, respectively.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\4VMGNB9P\Choi et al. - 2016 - Towards the Limit of Network Quantization.pdf}
}

@article{choudharyComprehensiveSurveyModel2020,
  title = {A Comprehensive Survey on Model Compression and Acceleration},
  author = {Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan},
  year = {2020},
  journal = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {53},
  number = {7},
  pages = {5113--5155},
  issn = {1573-7462},
  doi = {10.1007/s10462-020-09816-7},
  url = {https://doi.org/10.1007/s10462-020-09816-7},
  urldate = {2023-09-05},
  abstract = {In recent years, machine learning (ML) and deep learning (DL) have shown remarkable improvement in computer vision, natural language processing, stock prediction, forecasting, and audio processing to name a few. The size of the trained DL model is large for these complex tasks, which makes it difficult to deploy on resource-constrained devices. For instance, size of the pre-trained VGG16 model trained on the ImageNet dataset is more than 500~MB. Resource-constrained devices such as mobile phones and internet of things devices have limited memory and less computation power. For real-time applications, the trained models should be deployed on resource-constrained devices. Popular convolutional neural network models have millions of parameters that leads to increase in the size of the trained model. Hence, it becomes essential to compress and accelerate these models before deploying on resource-constrained devices while making the least compromise with the model accuracy. It is a challenging task to retain the same accuracy after compressing the model. To address this challenge, in the last couple of years many researchers have suggested different techniques for model compression and acceleration. In this paper, we have presented a survey of various techniques suggested for compressing and accelerating the ML and DL models. We have also discussed the challenges of the existing techniques and have provided future research directions in the field.},
  langid = {english},
  keywords = {CNN,Deep learning,Efficient neural networks,Machine learning,Model compression and acceleration,Resource-constrained devices,RNN},
  file = {C:\Users\mospr\Downloads\s10462-020-09816-7.pdf}
}



@inproceedings{cordtsCityscapesDatasetSemantic2016,
  title = {The {{Cityscapes Dataset}} for {{Semantic Urban Scene Understanding}}},
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  year = {2016},
  pages = {3213--3223},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\6PJYH2WS\Cordts et al. - 2016 - The Cityscapes Dataset for Semantic Urban Scene Un.pdf}
}

@online{courbariauxTrainingDeepNeural2015,
  title = {Training Deep Neural Networks with Low Precision Multiplications},
  author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  year = {2015},
  eprint = {1412.7024},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.7024},
  url = {http://arxiv.org/abs/1412.7024},
  urldate = {2023-09-06},
  abstract = {Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\QVRATDP4\\Courbariaux et al. - 2015 - Training deep neural networks with low precision m.pdf;C\:\\Users\\mospr\\Zotero\\storage\\UICDERYY\\1412.html}
}

@article{cuiJointStructuredPruning2021,
  title = {Joint Structured Pruning and Dense Knowledge Distillation for Efficient Transformer Model Compression},
  author = {Cui, Baiyun and Li, Yingming and Zhang, Zhongfei},
  year = {2021},
  booktitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {458},
  pages = {56--69},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.05.084},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221008390},
  urldate = {2023-09-07},
  abstract = {In this paper, we develop a novel Joint Model Compression (referred to as JMC) method by combining structured pruning and dense knowledge distillation techniques to significantly compress original large language model into a deep compressed shallow network. In particular, a new Direct Importance-aware Structured Pruning (referred as DISP) approach is proposed to structurally prune the redundant structures in the Transformer networks directly based on the corresponding parameter matrices in the model. Besides, a Dense Knowledge Distillation (referred to as DKD) method is developed with a many-to-one layer mapping strategy to leverage more comprehensive layer-wise linguistic knowledge for the distillation. Further, the proposed structured pruning and dense knowledge distillation are integrated together to perform the joint compression, which enables us to achieve a significant compression without sacrificing model accuracy. The extensive experimental results across four NLP tasks on seven datasets demonstrate its effectiveness and superiority to the baselines, while maintaining similar performance to original large model with further remarkable benefits for inference-time speedup and memory efficiency.},
  keywords = {Knowledge Distillation,Structured Pruning,Transformer Model Compression}
}

@inproceedings{li2023curriculum,
	title={Curriculum temperature for knowledge distillation},
	author={Li, Zheng and Li, Xiang and Yang, Lingfeng and Zhao, Borui and Song, Renjie and Luo, Lei and Li, Jun and Yang, Jian},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={37},
	number={2},
	pages={1504--1512},
	year={2023}
}

@online{CVPR2018Open,
  title = {{{CVPR}} 2018 {{Open Access Repository}}},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.html},
  urldate = {2023-10-05},
  file = {C:\Users\mospr\Zotero\storage\JYRGYZXA\Zhang_Deep_Mutual_Learning_CVPR_2018_paper.html}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {C:\Users\mospr\Zotero\storage\ADGSLBBG\5206848.html}
}

@inproceedings{denilPredictingParametersDeep2013,
  title = {Predicting {{Parameters}} in {{Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc' Aurelio and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2013/hash/7fec306d1e665bc9c748b5d2b99a6e97-Abstract.html},
  urldate = {2023-10-05},
  abstract = {We demonstrate that there is significant redundancy in the parameterization of   several deep learning models.  Given only a few weight values for each feature   it is possible to accurately predict the remaining values.  Moreover, we show   that not only can the parameter values be predicted, but many of them need not   be learned at all.  We train several different architectures by learning only   a small number of weights and predicting the rest.  In the best case we are   able to predict more than 95\% of the weights of a network without any drop in   accuracy.},
  file = {C:\Users\mospr\Zotero\storage\DI5WKH57\Denil et al. - 2013 - Predicting Parameters in Deep Learning.pdf}
}


@online{dettmersLLMInt88bit2022,
  title = {{{LLM}}.Int8(): 8-Bit {{Matrix Multiplication}} for {{Transformers}} at {{Scale}}},
  shorttitle = {{{LLM}}.Int8()},
  author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  year = {2022},
  eprint = {2208.07339},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.07339},
  url = {http://arxiv.org/abs/2208.07339},
  urldate = {2023-09-06},
  abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\Q4HSKJ3M\\Dettmers et al. - 2022 - LLM.int8() 8-bit Matrix Multiplication for Transf.pdf;C\:\\Users\\mospr\\Zotero\\storage\\B6NAATHA\\2208.html}
}

@online{dettmersSpQRSparseQuantizedRepresentation2023,
  title = {{{SpQR}}: {{A Sparse-Quantized Representation}} for {{Near-Lossless LLM Weight Compression}}},
  shorttitle = {{{SpQR}}},
  author = {Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  year = {2023},
  eprint = {2306.03078},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.03078},
  url = {http://arxiv.org/abs/2306.03078},
  urldate = {2023-09-05},
  abstract = {Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1\% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15\% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\GCHVUTKN\\Dettmers et al. - 2023 - SpQR A Sparse-Quantized Representation for Near-L.pdf;C\:\\Users\\mospr\\Zotero\\storage\\MJ7XZDSJ\\2306.html}
}


@inproceedings{dongHAWQV2HessianAware2020,
  title = {{{HAWQ-V2}}: {{Hessian Aware}} Trace-{{Weighted Quantization}} of {{Neural Networks}}},
  shorttitle = {{{HAWQ-V2}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dong, Zhen and Yao, Zhewei and Arfeen, Daiyaan and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  year = {2020},
  volume = {33},
  pages = {18518--18529},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/d77c703536718b95308130ff2e5cf9ee-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Quantization is an effective method for reducing memory footprint and inference time of Neural Networks. However, ultra low precision quantization could lead to significant degradation in model accuracy. A promising method to address this is to perform mixed-precision quantization, where more sensitive layers are kept at higher precision. However, the search space for a mixed-precision quantization is exponential in the number of layers. Recent work has proposed a novel Hessian based framework, with the aim of reducing this exponential search space by using second-order information. While promising, this prior work has three major limitations: (i) they only use a heuristic metric based on top Hessian eigenvalue as a measure of sensitivity and do not consider the rest of the Hessian spectrum; (ii) their approach only provides relative sensitivity of different layers and therefore requires a manual selection of the mixed-precision setting; and (iii) they do not consider mixed-precision activation quantization. Here, we present HAWQ-V2 which addresses these shortcomings. For (i), we theoretically prove that the right sensitivity metric is the average Hessian trace, instead of just top Hessian eigenvalue. For (ii), we develop a Pareto frontier based method for automatic bit precision selection of different layers without any manual intervention. For (iii), we develop the first Hessian based analysis for mixed-precision activation quantization, which is very beneficial for object detection. We show that HAWQ-V2 achieves new state-of-the-art results for a wide range of tasks. In particular, we present quantization results for InceptionV3, ResNet50, and SqueezeNext, all without any manual bit selection. Furthermore, we present results for object detection on Microsoft COCO, where we achieve 2.6 higher mAP than direct uniform quantization and 1.6 higher mAP than the recently proposed method of FQN, with a smaller model size of 17.9MB.},
  file = {C:\Users\mospr\Zotero\storage\UB4YWASD\Dong et al. - 2020 - HAWQ-V2 Hessian Aware trace-Weighted Quantization.pdf}
}


@inproceedings{fangDepGraphAnyStructural2023a,
  title = {{{DepGraph}}: {{Towards Any Structural Pruning}}},
  shorttitle = {{{DepGraph}}},
  author = {Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  year = {2023},
  pages = {16091--16101},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.html},
  urldate = {2023-10-05},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\8SJIBTP9\Fang et al. - 2023 - DepGraph Towards Any Structural Pruning.pdf}
}



@inproceedings{frankleLotteryTicketHypothesis2018,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2018},
  url = {https://openreview.net/forum?id=rJl-b3RcF7},
  urldate = {2023-10-05},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\D9FJTVT2\Frankle and Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf}
}


@online{frankleStabilizingLotteryTicket2020,
  title = {Stabilizing the {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
  year = {2020},
  eprint = {1903.01611},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1903.01611},
  url = {http://arxiv.org/abs/1903.01611},
  urldate = {2023-09-07},
  abstract = {Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the "lottery ticket hypothesis" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1\% to 7\% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80\% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork "stability," finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\T9W2Q85Z\\Frankle et al. - 2020 - Stabilizing the Lottery Ticket Hypothesis.pdf;C\:\\Users\\mospr\\Zotero\\storage\\NM84UFJA\\1903.html}
}

@online{frantarGPTQAccuratePostTraining2023,
  title = {{{GPTQ}}: {{Accurate Post-Training Quantization}} for {{Generative Pre-trained Transformers}}},
  shorttitle = {{{GPTQ}}},
  author = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  year = {2023},
  eprint = {2210.17323},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.17323},
  url = {http://arxiv.org/abs/2210.17323},
  urldate = {2023-09-06},
  abstract = {Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\77CH5696\\Frantar et al. - 2023 - GPTQ Accurate Post-Training Quantization for Gene.pdf;C\:\\Users\\mospr\\Zotero\\storage\\WBC2Z38P\\2210.html}
}

@article{frantarOptimalBrainCompression2022,
  title = {Optimal {{Brain Compression}}: {{A Framework}} for {{Accurate Post-Training Quantization}} and {{Pruning}}},
  shorttitle = {Optimal {{Brain Compression}}},
  author = {Frantar, Elias and Alistarh, Dan},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {4475--4488},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/1caf09c9f4e6b0150b06a07e77f2710c-Abstract-Conference.html},
  urldate = {2023-10-05},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\EZBD2MFN\Frantar and Alistarh - 2022 - Optimal Brain Compression A Framework for Accurat.pdf}
}


@online{gholamiSurveyQuantizationMethods2021,
  title = {A {{Survey}} of {{Quantization Methods}} for {{Efficient Neural Network Inference}}},
  author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
  year = {2021},
  doi = {10.48550/arXiv.2103.13630},
  url = {https://ui.adsabs.harvard.edu/abs/2021arXiv210313630G},
  urldate = {2023-09-06},
  abstract = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.},
  organization = {{arXiv e-prints}},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ADS Bibcode: 2021arXiv210313630G},
  file = {C:\Users\mospr\Zotero\storage\353TPEQA\Gholami et al. - 2021 - A Survey of Quantization Methods for Efficient Neu.pdf}
}

@article{gouKnowledgeDistillationSurvey2021,
  title = {Knowledge {{Distillation}}: {{A Survey}}},
  shorttitle = {Knowledge {{Distillation}}},
  author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
  year = {2021},
  journal = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {129},
  number = {6},
  pages = {1789--1819},
  issn = {1573-1405},
  doi = {10.1007/s11263-021-01453-z},
  url = {https://doi.org/10.1007/s11263-021-01453-z},
  urldate = {2023-09-05},
  abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
  langid = {english},
  keywords = {Deep neural networks,Knowledge distillation,Knowledge transfer,Model compression,Teacher–student architecture},
  file = {C:\Users\mospr\Zotero\storage\RL2BWDS5\Gou et al. - 2021 - Knowledge Distillation A Survey.pdf}
}



@online{guKnowledgeDistillationLarge2023,
  title = {Knowledge {{Distillation}} of {{Large Language Models}}},
  author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  year = {2023},
  eprint = {2306.08543},
  eprinttype = {arxiv},
  eprintclass = {cs},
  note = {10.48550/arXiv.2306.08543},
  url = {http://arxiv.org/abs/2306.08543},
  urldate = {2023-09-08},
  abstract = {Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge from white-box generative LLMs is still under-explored, which becomes more and more important with the prosperity of LLMs. In this work, we propose MiniLLM that distills smaller language models from generative larger language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. Extensive experiments in the instruction-following setting show that the MiniLLM models generate more precise responses with the higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance. Our method is also scalable for different model families with 120M to 13B parameters. We will release our code and model checkpoints at https://aka.ms/MiniLLM.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\WRLKJCPL\\Gu et al. - 2023 - Knowledge Distillation of Large Language Models.pdf;C\:\\Users\\mospr\\Zotero\\storage\\22UMCNLP\\2306.html}
}


@inproceedings{guoDynamicNetworkSurgery2016a,
  title = {Dynamic {{Network Surgery}} for {{Efficient DNNs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2016/hash/2823f4797102ce1a1aec05359cc16dd9-Abstract.html},
  urldate = {2023-10-05},
  file = {C:\Users\mospr\Zotero\storage\XPMZF4GZ\Guo et al. - 2016 - Dynamic Network Surgery for Efficient DNNs.pdf}
}


@article{guSearchBetterStudents2020a,
  title = {Search for {{Better Students}} to {{Learn Distilled Knowledge}}},
  author = {Gu, Jindong and Tresp, Volker},
  year = {2020},
  journal = {ECAI 2020: 24th European Conference on Artificial Intelligence},
  pages = {1159--1165},
  publisher = {{Ludwig-Maximilians-Universität München}},
  issn = {0922-6389},
  doi = {10.3233/FAIA200214},
  url = {https://epub.ub.uni-muenchen.de/89076/},
  urldate = {2023-10-05},
  abstract = {Knowledge Distillation, as a model compression technique, has received great attention. The knowledge of a well-performed teacher is distilled to a student with a small architecture. The architecture of the small student is often chosen to be similar to their teacher's, with fewer layers or fewer channels, or both. However, even with the same number of FLOPs or parameters, the students with different architecture can achieve different generalization ability. The configuration of a student architecture requires intensive network architecture engineering. In this work, instead of designing a good student architecture manually, we propose to search for the optimal student automatically. Based on L1-norm optimization, a subgraph from the teacher network topology graph is selected as a student, the goal of which is to minimize the KL-divergence between student's and teacher's outputs. We verify the proposal on CIFAR10 and CIFAR100 datasets. The empirical experiments show that the learned student architecture achieves better performance than ones specified manually. We also visualize and understand the architecture of the found student.},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\MKDD8FFP\89076.html}
}


@inproceedings{hanLearningBothWeights2015,
  title = {Learning Both {{Weights}} and {{Connections}} for {{Efficient Neural Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9×, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13×, from 138 million to 10.3 million, again with no loss of accuracy.},
  file = {C:\Users\mospr\Zotero\storage\FARYQE5Y\Han et al. - 2015 - Learning both Weights and Connections for Efficien.pdf}
}

@inproceedings{hansonComparingBiasesMinimal1988,
  title = {Comparing {{Biases}} for {{Minimal Network Construction}} with {{Back-Propagation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hanson, Stephen and Pratt, Lorien},
  year = {1988},
  volume = {1},
  publisher = {{Morgan-Kaufmann}},
  url = {https://proceedings.neurips.cc/paper/1988/hash/1c9ac0159c94d8d0cbedc973445af2da-Abstract.html},
  urldate = {2023-09-07},
  abstract = {learning},
  file = {C:\Users\mospr\Zotero\storage\YG6E9PMQ\Hanson and Pratt - 1988 - Comparing Biases for Minimal Network Construction .pdf}
}

@article{hassibiSecondOrderDerivatives1992,
	title={Second order derivatives for network pruning: Optimal brain surgeon},
	author={Hassibi, Babak and Stork, David},
	journal={Advances in neural information processing systems},
	volume={5},
	year={1992}
}



@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\J6E2PNXJ\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{heFilterPruningGeometric2019,
  title = {Filter {{Pruning}} via {{Geometric Median}} for {{Deep Convolutional Neural Networks Acceleration}}},
  author = {He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  year = {2019},
  pages = {4340--4349},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/He_Filter_Pruning_via_Geometric_Median_for_Deep_Convolutional_Neural_Networks_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\XJPTBL3H\He et al. - 2019 - Filter Pruning via Geometric Median for Deep Convo.pdf}
}

@online{heStructuredPruningDeep2023,
  title = {Structured {{Pruning}} for {{Deep Convolutional Neural Networks}}: {{A}} Survey},
  shorttitle = {Structured {{Pruning}} for {{Deep Convolutional Neural Networks}}},
  author = {He, Yang and Xiao, Lingao},
  year = {2023},
  eprint = {2303.00566},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.00566},
  url = {http://arxiv.org/abs/2303.00566},
  urldate = {2023-09-06},
  abstract = {The remarkable performance of deep Convolutional neural networks (CNNs) is generally attributed to their deeper and wider architectures, which can come with significant computational costs. Pruning neural networks has thus gained interest since it effectively lowers storage and computational costs. In contrast to weight pruning, which results in unstructured models, structured pruning provides the benefit of realistic acceleration by producing models that are friendly to hardware implementation. The special requirements of structured pruning have led to the discovery of numerous new challenges and the development of innovative solutions. This article surveys the recent progress towards structured pruning of deep CNNs. We summarize and compare the state-of-the-art structured pruning techniques with respect to filter ranking methods, regularization methods, dynamic execution, neural architecture search, the lottery ticket hypothesis, and the applications of pruning. While discussing structured pruning algorithms, we briefly introduce the unstructured pruning counterpart to emphasize their differences. Furthermore, we provide insights into potential research opportunities in the field of structured pruning. A curated list of neural network pruning papers can be found at https://github.com/he-y/Awesome-Pruning},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\SNRZJBIR\\He and Xiao - 2023 - Structured Pruning for Deep Convolutional Neural N.pdf;C\:\\Users\\mospr\\Zotero\\storage\\72PAXTE8\\2303.html}
}

@online{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  note = {10.48550/arXiv.1503.02531},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2023-09-05},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CBLP65WS\\Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;C\:\\Users\\mospr\\Zotero\\storage\\ESEH6WL5\\1503.html}
}

@inproceedings{horowitzComputingEnergyProblem2014,
  title = {1.1 {{Computing}}'s Energy Problem (and What We Can Do about It)},
  booktitle = {2014 {{IEEE International Solid-State Circuits Conference Digest}} of {{Technical Papers}} ({{ISSCC}})},
  author = {Horowitz, Mark},
  year = {2014},
  pages = {10--14},
  issn = {2376-8606},
  doi = {10.1109/ISSCC.2014.6757323},
  abstract = {Our challenge is clear: The drive for performance and the end of voltage scaling have made power, and not the number of transistors, the principal factor limiting further improvements in computing performance. Continuing to scale compute performance will require the creation and effective use of new specialized compute engines, and will require the participation of application experts to be successful. If we play our cards right, and develop the tools that allow our customers to become part of the design process, we will create a new wave of innovative and efficient computing devices.},
  eventtitle = {2014 {{IEEE International Solid-State Circuits Conference Digest}} of {{Technical Papers}} ({{ISSCC}})},
  keywords = {CMOS integrated circuits,CMOS technology,Energy efficiency,Hardware,Logic gates,Transistors,Voltage control},
  file = {C:\Users\mospr\Zotero\storage\336LH4P2\6757323.html}
}

@online{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  eprint = {1704.04861},
  eprinttype = {arxiv},
  eprintclass = {cs},
  note = {10.48550/arXiv.1704.04861},
  url = {http://arxiv.org/abs/1704.04861},
  urldate = {2023-09-08},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\YN2TI89Q\\Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;C\:\\Users\\mospr\\Zotero\\storage\\TCQ5I6IN\\1704.html}
}

@inproceedings{huangCoDeNetEfficientDeployment2021,
  title = {{{CoDeNet}}: {{Efficient Deployment}} of {{Input-Adaptive Object Detection}} on {{Embedded FPGAs}}},
  shorttitle = {{{CoDeNet}}},
  booktitle = {The 2021 {{ACM}}/{{SIGDA International Symposium}} on {{Field-Programmable Gate Arrays}}},
  author = {Huang, Qijing and Wang, Dequan and Dong, Zhen and Gao, Yizhao and Cai, Yaohui and Li, Tian and Wu, Bichen and Keutzer, Kurt and Wawrzynek, John},
  year = {2021},
  series = {{{FPGA}} '21},
  pages = {206--216},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3431920.3439295},
  url = {https://dl.acm.org/doi/10.1145/3431920.3439295},
  urldate = {2023-09-06},
  abstract = {Deploying deep learning models on embedded systems for computer vision tasks has been challenging due to limited compute resources and strict energy budgets. The majority of existing work focuses on accelerating image classification, while other fundamental vision problems, such as object detection, have not been adequately addressed. Compared with image classification, detection problems are more sensitive to the spatial variance of objects, and therefore, require specialized convolutions to aggregate spatial information. To address this need, recent work introduces dynamic deformable convolution to augment regular convolutions. Regular convolutions process a fixed grid of pixels across all the spatial locations in an image, while dynamic deformable convolution may access arbitrary pixels in the image with the access pattern being input-dependent and varying with spatial location. These properties lead to inefficient memory accesses of inputs with existing hardware. In this work, we harness the flexibility of FPGAs to develop a novel object detection pipeline with deformable convolutions. We show the speed-accuracy tradeoffs for a set of algorithm modifications including irregular-access versus limited-range and fixed-shape on a flexible hardware accelerator. We evaluate these algorithmic changes with corresponding hardware optimizations and show a 1.36x and 9.76x speedup respectively for the full and depthwise deformable convolution on hardware with minor accuracy loss. We then co-design a network called CoDeNet with the modified deformable convolution for object detection and quantize the network to 4-bit weights and 8-bit activations. With our high-efficiency implementation, our solution reaches 26.9 frames per second with a tiny model size of 0.76 MB while achieving 61.7 AP50 on the standard object detection dataset, Pascal VOC. With our higher-accuracy implementation, our model gets to 67.1 AP50 on Pascal VOC with only 2.9 MB of parameters--20.9x smaller but 10\% more accurate than Tiny-YOLO.},
  isbn = {978-1-4503-8218-2},
  keywords = {algorithm-hardware codesign,hardware accelerator,object detection},
  file = {C:\Users\mospr\Zotero\storage\U3UEH666\Huang et al. - 2021 - CoDeNet Efficient Deployment of Input-Adaptive Ob.pdf}
}

@inproceedings{hubaraAccuratePostTraining2021,
  title = {Accurate {{Post Training Quantization With Small Calibration Sets}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  year = {2021},
  pages = {4466--4475},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/hubara21a.html},
  urldate = {2023-09-06},
  abstract = {Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations’ dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer or block separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations’ dynamic ranges. We suggest two flavors for our method, parallel and sequential aim for a fixed and flexible bit-width allocation. For the latter, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\% accuracy degradation — with 4-bit weights and activations in all layers, but first and last. The suggested methods are two orders of magnitude faster than the traditional Quantize Aware Training approach used for lower than 8-bit quantization. We open-sourced our code \textbackslash textit\{https://github.com/papers-submission/CalibTIP\}.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\BP75KNNV\\Hubara et al. - 2021 - Accurate Post Training Quantization With Small Cal.pdf;C\:\\Users\\mospr\\Zotero\\storage\\PHT47DSW\\Hubara et al. - 2021 - Accurate Post Training Quantization With Small Cal.pdf}
}



@article{hubaraImprovingPostTraining2020,
  title = {Improving {{Post Training Neural Quantization}}: {{Layer-wise Calibration}} and {{Integer Programming}}},
  shorttitle = {Improving {{Post Training Neural Quantization}}},
  author = {Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  year = {2020},
  url = {https://openreview.net/forum?id=Mf4ZSXMZP7},
  urldate = {2023-10-05},
  abstract = {Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\textbackslash\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\NK5D2YDB\Hubara et al. - 2020 - Improving Post Training Neural Quantization Layer.pdf}
}



@inproceedings{kimFeatureFusionOnline2020,
  title = {Feature {{Fusion}} for {{Online Mutual Knowledge Distillation}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Kim, Jangho and Hyun, Minsung and Chung, Inseop and Kwak, Nojun},
  year = {2021},
  pages = {4619--4625},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9412615},
  url = {https://ieeexplore.ieee.org/abstract/document/9412615},
  urldate = {2023-10-05},
  abstract = {We propose a learning framework named Feature Fusion Learning (FFL) that efficiently trains a powerful classifier through a fusion module which combines the feature maps generated from parallel neural networks and generates meaningful feature maps. Specifically, we train a number of parallel neural networks as sub-networks, then we combine the feature maps from each sub-network using a fusion module to create a more meaningful feature map. The fused feature map is passed into the fused classifier for overall classification. Unlike existing feature fusion methods, in our framework, an ensemble of sub-network classifiers transfers its knowledge to the fused classifier and then the fused classifier delivers its knowledge back to each subnetwork, mutually teaching one another in an online-knowledge distillation manner. This mutually teaching system not only improves the performance of the fused classifier but also obtains performance gain in each sub-network. Moreover, our model is more beneficial than other alternative methods because different types of network can be used for each sub-network. We have performed a variety of experiments on multiple datasets such as CIFAR-10, CIFAR-100 and ImageNet and proved that our method is more effective than other alternative methods in terms of performances of both sub-networks and the fused classifier, and the aspect of generating meaningful feature maps. The code is available at this link1.},
  eventtitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  file = {C:\Users\mospr\Zotero\storage\2RH7IACQ\Kim et al. - 2021 - Feature Fusion for Online Mutual Knowledge Distill.pdf}
}



@inproceedings{KnowledgeDistillationOptimization,
	author={Shin, Sungho and Boo, Yoonho and Sung, Wonyong},
	booktitle={2020 IEEE Workshop on Signal Processing Systems (SiPS)}, 
	title={Knowledge Distillation for Optimization of Quantized Deep Neural Networks}, 
	year={2020},
	volume={},
	number={},
	pages={1-6},
	doi={10.1109/SiPS50750.2020.9195219}}

@online{krishnamoorthiQuantizingDeepConvolutional2018,
  title = {Quantizing Deep Convolutional Networks for Efficient Inference: {{A}} Whitepaper},
  shorttitle = {Quantizing Deep Convolutional Networks for Efficient Inference},
  author = {Krishnamoorthi, Raghuraman},
  year = {2018},
  eprint = {1806.08342},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1806.08342},
  url = {http://arxiv.org/abs/1806.08342},
  urldate = {2023-09-06},
  abstract = {We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2\% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. Quantization-aware training can provide further improvements, reducing the gap to floating point to 1\% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2\% to 10\%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\8NBWYC5L\\Krishnamoorthi - 2018 - Quantizing deep convolutional networks for efficie.pdf;C\:\\Users\\mospr\\Zotero\\storage\\8AIKKKPR\\1806.html}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  series = {{{NIPS}}'12},
  pages = {1097--1105},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.}
}

@article{krizhevskyLearningMultipleLayers2012,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  year = {2012},
  booktitle = {University of Toronto},
  shortjournal = {University of Toronto},
  abstract = {April 8, 2009Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it di cult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signi cantly}
}

@inproceedings{lazarevichPostTrainingDeepNeural2021,
  title = {Post-{{Training Deep Neural Network Pruning}} via {{Layer-Wise Calibration}}},
  author = {Lazarevich, Ivan and Kozlov, Alexander and Malinin, Nikita},
  year = {2021},
  pages = {798--805},
  url = {https://openaccess.thecvf.com/content/ICCV2021W/LPCV/html/Lazarevich_Post-Training_Deep_Neural_Network_Pruning_via_Layer-Wise_Calibration_ICCVW_2021_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\7XZAZN9V\Lazarevich et al. - 2021 - Post-Training Deep Neural Network Pruning via Laye.pdf}
}


@inproceedings{lebedevFastConvNetsUsing2015,
  title = {Fast {{ConvNets Using Group-Wise Brain Damage}}},
  author = {Lebedev, Vadim and Lempitsky, Victor},
  year = {2016},
  pages = {2554--2564},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Lebedev_Fast_ConvNets_Using_CVPR_2016_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\ITMN6YNZ\Lebedev and Lempitsky - 2016 - Fast ConvNets Using Group-Wise Brain Damage.pdf}
}

@incollection{lecunConvolutionalNetworksImages1995,
  title = {Convolutional Networks for Images, Speech, and Time-Series},
  booktitle = {The Handbook of Brain Theory and Neural Networks},
  author = {Lecun, Yann and Bengio, Yoshua},
  editor = {Arbib, M.A.},
  year = {1995},
  publisher = {{MIT Press}}
}

@inproceedings{lecunOptimalBrainDamage1989,
  title = {Optimal {{Brain Damage}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Denker, John and Solla, Sara},
  year = {1989},
  volume = {2},
  publisher = {{Morgan-Kaufmann}},
  url = {https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html},
  urldate = {2023-09-07},
  abstract = {We  have used  information-theoretic ideas  to derive  a class of prac(cid:173) tical  and  nearly  optimal schemes  for  adapting the size  of a  neural  network.  By  removing  unimportant  weights  from  a  network,  sev(cid:173) eral  improvements  can  be  expected:  better  generalization,  fewer  training examples required,  and improved speed  of learning and/or  classification.  The  basic  idea  is  to  use  second-derivative  informa(cid:173) tion to make a  tradeoff between  network  complexity  and  training  set error.  Experiments confirm  the usefulness  of the methods on a  real-world  application.},
  file = {C:\Users\mospr\Zotero\storage\SCAKFPGH\LeCun et al. - 1989 - Optimal Brain Damage.pdf}
}





@inproceedings{leeGraphbasedKnowledgeDistillation2019,
  title = {Graph-Based Knowledge Distillation by Multi-Head Attention Network},
  author = {Lee, Seunghyun and Song, Byung Cheol},
  year = {2020},
  url = {https://inha.elsevierpure.com/en/publications/graph-based-knowledge-distillation-by-multi-head-attention-networ},
  urldate = {2023-10-05},
  booktitle = {30th {{British Machine Vision Conference}}, {{BMVC}} 2019},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\KEE87X8Q\graph-based-knowledge-distillation-by-multi-head-attention-networ.html}
}


@inproceedings{leeSelfsupervisedKnowledgeDistillation2018,
  title = {Self-Supervised {{Knowledge Distillation Using Singular Value Decomposition}}},
  author = {Lee, Seung Hyun and Kim, Dae Ha and Song, Byung Cheol},
  year = {2018},
  pages = {335--350},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/SEUNG_HYUN_LEE_Self-supervised_Knowledge_Distillation_ECCV_2018_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  file = {C:\Users\mospr\Zotero\storage\IW6BHU69\Lee et al. - 2018 - Self-supervised Knowledge Distillation Using Singu.pdf}
}

@article{liangPruningQuantizationDeep2021,
  title = {Pruning and Quantization for Deep Neural Network Acceleration: {{A}} Survey},
  shorttitle = {Pruning and Quantization for Deep Neural Network Acceleration},
  author = {Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
  year = {2021},
  journal = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {461},
  pages = {370--403},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.07.045},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221010894},
  urldate = {2023-09-06},
  abstract = {Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.},
  keywords = {Convolutional neural network,Low-bit mathematics,Neural network acceleration,Neural network pruning,Neural network quantization},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\DIPTGN3G\\Liang et al. - 2021 - Pruning and quantization for deep neural network a.pdf;C\:\\Users\\mospr\\Zotero\\storage\\RKNS7IGL\\S0925231221010894.html}
}
@inproceedings{shufflenet,
	title={Shufflenet: An extremely efficient convolutional neural network for mobile devices},
	author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={6848--6856},
	year={2018}
}
@incollection{pytorch,
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	booktitle = {Advances in Neural Information Processing Systems 32},
	pages = {8024--8035},
	year = {2019},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{han2020ghostnet,
	title={Ghostnet: More features from cheap operations},
	author={Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={1580--1589},
	year={2020}
}
@inproceedings{mobilenetv3,
	title={Searching for mobilenetv3},
	author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
	booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
	pages={1314--1324},
	year={2019}
}
@inproceedings{liCurriculumTemperatureKnowledge2022,
	title={Curriculum temperature for knowledge distillation},
	author={Li, Zheng and Li, Xiang and Yang, Lingfeng and Zhao, Borui and Song, Renjie and Luo, Lei and Li, Jun and Yang, Jian},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={37},
	number={2},
	pages={1504--1512},
	year={2023}
}


@inproceedings{liFewSampleKnowledge2020,
  title = {Few {{Sample Knowledge Distillation}} for {{Efficient Network Compression}}},
  author = {Li, Tianhong and Li, Jianguo and Liu, Zhuang and Zhang, Changshui},
  year = {2020},
  pages = {14639--14647},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\Z9EGS73Y\Li et al. - 2020 - Few Sample Knowledge Distillation for Efficient Ne.pdf}
}

@article{liModelCompressionDeep2023,
  title = {Model {{Compression}} for {{Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Model {{Compression}} for {{Deep Neural Networks}}},
  author = {Li, Zhuo and Li, Hengyi and Meng, Lin},
  year = {2023},
  journal = {Computers},
  volume = {12},
  number = {3},
  pages = {60},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2073-431X},
  doi = {10.3390/computers12030060},
  url = {https://www.mdpi.com/2073-431X/12/3/60},
  urldate = {2023-09-06},
  abstract = {Currently, with the rapid development of deep learning, deep neural networks (DNNs) have been widely applied in various computer vision tasks. However, in the pursuit of performance, advanced DNN models have become more complex, which has led to a large memory footprint and high computation demands. As a result, the models are difficult to apply in real time. To address these issues, model compression has become a focus of research. Furthermore, model compression techniques play an important role in deploying models on edge devices. This study analyzed various model compression methods to assist researchers in reducing device storage space, speeding up model inference, reducing model complexity and training costs, and improving model deployment. Hence, this paper summarized the state-of-the-art techniques for model compression, including model pruning, parameter quantization, low-rank decomposition, knowledge distillation, and lightweight model design. In addition, this paper discusses research challenges and directions for future work.},
  issue = {3},
  langid = {english},
  keywords = {deep neural networks,knowledge distillation,lightweight model design,low-rank decomposition,model compression,model pruning,parameter quantization},
  file = {C:\Users\mospr\Zotero\storage\V6GJ5NMC\Li et al. - 2023 - Model Compression for Deep Neural Networks A Surv.pdf}
}


@inproceedings{linRotatedBinaryNeural2020,
  title = {Rotated {{Binary Neural Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lin, Mingbao and Ji, Rongrong and Xu, Zihan and Zhang, Baochang and Wang, Yan and Wu, Yongjian and Huang, Feiyue and Lin, Chia-Wen},
  year = {2020},
  volume = {33},
  pages = {7474--7485},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/53c5b2affa12eed84dfec9bfd83550b1-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Binary Neural Network (BNN) shows its predominance in reducing the complexity of deep neural networks. However, it suffers severe performance degradation. One of the major impediments is the large quantization error between the full-precision weight vector and its binary vector. Previous works focus on compensating for the norm gap while leaving the angular bias hardly touched. In this paper, for the first time, we explore the influence of angular bias on the quantization error and then introduce a Rotated Binary Neural Network (RBNN), which considers the angle alignment between the full-precision weight vector and its binarized version. At the beginning of each training epoch, we propose to rotate the full-precision weight vector to its binary vector to reduce the angular bias. To avoid the high complexity of learning a large rotation matrix, we further introduce a bi-rotation formulation that learns two smaller rotation matrices. In the training stage, we devise an adjustable rotated weight vector for binarization to escape the potential local optimum. Our rotation leads to around 50\% weight flips which maximize the information gain. Finally, we propose a training-aware approximation of the sign function for the gradient backward. Experiments on CIFAR-10 and ImageNet demonstrate the superiorities of RBNN over many state-of-the-arts. Our source code, experimental settings, training logs and binary models are available at https://github.com/lmbxmu/RBNN.},
  file = {C:\Users\mospr\Zotero\storage\8KJHJEU7\Lin et al. - 2020 - Rotated Binary Neural Network.pdf}
}

@inproceedings{liPruningFiltersEfficient2016,
  title = {Pruning {{Filters}} for {{Efficient ConvNets}}},
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  year = {2016},
  url = {https://openreview.net/forum?id=rJqFGTslg},
  urldate = {2023-10-05},
  abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34\% and ResNet-110 by up to 38\% on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\H7PAG8YD\Li et al. - 2016 - Pruning Filters for Efficient ConvNets.pdf}
}

@inproceedings{liuEfficientNLPStandard2022,
  title = {Towards {{Efficient NLP}}: {{A Standard Evaluation}} and {{A Strong Baseline}}},
  shorttitle = {Towards {{Efficient NLP}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Liu, Xiangyang and Sun, Tianxiang and He, Junliang and Wu, Jiawen and Wu, Lingling and Zhang, Xinyu and Jiang, Hao and Cao, Zhao and Huang, Xuanjing and Qiu, Xipeng},
  year = {2022},
  pages = {3288--3303},
  publisher = {{Association for Computational Linguistics}},
  location = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.240},
  url = {https://aclanthology.org/2022.naacl-main.240},
  urldate = {2023-10-05},
  abstract = {Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depicting the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.},
  eventtitle = {{{NAACL-HLT}} 2022},
  file = {C:\Users\mospr\Zotero\storage\N5XQYBTQ\Liu et al. - 2022 - Towards Efficient NLP A Standard Evaluation and A.pdf}
}

@online{liuPruningAlgorithmsAccelerate2020,
  title = {Pruning {{Algorithms}} to {{Accelerate Convolutional Neural Networks}} for {{Edge Applications}}: {{A Survey}}},
  shorttitle = {Pruning {{Algorithms}} to {{Accelerate Convolutional Neural Networks}} for {{Edge Applications}}},
  author = {Liu, Jiayi and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
  year = {2020},
  eprint = {2005.04275},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2005.04275},
  url = {http://arxiv.org/abs/2005.04275},
  urldate = {2023-09-06},
  abstract = {With the general trend of increasing Convolutional Neural Network (CNN) model sizes, model compression and acceleration techniques have become critical for the deployment of these models on edge devices. In this paper, we provide a comprehensive survey on Pruning, a major compression strategy that removes non-critical or redundant neurons from a CNN model. The survey covers the overarching motivation for pruning, different strategies and criteria, their advantages and drawbacks, along with a compilation of major pruning techniques. We conclude the survey with a discussion on alternatives to pruning and current challenges for the model compression community.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\XLAKTB9G\\Liu et al. - 2020 - Pruning Algorithms to Accelerate Convolutional Neu.pdf;C\:\\Users\\mospr\\Zotero\\storage\\F25SWL84\\2005.html}
}

@inproceedings{liuRethinkingValueNetwork2019,
  title = {Rethinking the {{Value}} of {{Network Pruning}}},
  author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  year = {2018},
  url = {https://openreview.net/forum?id=rJlnB3C5Ym},
  urldate = {2023-10-05},
  abstract = {Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned ``important'' weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited ``important'' weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the "Lottery Ticket Hypothesis" (Frankle \& Carbin 2019), and find that with optimal learning rate, the "winning ticket" initialization as used in Frankle \& Carbin (2019) does not bring improvement over random initialization.},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\E78EVTTY\Liu et al. - 2018 - Rethinking the Value of Network Pruning.pdf}
}


@online{maLLMPrunerStructuralPruning2023,
  title = {{{LLM-Pruner}}: {{On}} the {{Structural Pruning}} of {{Large Language Models}}},
  shorttitle = {{{LLM-Pruner}}},
  author = {Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  year = {2023},
  eprint = {2305.11627},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.11627},
  url = {http://arxiv.org/abs/2305.11627},
  urldate = {2023-09-07},
  abstract = {Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\FKK9CW4X\\Ma et al. - 2023 - LLM-Pruner On the Structural Pruning of Large Lan.pdf;C\:\\Users\\mospr\\Zotero\\storage\\99SIVQ5N\\2305.html}
}


@inproceedings{mckinstryDiscoveringLowPrecisionNetworks2019,
  title = {Discovering {{Low-Precision Networks Close}} to {{Full-Precision Networks}} for {{Efficient Inference}}},
  booktitle = {2019 {{Fifth Workshop}} on {{Energy Efficient Machine Learning}} and {{Cognitive Computing}} - {{NeurIPS Edition}} ({{EMC2-NIPS}})},
  author = {McKinstry, Jeffrey L. and Esser, Steven K. and Appuswamy, Rathinakumar and Bablani, Deepika and Arthur, John V. and Yildiz, Izzet B. and Modha, Dharmendra S.},
  year = {2019},
  pages = {6--9},
  doi = {10.1109/EMC2-NIPS53020.2019.00009},
  url = {https://ieeexplore.ieee.org/abstract/document/9463517},
  urldate = {2023-10-05},
  abstract = {To realize the promise of ubiquitous embedded deep network inference, it is essential to seek limits of energy and area efficiency. Low-precision networks offer promise as energy and area scale down quadratically with precision. We demonstrate 8- and 4-bit networks that meet or exceed the accuracy of their full-precision versions on the ImageNet classification benchmark. We hypothesize that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this. The number of iterations required by SGD to achieve a given training error is related to the square of (a) the distance of the initial solution from the final and (b) the maximum variance of the gradient estimates. Accordingly, we reduce solution distance by starting with pretrained fp32 baseline networks, and combat noise introduced by quantizing weights and activations during training by training longer and reducing learning rates. Sensitivity analysis indicates that these techniques, coupled with activation function range calibration, are sufficient to discover low-precision networks close to fp32 precision baseline networks. Our results provide evidence that 4-bits suffice for classification.},
  eventtitle = {2019 {{Fifth Workshop}} on {{Energy Efficient Machine Learning}} and {{Cognitive Computing}} - {{NeurIPS Edition}} ({{EMC2-NIPS}})},
  file = {C:\Users\mospr\Zotero\storage\SJGWA5B8\McKinstry et al. - 2019 - Discovering Low-Precision Networks Close to Full-P.pdf}
}

@inproceedings{meng2019conditional,
	title={Conditional teacher-student learning},
	author={Meng, Zhong and Li, Jinyu and Zhao, Yong and Gong, Yifan},
	booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={6445--6449},
	year={2019},
	organization={IEEE}
}

@inproceedings{mengConditionalTeacherStudentLearning2019,
  title = {Conditional {{Teacher-Student Learning}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Meng, Zhong and Li, Jinyu and Zhao, Yong and Gong, Yifan},
  year = {2019},
  eprint = {1904.12399},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  pages = {6445--6449},
  doi = {10.1109/ICASSP.2019.8683438},
  url = {http://arxiv.org/abs/1904.12399},
  urldate = {2023-10-01},
  abstract = {The teacher-student (T/S) learning has been shown to be effective for a variety of problems such as domain adaptation and model compression. One shortcoming of the T/S learning is that a teacher model, not always perfect, sporadically produces wrong guidance in form of posterior probabilities that misleads the student model towards a suboptimal performance. To overcome this problem, we propose a conditional T/S learning scheme, in which a "smart" student model selectively chooses to learn from either the teacher model or the ground truth labels conditioned on whether the teacher can correctly predict the ground truth. Unlike a naive linear combination of the two knowledge sources, the conditional learning is exclusively engaged with the teacher model when the teacher model's prediction is correct, and otherwise backs off to the ground truth. Thus, the student model is able to learn effectively from the teacher and even potentially surpass the teacher. We examine the proposed learning scheme on two tasks: domain adaptation on CHiME-3 dataset and speaker adaptation on Microsoft short message dictation dataset. The proposed method achieves 9.8\% and 12.8\% relative word error rate reductions, respectively, over T/S learning for environment adaptation and speaker-independent model for speaker adaptation.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\F3PISQZ5\\Meng et al. - 2019 - Conditional Teacher-Student Learning.pdf;C\:\\Users\\mospr\\Zotero\\storage\\Y7Q4UZCP\\1904.html}
}




@inproceedings{migaczNvidia8bitInference,
  title = {Nvidia 8-Bit Inference with Tensorrt.},
  author = {Migacz, Szymon},
  eventtitle = {{{GPU Technology Conference}}, 2017},
  year = {2017}
}


@article{mirzadehImprovedKnowledgeDistillation2019,
  title = {Improved {{Knowledge Distillation}} via {{Teacher Assistant}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  year = {2020},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {5191--5198},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i04.5963},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/5963},
  urldate = {2023-10-05},
  abstract = {Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.},
  issue = {04},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\YRJULJT5\Mirzadeh et al. - 2020 - Improved Knowledge Distillation via Teacher Assist.pdf}
}

@online{mishraApprenticeUsingKnowledge2017,
  title = {Apprentice: {{Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy}}},
  shorttitle = {Apprentice},
  author = {Mishra, Asit and Marr, Debbie},
  year = {2017},
  eprint = {1711.05852},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.05852},
  url = {http://arxiv.org/abs/1711.05852},
  urldate = {2023-09-08},
  abstract = {Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems - the models (often deep networks or wide networks or both) are compute and memory intensive. Low-precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the combination of these two techniques and show that the performance of low-precision networks can be significantly improved by using knowledge distillation techniques. Our approach, Apprentice, achieves state-of-the-art accuracies using ternary precision and 4-bit precision for variants of ResNet architecture on ImageNet dataset. We present three schemes using which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CKXV7YSH\\Mishra and Marr - 2017 - Apprentice Using Knowledge Distillation Technique.pdf;C\:\\Users\\mospr\\Zotero\\storage\\6B4AI5BB\\1711.html}
}

@online{miyashitaConvolutionalNeuralNetworks2016,
  title = {Convolutional {{Neural Networks}} Using {{Logarithmic Data Representation}}},
  author = {Miyashita, Daisuke and Lee, Edward H. and Murmann, Boris},
  year = {2016},
  eprint = {1603.01025},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1603.01025},
  url = {http://arxiv.org/abs/1603.01025},
  urldate = {2023-09-06},
  abstract = {Recent advances in convolutional neural networks have considered model complexity and hardware efficiency to enable deployment onto embedded systems and mobile devices. For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance. However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance. In this paper we propose a new data representation that enables state-of-the-art networks to be encoded to 3 bits with negligible loss in classification performance. To perform this, we take advantage of the fact that the weights and activations in a trained network naturally have non-uniform distributions. Using non-uniform, base-2 logarithmic representation to encode weights, communicate activations, and perform dot-products enables networks to 1) achieve higher classification accuracies than fixed-point at the same resolution and 2) eliminate bulky digital multipliers. Finally, we propose an end-to-end training procedure that uses log representation at 5-bits, which achieves higher final test accuracy than linear at 5-bits.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\TXBP8UTP\\Miyashita et al. - 2016 - Convolutional Neural Networks using Logarithmic Da.pdf;C\:\\Users\\mospr\\Zotero\\storage\\Z2M9357B\\1603.html}
}


@inproceedings{mobahiSelfDistillationAmplifiesRegularization2020,
  title = {Self-{{Distillation Amplifies Regularization}} in {{Hilbert Space}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter},
  year = {2020},
  volume = {33},
  pages = {3351--3361},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/2288f691b58edecadcc9a8691762b4fd-Abstract.html},
  urldate = {2023-10-05},
  abstract = {Knowledge distillation introduced in the deep learning context is a method to transfer knowledge from one architecture to another. In particular, when the architectures are identical, this is called self-distillation. The idea is to feed in predictions of the trained model as new target values for retraining (and iterate this loop possibly a few times). It has been empirically observed that the self-distilled model often achieves higher accuracy on held out data. Why this happens, however, has been a mystery: the self-distillation dynamics does not receive any new information about the task and solely evolves by looping over training. To the best of our knowledge, there is no rigorous understanding of why this happens. This work provides the first theoretical analysis of self-distillation. We focus on fitting a nonlinear function to training data, where the model space is Hilbert space and fitting is subject to L2 regularization in this function space. We show that self-distillation iterations modify regularization by progressively limiting the number of basis functions that can be used to represent the solution. This implies (as we also verify empirically) that while a few rounds of self-distillation may reduce over-fitting, further rounds may lead to under-fitting and thus worse performance.},
  file = {C:\Users\mospr\Zotero\storage\KS8NW3DN\Mobahi et al. - 2020 - Self-Distillation Amplifies Regularization in Hilb.pdf}
}


@inproceedings{nagelAdaptiveRoundingPostTraining2020,
  title = {Up or {{Down}}? {{Adaptive Rounding}} for {{Post-Training Quantization}}},
  shorttitle = {Up or {{Down}}?},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Nagel, Markus and Amjad, Rana Ali and Baalen, Mart Van and Louizos, Christos and Blankevoort, Tijmen},
  year = {2020},
  pages = {7197--7206},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/nagel20a.html},
  urldate = {2023-10-05},
  abstract = {When quantizing neural networks, assigning each floating-point weight to its nearest fixed-point value is the predominant approach. We find that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. We start by theoretically analyzing the rounding problem for a pre-trained neural network. By approximating the task loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a layer-wise local loss and propose to optimize this loss with a soft relaxation. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1\%.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\CGK3ZQXP\\Nagel et al. - 2020 - Up or Down Adaptive Rounding for Post-Training Qu.pdf;C\:\\Users\\mospr\\Zotero\\storage\\SUY3W7R4\\Nagel et al. - 2020 - Up or Down Adaptive Rounding for Post-Training Qu.pdf}
}

@online{parkLUTGEMMQuantizedMatrix2023,
  title = {{{LUT-GEMM}}: {{Quantized Matrix Multiplication}} Based on {{LUTs}} for {{Efficient Inference}} in {{Large-Scale Generative Language Models}}},
  shorttitle = {{{LUT-GEMM}}},
  author = {Park, Gunho and Park, Baeseong and Kim, Minsub and Lee, Sungjae and Kim, Jeonghoon and Kwon, Beomseok and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  year = {2023},
  eprint = {2206.09557},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.09557},
  url = {http://arxiv.org/abs/2206.09557},
  urldate = {2023-09-06},
  abstract = {The recent advancements in self-supervised learning, combined with the Transformer architecture, have enabled natural language processing (NLP) to achieve remarkably low perplexity. However, powerful NLP models necessitate increasing model size, leading to substantial computational and memory requirements. In this paper, we introduce an efficient inference framework tailored for large-scale generative language models. To reduce the model size, we employ a weight-only quantization strategy while preserving full precision for activations. As a result, we attain sub-4-bit quantization for each weight through non-uniform or uniform quantization techniques. Our proposed kernel, called LUT-GEMM, then accelerates quantized matrix multiplications, offering a flexible balance between compression ratio and accuracy. Unlike earlier matrix multiplication kernels that accommodated weight-only quantization, LUT-GEMM efficiently eliminates the resource-demanding dequantization process for both uniform and non-uniform quantization methods. By reducing the latency of individual GPUs and the overall inference process for large-scale language models, LUT-GEMM provides significant performance improvements in inference. The impact of LUT-GEMM is facilitated by implementing high compression ratios through low-bit quantization and efficient LUT-based operations, which decreases the number of required GPUs. For the OPT-175B model with 3-bit quantization, we show that LUT-GEMM accelerates the latency for generating each token by 2.1x compared to OPTQ, which requires costly dequantization. Consequently, LUT-GEMM enables inference of the OPT-175B model on a single GPU without noticeable degradation in accuracy or performance, while the non-quantized OPT-175B model requires a minimum of 8 GPUs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\J74KQ39J\\Park et al. - 2023 - LUT-GEMM Quantized Matrix Multiplication based on.pdf;C\:\\Users\\mospr\\Zotero\\storage\\AK7C8Z9E\\2206.html}
}



@inproceedings{parkRelationalKnowledgeDistillation2019,
  title = {Relational {{Knowledge Distillation}}},
  author = {Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  year = {2019},
  pages = {3967--3976},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\LX6GGTMJ\Park et al. - 2019 - Relational Knowledge Distillation.pdf}
}


@inproceedings{passalisHeterogeneousKnowledgeDistillation2020,
  title = {Heterogeneous {{Knowledge Distillation Using Information Flow Modeling}}},
  author = {Passalis, Nikolaos and Tzelepi, Maria and Tefas, Anastasios},
  year = {2020},
  pages = {2339--2348},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Passalis_Heterogeneous_Knowledge_Distillation_Using_Information_Flow_Modeling_CVPR_2020_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\6QM4UB7E\Passalis et al. - 2020 - Heterogeneous Knowledge Distillation Using Informa.pdf}
}

@inproceedings{passalisLearningDeepRepresentations2019,
  title = {Learning {{Deep Representations}} with {{Probabilistic Knowledge Transfer}}},
  author = {Passalis, Nikolaos and Tefas, Anastasios},
  year = {2018},
  pages = {268--284},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  file = {C:\Users\mospr\Zotero\storage\F9KRDKTD\Passalis and Tefas - 2018 - Learning Deep Representations with Probabilistic K.pdf}
}


@article{passbanALPKDAttentionBasedLayer2020,
  title = {{{ALP-KD}}: {{Attention-Based Layer Projection}} for {{Knowledge Distillation}}},
  shorttitle = {{{ALP-KD}}},
  author = {Passban, Peyman and Wu, Yimeng and Rezagholizadeh, Mehdi and Liu, Qun},
  year = {2021},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {15},
  pages = {13657--13665},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i15.17610},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17610},
  urldate = {2023-10-05},
  abstract = {Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher’s supervision for internal components. Motivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer’s significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques.},
  issue = {15},
  langid = {english},
  keywords = {Language Models},
  file = {C:\Users\mospr\Zotero\storage\PEC522S4\Passban et al. - 2021 - ALP-KD Attention-Based Layer Projection for Knowl.pdf}
}

@inproceedings{phuongDistillationBasedTrainingMultiExit2019,
  title = {Distillation-{{Based Training}} for {{Multi-Exit Architectures}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Phuong, Mary and Lampert, Christoph},
  year = {2019},
  pages = {1355--1364},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2019.00144},
  abstract = {Multi-exit architectures, in which a stack of processing layers is interleaved with early output layers, allow the processing of a test example to stop early and thus save computation time and/or energy. In this work, we propose a new training procedure for multi-exit architectures based on the principle of knowledge distillation. The method encourages early exits to mimic later, more accurate exits, by matching their probability outputs. Experiments on CIFAR100 and ImageNet show that distillation-based training significantly improves the accuracy of early exits while maintaining state-of-the-art accuracy for late ones. The method is particularly beneficial when training data is limited and also allows a straight-forward extension to semi-supervised learning, i.e. make use also of unlabeled data at training time. Moreover, it takes only a few lines to implement and imposes almost no computational overhead at training time, and none at all at test time.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {Computational modeling,Computer architecture,Entropy,Predictive models,Task analysis,Training,Training data},
  file = {C:\Users\mospr\Zotero\storage\GJGL2L2G\9009834.html}
}


@article{polinoModelCompressionDistillation2018,
  title = {Model Compression via Distillation and Quantization},
  author = {Polino, Antonio and Pascanu, Razvan and Alistarh, Dan-Adrian},
  year = {2018},
  journal = {6th International Conference on Learning Representations},
  url = {https://research-explorer.ista.ac.at/record/7812},
  urldate = {2023-10-05},
  abstract = {Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\QY42M76Z\Polino et al. - 2018 - Model compression via distillation and quantizatio.pdf}
}

@inproceedings{reddiMLPerfInferenceBenchmark2020,
  title = {{{MLPerf Inference Benchmark}}},
  booktitle = {2020 {{ACM}}/{{IEEE}} 47th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
  year = {2020},
  pages = {446--459},
  doi = {10.1109/ISCA45697.2020.00045},
  abstract = {Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark’s flexibility and adaptability.},
  eventtitle = {2020 {{ACM}}/{{IEEE}} 47th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  keywords = {Benchmarking,Inference,Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\UPG9Q28G\\Reddi et al. - 2020 - MLPerf Inference Benchmark.pdf;C\:\\Users\\mospr\\Zotero\\storage\\4IM6FEIV\\9138989.html}
}

@inproceedings{reutherSurveyBenchmarkingMachine2019,
  title = {Survey and {{Benchmarking}} of {{Machine Learning Accelerators}}},
  booktitle = {2019 {{IEEE High Performance Extreme Computing Conference}} ({{HPEC}})},
  author = {Reuther, Albert and Michaleas, Peter and Jones, Michael and Gadepally, Vijay and Samsi, Siddharth and Kepner, Jeremy},
  year = {2019},
  eprint = {1908.11348},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--9},
  doi = {10.1109/HPEC.2019.8916327},
  url = {http://arxiv.org/abs/1908.11348},
  urldate = {2023-09-09},
  abstract = {Advances in multicore processors and accelerators have opened the flood gates to greater exploration and application of machine learning techniques to a variety of applications. These advances, along with breakdowns of several trends including Moore's Law, have prompted an explosion of processors and accelerators that promise even greater computational and machine learning capabilities. These processors and accelerators are coming in many forms, from CPUs and GPUs to ASICs, FPGAs, and dataflow accelerators. This paper surveys the current state of these processors and accelerators that have been publicly announced with performance and power consumption numbers. The performance and power values are plotted on a scatter graph and a number of dimensions and observations from the trends on this plot are discussed and analyzed. For instance, there are interesting trends in the plot regarding power consumption, numerical precision, and inference versus training. We then select and benchmark two commercially-available low size, weight, and power (SWaP) accelerators as these processors are the most interesting for embedded and mobile machine learning inference applications that are most applicable to the DoD and other SWaP constrained users. We determine how they actually perform with real-world images and neural network models, compare those results to the reported performance and power consumption values and evaluate them against an Intel CPU that is used in some embedded applications.},
  keywords = {B.8,C.4,Computer Science - Performance},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\J2CJI24R\\Reuther et al. - 2019 - Survey and Benchmarking of Machine Learning Accele.pdf;C\:\\Users\\mospr\\Zotero\\storage\\8EBJZP3N\\1908.html}
}


@article{rokhComprehensiveSurveyModel2023,
  title = {A {{Comprehensive Survey}} on {{Model Quantization}} for {{Deep Neural Networks}} in {{Image Classification}}},
  author = {Rokh, Babak and Azarpeyvand, Ali and Khanteymoori, Alireza},
  year = {2023},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  issn = {2157-6904},
  doi = {10.1145/3623402},
  url = {https://dl.acm.org/doi/10.1145/3623402},
  urldate = {2023-10-05},
  abstract = {Recent advancements in machine learning achieved by Deep Neural Networks (DNNs) have been significant. While demonstrating high accuracy, DNNs are associated with a huge number of parameters and computations, which leads to high memory usage and energy consumption. As a result, deploying DNNs on devices with constrained hardware resources poses significant challenges. To overcome this, various compression techniques have been widely employed to optimize DNN accelerators. A promising approach is quantization, in which the full-precision values are stored in low bit-width precision. Quantization not only reduces memory requirements but also replaces high-cost operations with low-cost ones. DNN quantization offers flexibility and efficiency in hardware design, making it a widely adopted technique in various methods. Since quantization has been extensively utilized in previous works, there is a need for an integrated report that provides an understanding, analysis, and comparison of different quantization approaches. Consequently, we present a comprehensive survey of quantization concepts and methods, with a focus on image classification. We describe clustering-based quantization methods and explore the use of a scale factor parameter for approximating full-precision values. Moreover, we thoroughly review the training of a quantized DNN, including the use of a straight-through estimator and quantized regularization. We explain the replacement of floating-point operations with low-cost bitwise operations in a quantized DNN and the sensitivity of different layers in quantization. Furthermore, we highlight the evaluation metrics for quantized methods and important benchmarks in the image classification task. We also present the accuracy of the state-of-the-art methods on CIFAR-10 and ImageNet. This paper attempts to make the readers familiar with the basic and advanced concepts of quantization, introduce important works in DNN quantization, and highlight challenges for future research in this field.},
  keywords = {Deep neural network acceleration,Discrete neural network optimization,Image classification,Model compression,Quantization},
  annotation = {Just Accepted},
  file = {C:\Users\mospr\Zotero\storage\NGP2D4K5\Rokh et al. - 2023 - A Comprehensive Survey on Model Quantization for D.pdf}
}

@article{shenQBERTHessianBased2019,
  title = {Q-{{BERT}}: {{Hessian Based Ultra Low Precision Quantization}} of {{BERT}}},
  shorttitle = {Q-{{BERT}}},
  author = {Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
  year = {2020},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {8815--8821},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i05.6409},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/6409},
  urldate = {2023-10-05},
  abstract = {Transformer based architectures have become de-facto models used for a range of Natural Language Processing tasks. In particular, the BERT based models achieved significant accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However, BERT based models have a prohibitive memory footprint and latency. As a result, deploying BERT based models in resource constrained environments has become a challenging task. In this work, we perform an extensive analysis of fine-tuned BERT models using second order Hessian information, and we use our results to propose a novel method for quantizing BERT models to ultra low precision. In particular, we propose a new group-wise quantization scheme, and we use Hessian-based mix-precision method to compress the model further. We extensively test our proposed method on BERT downstream tasks of SST-2, MNLI, CoNLL-03, and SQuAD. We can achieve comparable performance to baseline with at most 2.3\% performance degradation, even with ultra-low precision quantization down to 2 bits, corresponding up to 13× compression of the model parameters, and up to 4× compression of the embedding table as well as activations. Among all tasks, we observed the highest performance loss for BERT fine-tuned on SQuAD. By probing into the Hessian based analysis as well as visualization, we show that this is related to the fact that current training/fine-tuning strategy of BERT does not converge for SQuAD.},
  issue = {05},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\97VQTVXT\Shen et al. - 2020 - Q-BERT Hessian Based Ultra Low Precision Quantiza.pdf}
}

@article{shinFixedPointPerformanceAnalysis2019,
  title = {Fixed-{{Point Performance Analysis}} of {{Recurrent Neural Networks}}},
  author = {Shin, Sungho and Hwang, Kyuyeon and Sung, Wonyong},
  year = {2015},
  journal = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {32},
  number = {4},
  eprint = {1512.01322},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {158--158},
  issn = {1053-5888},
  doi = {10.1109/MSP.2015.2411564},
  url = {http://arxiv.org/abs/1512.01322},
  urldate = {2023-09-06},
  abstract = {Recurrent neural networks have shown excellent performance in many applications, however they require increased complexity in hardware or software based implementations. The hardware complexity can be much lowered by minimizing the word-length of weights and signals. This work analyzes the fixed-point performance of recurrent neural networks using a retrain based quantization method. The quantization sensitivity of each layer in RNNs is studied, and the overall fixed-point optimization results minimizing the capacity of weights while not sacrificing the performance are presented. A language model and a phoneme recognition examples are used.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\LTKV6PWP\\Shin et al. - 2015 - Fixed-Point Performance Analysis of Recurrent Neur.pdf;C\:\\Users\\mospr\\Zotero\\storage\\5BZZRHAI\\1512.html}
}



@online{srinivasDatafreeParameterPruning2015,
  title = {Data-Free Parameter Pruning for {{Deep Neural Networks}}},
  author = {Srinivas, Suraj and Babu, R. Venkatesh},
  year = {2015},
  eprint = {1507.06149},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1507.06149},
  url = {http://arxiv.org/abs/1507.06149},
  urldate = {2023-09-07},
  abstract = {Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85\textbackslash\% of the total parameters in an MNIST-trained network, and about 35\textbackslash\% for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\V3EH4KL5\\Srinivas and Babu - 2015 - Data-free parameter pruning for Deep Neural Networ.pdf;C\:\\Users\\mospr\\Zotero\\storage\\K7X2GYGL\\1507.html}
}

@inproceedings{stockTrainingQuantizationNoise2021,
  title = {Training with {{Quantization Noise}} for {{Extreme Model Compression}}},
  booktitle = {{{ICLR}} 2021 - {{International Conference}} on {{Learning Representations}}},
  author = {Stock, Pierre and Fan, Angela and Graham, Benjamin and Grave, Edouard and Gribonval, Rémi and Jegou, Herve and Joulin, Armand},
  year = {2021},
  location = {{Vienna, Austria}},
  url = {https://inria.hal.science/hal-03136442},
  urldate = {2023-10-05},
  abstract = {We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training, where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator. In this paper, we extend this approach to work with extreme compression methods where the approximations introduced by STE are severe. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5\% accuracy on MNLI by compressing RoBERTa to 14 MB and 80.0\% top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3 MB.},
  keywords = {Compression,Efficiency,Product Quantization}
}

@online{sungResiliencyDeepNeural2016,
  title = {Resiliency of {{Deep Neural Networks}} under {{Quantization}}},
  author = {Sung, Wonyong and Shin, Sungho and Hwang, Kyuyeon},
  year = {2016},
  eprint = {1511.06488},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1511.06488},
  url = {http://arxiv.org/abs/1511.06488},
  urldate = {2023-09-06},
  abstract = {The complexity of deep neural network algorithms for hardware implementation can be much lowered by optimizing the word-length of weights and signals. Direct quantization of floating-point weights, however, does not show good performance when the number of bits assigned is small. Retraining of quantized networks has been developed to relieve this problem. In this work, the effects of retraining are analyzed for a feedforward deep neural network (FFDNN) and a convolutional neural network (CNN). The network complexity is controlled to know their effects on the resiliency of quantized networks by retraining. The complexity of the FFDNN is controlled by varying the unit size in each hidden layer and the number of layers, while that of the CNN is done by modifying the feature map configuration. We find that the performance gap between the floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks exists with a fair amount in 'complexity limited' networks, but the discrepancy almost vanishes in fully complex networks whose capability is limited by the training data, rather than by the number of connections. This research shows that highly complex DNNs have the capability of absorbing the effects of severe weight quantization through retraining, but connection limited networks are less resilient. This paper also presents the effective compression ratio to guide the trade-off between the network size and the precision when the hardware resource is limited.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\INGDQEED\\Sung et al. - 2016 - Resiliency of Deep Neural Networks under Quantizat.pdf;C\:\\Users\\mospr\\Zotero\\storage\\UTUGQ3RK\\1511.html}
}

@online{sunSimpleEffectivePruning2023,
  title = {A {{Simple}} and {{Effective Pruning Approach}} for {{Large Language Models}}},
  author = {Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},
  year = {2023},
  eprint = {2306.11695},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.11695},
  url = {http://arxiv.org/abs/2306.11695},
  urldate = {2023-09-07},
  abstract = {As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prune weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method on LLaMA across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and competes favorably against recent methods involving intensive weight update. Code is available at https://github.com/locuslab/wanda.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\4TCBWK3N\\Sun et al. - 2023 - A Simple and Effective Pruning Approach for Large .pdf;C\:\\Users\\mospr\\Zotero\\storage\\EVLQFTLS\\2306.html}
}

@article{tavaraParallelComputingSupport2019,
  title = {Parallel {{Computing}} of {{Support Vector Machines}}: {{A Survey}}},
  shorttitle = {Parallel {{Computing}} of {{Support Vector Machines}}},
  author = {Tavara, Shirin},
  year = {2019},
  journal = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {51},
  number = {6},
  pages = {123:1--123:38},
  issn = {0360-0300},
  doi = {10.1145/3280989},
  url = {https://dl.acm.org/doi/10.1145/3280989},
  urldate = {2023-09-05},
  abstract = {The immense amount of data created by digitalization requires parallel computing for machine-learning methods. While there are many parallel implementations for support vector machines (SVMs), there is no clear suggestion for every application scenario. Many factor—including optimization algorithm, problem size and dimension, kernel function, parallel programming stack, and hardware architecture—impact the efficiency of implementations. It is up to the user to balance trade-offs, particularly between computation time and classification accuracy. In this survey, we review the state-of-the-art implementations of SVMs, their pros and cons, and suggest possible avenues for future research.},
  keywords = {CPU parallelism,data movement,decomposition,Dual optimization,GPU parallelism,primal optimization,speedup},
  file = {C:\Users\mospr\Zotero\storage\LTPICCBU\Tavara - 2019 - Parallel Computing of Support Vector Machines A S.pdf}
}

@online{tianContrastiveRepresentationDistillation2022,
  title = {Contrastive {{Representation Distillation}}},
  author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  year = {2022},
  eprint = {1910.10699},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1910.10699},
  url = {http://arxiv.org/abs/1910.10699},
  urldate = {2023-09-05},
  abstract = {Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: http://github.com/HobbitLong/RepDistiller.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\8VZQUMD7\\Tian et al. - 2022 - Contrastive Representation Distillation.pdf;C\:\\Users\\mospr\\Zotero\\storage\\CNVTWBKS\\1910.html}
}

@article{vaderaMethodsPruningDeep2021,
	title={Methods for pruning deep neural networks},
	author={Vadera, Sunil and Ameen, Salem},
	journal={IEEE Access},
	volume={10},
	pages={63280--63300},
	year={2022},
	publisher={IEEE}
}

@inproceedings{vaswaniAttentionAllYou2023,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2023-10-05},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {C:\Users\mospr\Zotero\storage\X5H93BIT\Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@online{voitaAnalyzingMultiHeadSelfAttention2019,
  title = {Analyzing {{Multi-Head Self-Attention}}: {{Specialized Heads Do}} the {{Heavy Lifting}}, the {{Rest Can Be Pruned}}},
  shorttitle = {Analyzing {{Multi-Head Self-Attention}}},
  author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  year = {2019},
  eprint = {1905.09418},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1905.09418},
  url = {http://arxiv.org/abs/1905.09418},
  urldate = {2023-09-07},
  abstract = {Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\JVBACF4R\\Voita et al. - 2019 - Analyzing Multi-Head Self-Attention Specialized H.pdf;C\:\\Users\\mospr\\Zotero\\storage\\G363G3WG\\1905.html}
}
@inproceedings{walawalkarOnlineEnsembleModel2020,
	title={Online ensemble model compression using knowledge distillation},
	author={Walawalkar, Devesh and Shen, Zhiqiang and Savvides, Marios},
	booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XIX 16},
	pages={18--35},
	year={2020},
	organization={Springer}
}


@inproceedings{wangHAQHardwareAwareAutomated2019,
  title = {{{HAQ}}: {{Hardware-Aware Automated Quantization With Mixed Precision}}},
  shorttitle = {{{HAQ}}},
  author = {Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  year = {2019},
  pages = {8612--8620},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\27UFSLPN\Wang et al. - 2019 - HAQ Hardware-Aware Automated Quantization With Mi.pdf}
}



@inproceedings{wangProgressiveBlockwiseKnowledge2018,
  title = {Progressive Blockwise Knowledge Distillation for Neural Network Acceleration},
  booktitle = {Proceedings of the 27th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Wang, Hui and Zhao, Hanbin and Li, Xi and Tan, Xu},
  year = {2018},
  series = {{{IJCAI}}'18},
  pages = {2769--2775},
  publisher = {{AAAI Press}},
  location = {{Stockholm, Sweden}},
  abstract = {As an important and challenging problem in machine learning and computer vision, neural network acceleration essentially aims to enhance the computational efficiency without sacrificing the model accuracy too much. In this paper, we propose a progressive blockwise learning scheme for teacher-student model distillation at the subnetwork block level. The proposed scheme is able to distill the knowledge of the entire teacher network by locally extracting the knowledge of each block in terms of progressive blockwise function approximation. Furthermore, we propose a structure design criterion for the student subnetwork block, which is able to effectively preserve the original receptive field from the teacher network. Experimental results demonstrate the effectiveness of the proposed scheme against the state-of-the-art approaches.},
  isbn = {978-0-9992411-2-7}
}

@inproceedings{wangStructuredPruningLarge2020,
	title={Structured Pruning of Large Language Models},
	author={Wang, Ziheng and Wohlwend, Jeremy and Lei, Tao},
	booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	pages={6151--6162},
	year={2020}
}


@inproceedings{wangSuperGLUEStickierBenchmark2020,
  title = {{{SuperGLUE}}: {{A Stickier Benchmark}} for {{General-Purpose Language Understanding Systems}}},
  shorttitle = {{{SuperGLUE}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
  urldate = {2023-10-05},
  abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at https://super.gluebenchmark.com.},
  file = {C:\Users\mospr\Zotero\storage\G3A4R6NE\Wang et al. - 2019 - SuperGLUE A Stickier Benchmark for General-Purpos.pdf}
}



@inproceedings{wenLearningStructuredSparsity2016,
  title = {Learning {{Structured Sparsity}} in {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/41bfd20a38bb1b0bec75acf0845530a7-Abstract.html},
  urldate = {2023-10-05},
  abstract = {High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNN’s evaluation. Experimental results show that SSL achieves on average 5.1X and 3.1X speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth reduces a 20-layer Deep Residual Network (ResNet) to 18 layers while improves the accuracy from 91.25\% to 92.60\%, which is still higher than that of original ResNet with 32 layers. For AlexNet, SSL reduces the error by \textasciitilde 1\%.},
  file = {C:\Users\mospr\Zotero\storage\ZL9YRLYK\Wen et al. - 2016 - Learning Structured Sparsity in Deep Neural Networ.pdf}
}

@article{wuExtremeCompressionPretrained2022,
	title = {{{XTC}}: {{Extreme Compression}} for {{Pre-trained Transformers Made Simple}} and {{Efficient}}},
	shorttitle = {{{XTC}}},
	author = {Wu, Xiaoxia and Yao, Zhewei and Zhang, Minjia and Li, Conglong and He, Yuxiong},
	year = {2022},
	journal = {Advances in Neural Information Processing Systems},
	volume = {35},
	pages = {3217--3231},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/1579d5d8edacd85ac1a86aea28bdf32d-Abstract-Conference.html},
	urldate = {2023-10-05},
	langid = {english},
	file = {C:\Users\mospr\Zotero\storage\UI5TNNFP\Wu et al. - 2022 - XTC Extreme Compression for Pre-trained Transform.pdf}
}


@inproceedings{wuQuantizedConvolutionalNeural2016,
  title = {Quantized {{Convolutional Neural Networks}} for {{Mobile Devices}}},
  author = {Wu, Jiaxiang and Leng, Cong and Wang, Yuhang and Hu, Qinghao and Cheng, Jian},
  year = {2016},
  pages = {4820--4828},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Wu_Quantized_Convolutional_Neural_CVPR_2016_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\JB62SLEY\Wu et al. - 2016 - Quantized Convolutional Neural Networks for Mobile.pdf}
}



@inproceedings{xuConvolutionalNeuralNetwork2020,
  title = {Convolutional {{Neural Network Pruning}}: {{A Survey}}},
  shorttitle = {Convolutional {{Neural Network Pruning}}},
  booktitle = {2020 39th {{Chinese Control Conference}} ({{CCC}})},
  author = {Xu, Sheng and Huang, Anran and Chen, Lei and Zhang, Baochang},
  year = {2020},
  pages = {7458--7463},
  issn = {1934-1768},
  doi = {10.23919/CCC50068.2020.9189610},
  abstract = {Deep convolutional neural networks have enabled remarkable progress over the last years on a variety of visual tasks, such as image recognition, speech recognition, and machine translation. These tasks contribute many to machine intelligence. However, developments of deep convolutional neural networks to a machine terminal remains challenging due to massive number of parameters and float operations that a typical model contains. Therefore, there is growing interest in convolutional neural network pruning. Existing work in this field of research can be categorized according to three dimensions: pruning method, training strategy, estimation criterion.},
  eventtitle = {2020 39th {{Chinese Control Conference}} ({{CCC}})},
  keywords = {Computational modeling,Computer architecture,convolutional neural networks,Convolutional neural networks,Estimation,estimation criterion,Kernel,machine intelligence,pruning method,Task analysis,Training,training strategy},
  file = {C:\Users\mospr\Zotero\storage\2ZW8UHKP\9189610.html}
}


@inproceedings{yangQuantizationNetworks2019,
  title = {Quantization {{Networks}}},
  author = {Yang, Jiwei and Shen, Xu and Xing, Jun and Tian, Xinmei and Li, Houqiang and Deng, Bing and Huang, Jianqiang and Hua, Xian-sheng},
  year = {2019},
  pages = {7308--7316},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Quantization_Networks_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\DQULL2GQ\Yang et al. - 2019 - Quantization Networks.pdf}
}


@inproceedings{yangSnapshotDistillationTeacherStudent2018,
  title = {Snapshot {{Distillation}}: {{Teacher-Student Optimization}} in {{One Generation}}},
  shorttitle = {Snapshot {{Distillation}}},
  author = {Yang, Chenglin and Xie, Lingxi and Su, Chi and Yuille, Alan L.},
  year = {2019},
  pages = {2859--2868},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Snapshot_Distillation_Teacher-Student_Optimization_in_One_Generation_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\BK678KHJ\Yang et al. - 2019 - Snapshot Distillation Teacher-Student Optimizatio.pdf}
}


@inproceedings{yaoHAWQV3DyadicNeural2021,
  title = {{{HAWQ-V3}}: {{Dyadic Neural Network Quantization}}},
  shorttitle = {{{HAWQ-V3}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael and Keutzer, Kurt},
  year = {2021},
  pages = {11875--11886},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/yao21a.html},
  urldate = {2023-10-05},
  abstract = {Current low-precision quantization algorithms often have the hidden cost of conversion back and forth from floating point to quantized integer values. This hidden cost limits the latency improvement realized by quantizing Neural Networks. To address this, we present HAWQ-V3, a novel mixed-precision integer-only quantization framework. The contributions of HAWQ-V3 are the following: (i) An integer-only inference where the entire computational graph is performed only with integer multiplication, addition, and bit shifting, without any floating point operations or even integer division; (ii) A novel hardware-aware mixed-precision quantization method where the bit-precision is calculated by solving an integer linear programming problem that balances the trade-off between model perturbation and other constraints, e.g., memory footprint and latency; (iii) Direct hardware deployment and open source contribution for 4-bit uniform/mixed-precision quantization in TVM, achieving an average speed up of 1.45x for uniform 4-bit, as compared to uniform 8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed methods on ResNet18/50 and InceptionV3, for various model compression levels with/without mixed precision. For ResNet50, our INT8 quantization achieves an accuracy of 77.58\%, which is 2.68\% higher than prior integer-only work, and our mixed-precision INT4/8 quantization can reduce INT8 latency by 23\% and still achieve 76.73\% accuracy. Our framework and the TVM implementation have been open sourced (HAWQ, 2020).},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\4YGHLSDI\\Yao et al. - 2021 - HAWQ-V3 Dyadic Neural Network Quantization.pdf;C\:\\Users\\mospr\\Zotero\\storage\\TE5Q7HCU\\Yao et al. - 2021 - HAWQ-V3 Dyadic Neural Network Quantization.pdf}
}


@article{yaoZeroQuantEfficientAffordable2022,
  title = {{{ZeroQuant}}: {{Efficient}} and {{Affordable Post-Training Quantization}} for {{Large-Scale Transformers}}},
  shorttitle = {{{ZeroQuant}}},
  author = {Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {27168--27183},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html},
  urldate = {2023-10-05},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\NJHZDA9M\Yao et al. - 2022 - ZeroQuant Efficient and Affordable Post-Training .pdf}
}

@article{yuWidthDepthPruning2022,
  title = {Width \& {{Depth Pruning}} for {{Vision Transformers}}},
  author = {Yu, Fang and Huang, Kun and Wang, Meng and Cheng, Yuan and Chu, Wei and Cui, Li},
  year = {2022},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {3},
  pages = {3143--3151},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i3.20222},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/20222},
  urldate = {2023-09-07},
  abstract = {Transformer models have demonstrated their promising potential and achieved excellent performance on a series of computer vision tasks. However, the huge computational cost of vision transformers hinders their deployment and application to edge devices. Recent works have proposed to find and remove the unimportant units of vision transformers. Despite achieving remarkable results, these methods take one dimension of network width into consideration and ignore network depth, which is another important dimension for pruning vision transformers. Therefore, we propose a Width \& Depth Pruning (WDPruning) framework that reduces both width and depth dimensions simultaneously. Specifically, for width pruning, a set of learnable pruning-related parameters is used to adaptively adjust the width of transformer. For depth pruning, we introduce several shallow classifiers by using the intermediate information of the transformer blocks, which allows images to be classified by shallow classifiers instead of the deeper classifiers. In the inference period, all of the blocks after shallow classifiers can be dropped so they don’t bring additional parameters and computation. Experimental results on benchmark datasets demonstrate that the proposed method can significantly reduce the computational costs of mainstream vision transformers such as DeiT and Swin Transformer with a minor accuracy drop. In particular, on ILSVRC-12, we achieve over 22\% pruning ratio of FLOPs by compressing DeiT-Base, even with an increase of 0.14\% Top-1 accuracy.},
  issue = {3},
  langid = {english},
  keywords = {Computer Vision (CV)},
  file = {C:\Users\mospr\Zotero\storage\2E5ZBPSL\Yu et al. - 2022 - Width & Depth Pruning for Vision Transformers.pdf}
}

@article{zhangAdversarialCodistillationLearning2021,
  title = {Adversarial Co-Distillation Learning for Image Recognition},
  author = {Zhang, Haoran and Hu, Zhenzhen and Qin, Wei and Xu, Mingliang and Wang, Meng},
  year = {2021},
  booktitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {111},
  pages = {107659},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2020.107659},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320320304623},
  urldate = {2023-09-08},
  abstract = {Knowledge distillation is an effective way to transfer the knowledge from a pre-trained teacher model to a student model. Co-distillation, as an online variant of distillation, further accelerates the training process and paves a new way to explore the “dark knowledge” by training n models in parallel. In this paper, we explore the “divergent examples”, which can make the classifiers have different predictions and thus induce the “dark knowledge”, and we propose a novel approach named Adversarial Co-distillation Networks (ACNs) to enhance the “dark knowledge” by generating extra divergent examples. Note that we do not involve any extra dataset, and we only utilize the standard training set to train the entire framework. ACNs are end-to-end frameworks composed of two parts: an adversarial phase consisting of Generative Adversarial Networks (GANs) to generate the divergent examples and a co-distillation phase consisting of multiple classifiers to learn the divergent examples. These two phases are learned in an iterative and adversarial way. To guarantee the quality of the divergent examples and the stability of ACNs, we further design “Weakly Residual Connection” module and “Restricted Adversarial Search” module to assist in the training process. Extensive experiments with various deep architectures on different datasets well demonstrate the effectiveness of our approach.},
  keywords = {Data augmentation,Divergent examples,Generative adversarial nets,Image classification,Knowledge distillation},
  file = {C:\Users\mospr\Zotero\storage\LT6XEV3R\S0031320320304623.html}
}


@inproceedings{zhangBeYourOwn2019,
  title = {Be {{Your Own Teacher}}: {{Improve}} the {{Performance}} of {{Convolutional Neural Networks}} via {{Self Distillation}}},
  shorttitle = {Be {{Your Own Teacher}}},
  author = {Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
  year = {2019},
  pages = {3713--3722},
  url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  file = {C:\Users\mospr\Zotero\storage\DTGLZWY3\Zhang et al. - 2019 - Be Your Own Teacher Improve the Performance of Co.pdf}
}


@inproceedings{zhangDeepMutualLearning2017,
	title={Deep mutual learning},
	author={Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={4320--4328},
	year={2018}
}




@inproceedings{zhangFastHumanPose2019,
  title = {Fast {{Human Pose Estimation}}},
  author = {Zhang, Feng and Zhu, Xiatian and Ye, Mao},
  year = {2019},
  pages = {3517--3526},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\3ASHSL5B\Zhang et al. - 2019 - Fast Human Pose Estimation.pdf}
}



@inproceedings{zhangLQNetsLearnedQuantization2018,
  title = {{{LQ-Nets}}: {{Learned Quantization}} for {{Highly Accurate}} and {{Compact Deep Neural Networks}}},
  shorttitle = {{{LQ-Nets}}},
  author = {Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
  year = {2018},
  pages = {365--382},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  file = {C:\Users\mospr\Zotero\storage\KLJHV3K7\Zhang et al. - 2018 - LQ-Nets Learned Quantization for Highly Accurate .pdf}
}



@inproceedings{zhaoContrastiveKnowledgeTransfer2023,
  title = {A {{Contrastive Knowledge Transfer Framework}} for {{Model Compression}} and {{Transfer Learning}}},
  booktitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhao, Kaiqi and Chen, Yitao and Zhao, Ming},
  year = {2023},
  pages = {1--5},
  doi = {10.1109/ICASSP49357.2023.10095744},
  url = {https://ieeexplore.ieee.org/abstract/document/10095744},
  urldate = {2023-10-05},
  abstract = {Knowledge Transfer (KT) achieves competitive performance and is widely used for image classification tasks in model compression and transfer learning. Existing KT works transfer the information from a large model ("teacher") to train a small model ("student") by minimizing the difference of their conditionally independent output distributions. However, these works overlook the high-dimension structural knowledge from the intermediate representations of the teacher, which leads to limited effectiveness, and they are motivated by various heuristic intuitions, which makes it difficult to generalize. This paper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which enables the transfer of sufficient structural knowledge from the teacher to the student by optimizing multiple contrastive objectives across the intermediate representations between them. Also, CKTF provides a generalized agreement to existing KT techniques and increases their performance significantly by deriving them as specific cases of CKTF. The extensive evaluation shows that CKTF consistently outperforms the existing KT works by 0.04\% to 11.59\% in model compression and by 0.4\% to 4.75\% in transfer learning on various models and datasets.},
  booktitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C:\Users\mospr\Zotero\storage\SACWIK9M\Zhao et al. - 2023 - A Contrastive Knowledge Transfer Framework for Mod.pdf}
}

@inproceedings{zhaoVariationalConvolutionalNeural2019,
  title = {Variational {{Convolutional Neural Network Pruning}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhao, Chenglong and Ni, Bingbing and Zhang, Jian and Zhao, Qiwei and Zhang, Wenjun and Tian, Qi},
  year = {2019},
  pages = {2775--2784},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00289},
  abstract = {We propose a variational Bayesian scheme for pruning convolutional neural networks in channel level. This idea is motivated by the fact that deterministic value based pruning methods are inherently improper and unstable. In a nutshell, variational technique is introduced to estimate distribution of a newly proposed parameter, called channel saliency, based on this, redundant channels can be removed from model via a simple criterion. The advantages are two-fold: 1) Our method conducts channel pruning without desire of re-training stage, thus improving the computation efficiency. 2) Our method is implemented as a stand-alone module, called variational pruning layer, which can be straightforwardly inserted into off-the-shelf deep learning packages, without any special network design. Extensive experimental results well demonstrate the effectiveness of our method: For CIFAR-10, we perform channel removal on different CNN models up to 74\% reduction, which results in significant size reduction and computation saving. For ImageNet, about 40\% channels of ResNet-50 are removed without compromising accuracy.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Computer Vision Theory,Deep Learning},
  file = {C:\Users\mospr\Zotero\storage\Q5USH4KY\8954234.html}
}
@inproceedings{zhouHULKEnergyEfficiency2020,
	title={HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing},
	author={Zhou, Xiyou and Chen, Zhiyu and Jin, Xiaoyong and Wang, William Yang},
	booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations},
	pages={329--336},
	year={2021}
}




@inproceedings{zhuangStructuredBinaryNeural2018,
  title = {Structured {{Binary Neural Networks}} for {{Accurate Image Classification}} and {{Semantic Segmentation}}},
  author = {Zhuang, Bohan and Shen, Chunhua and Tan, Mingkui and Liu, Lingqiao and Reid, Ian},
  year = {2019},
  pages = {413--422},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhuang_Structured_Binary_Neural_Networks_for_Accurate_Image_Classification_and_Semantic_CVPR_2019_paper.html},
  urldate = {2023-10-05},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\mospr\Zotero\storage\N8DNXDX8\Zhuang et al. - 2019 - Structured Binary Neural Networks for Accurate Ima.pdf}
}



@article{zhuPruneNotPrune2017,
  title = {To {{Prune}}, or {{Not}} to {{Prune}}: {{Exploring}} the {{Efficacy}} of {{Pruning}} for {{Model Compression}}},
  shorttitle = {To {{Prune}}, or {{Not}} to {{Prune}}},
  author = {Zhu, Michael H. and Gupta, Suyog},
  year = {2018},
  url = {https://openreview.net/forum?id=S1lN69AT-},
  urldate = {2023-10-05},
  abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
  langid = {english},
  file = {C:\Users\mospr\Zotero\storage\4SCC7SSP\Zhu and Gupta - 2018 - To Prune, or Not to Prune Exploring the Efficacy .pdf}
}

@online{zhuSurveyModelCompression2023,
  title = {A {{Survey}} on {{Model Compression}} for {{Large Language Models}}},
  author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  year = {2023},
  eprint = {2308.07633},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.07633},
  url = {http://arxiv.org/abs/2308.07633},
  urldate = {2023-09-06},
  abstract = {Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of compressed LLMs. By providing insights into the latest developments and practical implications, this survey serves as an invaluable resource for both researchers and practitioners. As LLMs continue to evolve, this survey aims to facilitate enhanced efficiency and real-world applicability, establishing a foundation for future advancements in the field.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\6DSWIABA\\Zhu et al. - 2023 - A Survey on Model Compression for Large Language M.pdf;C\:\\Users\\mospr\\Zotero\\storage\\VRGW2SGT\\2308.html}
}

@online{zhuVisionTransformerPruning2021,
  title = {Vision {{Transformer Pruning}}},
  author = {Zhu, Mingjian and Tang, Yehui and Han, Kai},
  year = {2021},
  eprint = {2104.08500},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.08500},
  url = {http://arxiv.org/abs/2104.08500},
  urldate = {2023-09-07},
  abstract = {Vision transformer has achieved competitive performance on a variety of computer vision applications. However, their storage, run-time memory, and computational demands are hindering the deployment to mobile devices. Here we present a vision transformer pruning approach, which identifies the impacts of dimensions in each layer of transformer and then executes pruning accordingly. By encouraging dimension-wise sparsity in the transformer, important dimensions automatically emerge. A great number of dimensions with small importance scores can be discarded to achieve a high pruning ratio without significantly compromising accuracy. The pipeline for vision transformer pruning is as follows: 1) training with sparsity regularization; 2) pruning dimensions of linear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of the proposed algorithm are well evaluated and analyzed on ImageNet dataset to demonstrate the effectiveness of our proposed method.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\mospr\\Zotero\\storage\\K3DT7H3F\\Zhu et al. - 2021 - Vision Transformer Pruning.pdf;C\:\\Users\\mospr\\Zotero\\storage\\V9U4P6SL\\2104.html}
}
